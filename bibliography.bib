
@online{noauthor_cmake_nodate,
	title = {{CMake}},
	url = {https://cmake.org/},
	urldate = {2023-06-11},
	langid = {american},
}

@online{noauthor_juce_nodate,
	title = {Juce},
	url = {https://juce.com/},
	abstract = {{JUCE} is the most widely used framework for audio application and plug-in development on Windows, {macOS}, Linux, {iOS}, Android, {VST}, {VST}3, {AU}, {AUv}3, {AAX} and {LV}2 plug-ins.},
	titleaddon = {{JUCE}},
	urldate = {2023-06-11},
	langid = {british},
}

@book{parker_good_2009,
	location = {Baltimore},
	title = {Good vibrations: the physics of music},
	isbn = {978-0-8018-9264-6},
	shorttitle = {Good vibrations},
	pagetotal = {274},
	publisher = {Johns Hopkins University Press},
	author = {Parker, Barry R.},
	date = {2009},
	note = {{OCLC}: ocn320194527},
	keywords = {Acoustics and physics, Music, Sound, Sound-waves, Vibration, Wave-motion, Theory of},
}

@book{tsuji_physics_2021,
	location = {Cham, Switzerland},
	title = {Physics and music: essential connections and illuminating excursions},
	isbn = {978-3-030-68675-8},
	shorttitle = {Physics and music},
	publisher = {Springer Nature},
	author = {Tsuji, Kinko and Müller, Stefan C.},
	date = {2021},
}

@book{white_physics_2014,
	location = {Mineola, New York},
	title = {Physics and music: the science of musical sound},
	isbn = {978-0-486-79400-6},
	shorttitle = {Physics and music},
	abstract = {"This foundational text is written for students who want to go beyond the perceptual stage of music to learn how musical sound is created and perceived. It surveys a wide range of topics related to acoustics, beginning with a brief history of the art and science of music. Succeeding chapters explore the general principles of sound, musical scales, the primary ways in which sound can be generated, the characteristics of instruments, the use of mechanical and electronic recording devices, hi-fi stereophonic and quadraphonic sound, the design of electronic musical instruments, and architectural acoustics. Comprehensive yet accessible, Physics and Music includes over 300 diagrams, photographs, and tables. Each chapter concludes with questions, problems, and projects, in addition to references for further study. 1980 edition."-- Provided by publisher},
	publisher = {Dover Publications, Inc.},
	author = {White, Harvey Elliott and White, Donald H.},
	date = {2014},
	note = {{OCLC}: 878661301},
}

@book{katz_capturing_2010,
	location = {Berkeley},
	edition = {Rev. ed},
	title = {Capturing sound: how technology has changed music},
	isbn = {978-0-520-26105-1},
	shorttitle = {Capturing sound},
	abstract = {Synopsis: Fully revised and updated, this new edition of Mark Katz's award-winning text adds coverage of mashups and Auto-Tune, explores recent developments in file-sharing, and includes an expanded conclusion and bibliography. Find illustrative sound and film clips www.ucpress.edu/go/capturingsound},
	pagetotal = {320},
	publisher = {University of California Press},
	author = {Katz, Mark},
	date = {2010},
	note = {{OCLC}: ocn610019531},
	keywords = {Music and technology, Sound recording industry},
}

@article{mathews_digital_1963,
	title = {The Digital Computer as a Musical Instrument},
	volume = {142},
	issn = {00368075, 10959203},
	url = {http://www.jstor.org/stable/1712380},
	pages = {553--557},
	number = {3592},
	journaltitle = {Science},
	author = {Mathews, M. V.},
	urldate = {2023-06-06},
	date = {1963},
	note = {Publisher: American Association for the Advancement of Science},
}

@unpublished{mathews_music_2004,
	location = {Computer History Museum},
	title = {Music Meets the Computer},
	url = {https://www.youtube.com/watch?v=Hloic1oBfug},
	abstract = {Computers have revolutionized music making. Two of the most important pioneers of computer music, Max Mathews and John Chowning, stand at the epicenter of this musical revolution. Research led by Mathews at Bell Laboratories, beginning in the 1950s, created a series of programming languages that are the direct precursors of today's software synthesizers. Max Mathew's many contributions to interactive music systems, algorithmic composition, and psychoacoustics (with Jean-Claude Risset) are equally seminal. Stanford's legendary Center for Computer Research in Music and Acoustics ({CCRMA}) led for many years by Chowning, has long been a hotbed of innovation. After groundbreaking research in sound spatialization, Chowning's invention of frequency modulation ({FM}) synthesis led to the most successful synthesizer of all time: the Yamaha {DX}7. In this video, Chowning and Mathews are in conversation with Curtis Roads, composer and music historian. The video also includes a performance by pianist Chryssie Nanou performing "Duet for One Pianist."},
	author = {Mathews, Max and Chowning, John and Roads, Curtis},
	urldate = {2023-06-06},
	date = {2004-12-14},
}

@book{lai_practical_2004,
	location = {London ; Burlington, {MA}},
	title = {Practical digital signal processing for engineers and technicians},
	isbn = {978-0-7506-5798-3},
	pagetotal = {289},
	publisher = {Newnes},
	author = {Lai, Edmund},
	date = {2004},
	note = {{OCLC}: ocm51527224},
	keywords = {Digital techniques, Signal processing, Techniques numériques, Traitement du signal},
}

@book{ruschkowski_elektronische_2019,
	location = {Ditzingen},
	edition = {3., ergänzte Auflage 2019},
	title = {Elektronische Klänge und musikalische Entdeckungen},
	isbn = {978-3-15-019613-7},
	series = {Reclams Universal-Bibliothek},
	abstract = {Die Digitalisierung brachte nicht nur neue Notations-, Speicher- und Übertragungsformate der Musik mit sich, sondern ermöglichte auch die Erzeugung neuer Klänge und Klangeffekte im Tonstudio. Der erste Teil des Bandes widmet sich der analogen Klangsynthese und Klangsteuerung, der zweite Teil bietet einen Überblick über die digitale Klangerzeugung und den Einsatz von Hardware und Software bei Komposition und Klangbearbeitung von den Anfängen bis heute},
	number = {19613},
	publisher = {Reclam},
	author = {Ruschkowski, André},
	date = {2019},
}

@book{robinson_getting_2013,
	location = {Birmingham},
	title = {Getting started with {JUCE}: leverage the power of the {JUCE} framework to start developing applications},
	isbn = {978-1-4619-4968-8},
	shorttitle = {Getting started with {JUCE}},
	abstract = {His book is a fast-paced, practical guide full of step-by-step examples which are easy to follow and implement. This book is for programmers with a basic grasp of C++. The examples start at a basic level, making few assumptions beyond fundamental C++ concepts. Those without any experience with C++ should be able to follow and construct the examples, although you may need further support to understand the fundamental concepts},
	publisher = {Packt Publishing},
	author = {Robinson, Martin},
	date = {2013},
	note = {{OCLC}: 862386437},
}

@book{noauthor_notitle_nodate,
}

@misc{long_fully_2015,
	title = {Fully Convolutional Networks for Semantic Segmentation},
	url = {http://arxiv.org/abs/1411.4038},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks ({AlexNet}, the {VGG} net, and {GoogLeNet}) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of {PASCAL} {VOC} (20\% relative improvement to 62.2\% mean {IU} on 2012), {NYUDv}2, and {SIFT} Flow, while inference takes one third of a second for a typical image.},
	number = {{arXiv}:1411.4038},
	publisher = {{arXiv}},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	urldate = {2023-04-18},
	date = {2015-03-08},
	eprinttype = {arxiv},
	eprint = {1411.4038 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{salimans_progressive_2022,
	title = {Progressive Distillation for Fast Sampling of Diffusion Models},
	url = {http://arxiv.org/abs/2202.00512},
	abstract = {Diffusion models have recently shown great promise for generative modeling, outperforming {GANs} on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like {CIFAR}-10, {ImageNet}, and {LSUN}, we start out with state-of-the-art samplers taking as many as 8192 steps, and are able to distill down to models taking as few as 4 steps without losing much perceptual quality; achieving, for example, a {FID} of 3.0 on {CIFAR}-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time.},
	number = {{arXiv}:2202.00512},
	publisher = {{arXiv}},
	author = {Salimans, Tim and Ho, Jonathan},
	urldate = {2023-04-18},
	date = {2022-06-07},
	eprinttype = {arxiv},
	eprint = {2202.00512 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{song_denoising_2022,
	title = {Denoising Diffusion Implicit Models},
	url = {http://arxiv.org/abs/2010.02502},
	abstract = {Denoising diffusion probabilistic models ({DDPMs}) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models ({DDIMs}), a more efficient class of iterative implicit probabilistic models with the same training procedure as {DDPMs}. In {DDPMs}, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that {DDIMs} can produce high quality samples \$10 {\textbackslash}times\$ to \$50 {\textbackslash}times\$ faster in terms of wall-clock time compared to {DDPMs}, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
	number = {{arXiv}:2010.02502},
	publisher = {{arXiv}},
	author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
	urldate = {2023-04-18},
	date = {2022-10-05},
	eprinttype = {arxiv},
	eprint = {2010.02502 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ho_denoising_2020,
	title = {Denoising Diffusion Probabilistic Models},
	url = {http://arxiv.org/abs/2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional {CIFAR}10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art {FID} score of 3.17. On 256x256 {LSUN}, we obtain sample quality similar to {ProgressiveGAN}. Our implementation is available at https://github.com/hojonathanho/diffusion},
	number = {{arXiv}:2006.11239},
	publisher = {{arXiv}},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	urldate = {2023-04-06},
	date = {2020-12-16},
	eprinttype = {arxiv},
	eprint = {2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ronneberger_u-net_2015,
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	url = {http://arxiv.org/abs/1505.04597},
	shorttitle = {U-Net},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the {ISBI} challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and {DIC}) we won the {ISBI} cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent {GPU}. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	number = {{arXiv}:1505.04597},
	publisher = {{arXiv}},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	urldate = {2023-04-06},
	date = {2015-05-18},
	eprinttype = {arxiv},
	eprint = {1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{pasini_musika_2022,
	title = {Musika! Fast Infinite Waveform Music Generation},
	url = {http://arxiv.org/abs/2208.08706},
	abstract = {Fast and user-controllable music generation could enable novel ways of composing or performing music. However, state-of-the-art music generation systems require large amounts of data and computational resources for training, and are slow at inference. This makes them impractical for real-time interactive use. In this work, we introduce Musika, a music generation system that can be trained on hundreds of hours of music using a single consumer {GPU}, and that allows for much faster than real-time generation of music of arbitrary length on a consumer {CPU}. We achieve this by first learning a compact invertible representation of spectrogram magnitudes and phases with adversarial autoencoders, then training a Generative Adversarial Network ({GAN}) on this representation for a particular music domain. A latent coordinate system enables generating arbitrarily long sequences of excerpts in parallel, while a global context vector allows the music to remain stylistically coherent through time. We perform quantitative evaluations to assess the quality of the generated samples and showcase options for user control in piano and techno music generation. We release the source code and pretrained autoencoder weights at github.com/marcoppasini/musika, such that a {GAN} can be trained on a new music domain with a single {GPU} in a matter of hours.},
	number = {{arXiv}:2208.08706},
	publisher = {{arXiv}},
	author = {Pasini, Marco and Schlüter, Jan},
	urldate = {2023-04-03},
	date = {2022-08-18},
	eprinttype = {arxiv},
	eprint = {2208.08706 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{borsos_audiolm_2022,
	title = {{AudioLM}: a Language Modeling Approach to Audio Generation},
	url = {http://arxiv.org/abs/2209.03143},
	shorttitle = {{AudioLM}},
	abstract = {We introduce {AudioLM}, a framework for high-quality audio generation with long-term consistency. {AudioLM} maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, {AudioLM} learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, {AudioLM} generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.},
	number = {{arXiv}:2209.03143},
	publisher = {{arXiv}},
	author = {Borsos, Zalán and Marinier, Raphaël and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and Zeghidour, Neil},
	urldate = {2023-04-03},
	date = {2022-09-07},
	eprinttype = {arxiv},
	eprint = {2209.03143 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{dhariwal_jukebox_2020,
	title = {Jukebox: A Generative Model for Music},
	url = {http://arxiv.org/abs/2005.00341},
	shorttitle = {Jukebox},
	abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale {VQ}-{VAE} to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
	number = {{arXiv}:2005.00341},
	publisher = {{arXiv}},
	author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
	urldate = {2023-04-03},
	date = {2020-04-30},
	eprinttype = {arxiv},
	eprint = {2005.00341 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@misc{oord_wavenet_2016,
	title = {{WaveNet}: A Generative Model for Raw Audio},
	url = {http://arxiv.org/abs/1609.03499},
	shorttitle = {{WaveNet}},
	abstract = {This paper introduces {WaveNet}, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single {WaveNet} can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	number = {{arXiv}:1609.03499},
	publisher = {{arXiv}},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	urldate = {2023-04-03},
	date = {2016-09-19},
	eprinttype = {arxiv},
	eprint = {1609.03499 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
}

@misc{schneider_archisound_2023,
	title = {{ArchiSound}: Audio Generation with Diffusion},
	url = {http://arxiv.org/abs/2301.13267},
	shorttitle = {{ArchiSound}},
	abstract = {The recent surge in popularity of diffusion models for image generation has brought new attention to the potential of these models in other areas of media generation. One area that has yet to be fully explored is the application of diffusion models to audio generation. Audio generation requires an understanding of multiple aspects, such as the temporal dimension, long term structure, multiple layers of overlapping sounds, and the nuances that only trained listeners can detect. In this work, we investigate the potential of diffusion models for audio generation. We propose a set of models to tackle multiple aspects, including a new method for text-conditional latent audio diffusion with stacked 1D U-Nets, that can generate multiple minutes of music from a textual description. For each model, we make an effort to maintain reasonable inference speed, targeting real-time on a single consumer {GPU}. In addition to trained models, we provide a collection of open source libraries with the hope of simplifying future work in the field. Samples can be found at https://bit.ly/audio-diffusion. Codes are at https://github.com/archinetai/audio-diffusion-pytorch.},
	number = {{arXiv}:2301.13267},
	publisher = {{arXiv}},
	author = {Schneider, Flavio},
	urldate = {2023-03-19},
	date = {2023-01-30},
	eprinttype = {arxiv},
	eprint = {2301.13267 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{liu_audioldm_2023,
	title = {{AudioLDM}: Text-to-Audio Generation with Latent Diffusion Models},
	url = {http://arxiv.org/abs/2301.12503},
	shorttitle = {{AudioLDM}},
	abstract = {Text-to-audio ({TTA}) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in {TTA} have limited generation quality with high computational costs. In this study, we propose {AudioLDM}, a {TTA} system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining ({CLAP}) latents. The pretrained {CLAP} models enable us to train {LDMs} with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, {AudioLDM} is advantageous in both generation quality and computational efficiency. Trained on {AudioCaps} with a single {GPU}, {AudioLDM} achieves state-of-the-art {TTA} performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, {AudioLDM} is the first {TTA} system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm.github.io.},
	number = {{arXiv}:2301.12503},
	publisher = {{arXiv}},
	author = {Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D.},
	urldate = {2023-03-19},
	date = {2023-02-16},
	eprinttype = {arxiv},
	eprint = {2301.12503 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
}

@misc{huang_make--audio_2023,
	title = {Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models},
	url = {http://arxiv.org/abs/2301.12661},
	shorttitle = {Make-An-Audio},
	abstract = {Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining ({CLAP}) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation. Moreover, we present its controllability and generalization for X-to-Audio with "No Modality Left Behind", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input. Audio samples are available at https://Text-to-Audio.github.io},
	number = {{arXiv}:2301.12661},
	publisher = {{arXiv}},
	author = {Huang, Rongjie and Huang, Jiawei and Yang, Dongchao and Ren, Yi and Liu, Luping and Li, Mingze and Ye, Zhenhui and Liu, Jinglin and Yin, Xiang and Zhao, Zhou},
	urldate = {2023-03-19},
	date = {2023-01-29},
	eprinttype = {arxiv},
	eprint = {2301.12661 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{peebles_scalable_2023,
	title = {Scalable Diffusion Models with Transformers},
	url = {http://arxiv.org/abs/2212.09748},
	abstract = {We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers ({DiTs}) through the lens of forward pass complexity as measured by Gflops. We find that {DiTs} with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower {FID}. In addition to possessing good scalability properties, our largest {DiT}-{XL}/2 models outperform all prior diffusion models on the class-conditional {ImageNet} 512x512 and 256x256 benchmarks, achieving a state-of-the-art {FID} of 2.27 on the latter.},
	number = {{arXiv}:2212.09748},
	publisher = {{arXiv}},
	author = {Peebles, William and Xie, Saining},
	urldate = {2023-03-19},
	date = {2023-03-02},
	eprinttype = {arxiv},
	eprint = {2212.09748 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2023-03-19},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}
