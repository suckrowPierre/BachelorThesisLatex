
@misc{openai_gpt-4_2023,
	title = {{GPT}-4 Technical Report},
	url = {http://arxiv.org/abs/2303.08774},
	abstract = {We report the development of {GPT}-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, {GPT}-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. {GPT}-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of {GPT}-4's performance based on models trained with no more than 1/1,000th the compute of {GPT}-4.},
	number = {{arXiv}:2303.08774},
	publisher = {{arXiv}},
	author = {{OpenAI}},
	urldate = {2023-09-08},
	date = {2023-03-27},
	eprinttype = {arxiv},
	eprint = {2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{liu_audioldm2_2023,
	title = {{AudioLDM}2: Learning Holistic Audio Generation with Self-supervised Pretraining},
	url = {http://arxiv.org/abs/2308.05734},
	shorttitle = {{AudioLDM} 2},
	abstract = {Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called language of audio ({LOA}). Any audio can be translated into {LOA} based on {AudioMAE}, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into {LOA} by using a {GPT}-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on {LOA}. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained {AudioMAE} and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate new state-of-the-art or competitive performance to previous approaches. Our demo and code are available at https://audioldm.github.io/audioldm2.},
	number = {{arXiv}:2308.05734},
	publisher = {{arXiv}},
	author = {Liu, Haohe and Tian, Qiao and Yuan, Yi and Liu, Xubo and Mei, Xinhao and Kong, Qiuqiang and Wang, Yuping and Wang, Wenwu and Wang, Yuxuan and Plumbley, Mark D.},
	urldate = {2023-09-05},
	date = {2023-08-10},
	eprinttype = {arxiv},
	eprint = {2308.05734 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
}

@software{pierre-louis_suckrow_thesismodelsresultsgenerator_2023,
	title = {{ThesisModelsResultsGenerator}},
	url = {https://github.com/suckrowPierre/ThesisModelsResultsGenerator},
	author = {Pierre-Louis Suckrow},
	urldate = {2023-09-08},
	date = {2023-09-07},
	note = {original-date: 2023-09-07T14:32:45Z},
}

@online{pierre-louis_suckrow_text-zu-spielbarem-klang_nodate,
	title = {Text-zu-spielbarem-Klang: Synthesizer basierend auf Latent-Diffusion-Technologie},
	url = {https://suckrowpierre.github.io/TtPS.github.io/},
	author = {{Pierre-Louis Suckrow}},
	urldate = {2023-09-07},
}

@misc{tan_regeneration_2023,
	title = {Regeneration Learning: A Learning Paradigm for Data Generation},
	url = {http://arxiv.org/abs/2301.08846},
	shorttitle = {Regeneration Learning},
	abstract = {Machine learning methods for conditional data generation usually build a mapping from source conditional data X to target data Y. The target Y (e.g., text, speech, music, image, video) is usually high-dimensional and complex, and contains information that does not exist in source data, which hinders effective and efficient learning on the source-target mapping. In this paper, we present a learning paradigm called regeneration learning for data generation, which first generates Y' (an abstraction/representation of Y) from X and then generates Y from Y'. During training, Y' is obtained from Y through either handcrafted rules or self-supervised learning and is used to learn X--{\textgreater}Y' and Y'--{\textgreater}Y. Regeneration learning extends the concept of representation learning to data generation tasks, and can be regarded as a counterpart of traditional representation learning, since 1) regeneration learning handles the abstraction (Y') of the target data Y for data generation while traditional representation learning handles the abstraction (X') of source data X for data understanding; 2) both the processes of Y'--{\textgreater}Y in regeneration learning and X--{\textgreater}X' in representation learning can be learned in a self-supervised way (e.g., pre-training); 3) both the mappings from X to Y' in regeneration learning and from X' to Y in representation learning are simpler than the direct mapping from X to Y. We show that regeneration learning can be a widely-used paradigm for data generation (e.g., text generation, speech recognition, speech synthesis, music composition, image generation, and video generation) and can provide valuable insights into developing data generation methods.},
	number = {{arXiv}:2301.08846},
	publisher = {{arXiv}},
	author = {Tan, Xu and Qin, Tao and Bian, Jiang and Liu, Tie-Yan and Bengio, Yoshua},
	urldate = {2023-09-06},
	date = {2023-01-20},
	eprinttype = {arxiv},
	eprint = {2301.08846 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@book{thyagarajan_introduction_2019,
	location = {Cham},
	edition = {1st ed. 2019},
	title = {Introduction to Digital Signal Processing Using {MATLAB} with Application to Digital Communications},
	isbn = {978-3-319-76029-2},
	abstract = {This textbook provides engineering students with instruction on processing signals encountered in speech, music, and wireless communications using software or hardware by employing basic mathematical methods. The book starts with an overview of signal processing, introducing readers to the field. It goes on to give instruction in converting continuous time signals into digital signals and discusses various methods to process the digital signals, such as filtering. The author uses {MATLAB} throughout as a user-friendly software tool to perform various digital signal processing algorithms and to simulate real-time systems. Readers learn how to convert analog signals into digital signals; how to process these signals using software or hardware; and how to write algorithms to perform useful operations on the acquired signals such as filtering, detecting digitally modulated signals, correcting channel distortions, et cetera Students are also shown how to convert {MATLAB} codes into firmware codes. Further, students will be able to apply the basic digital signal processing techniques in their workplace. The book is based on the author's popular online course at University of California, San Diego. Based on the author's popular online courses in signal processing at {UC} San Diego Presents an overview of signal processing, followed by easy to follow instructions without a myriad of complex mathematical derivations Enhanced by the use of {MATLAB} and a variety of exercises case studies, giving students hands-on experience in designing and processing signals in the digital domain},
	pagetotal = {1},
	publisher = {Springer International Publishing : Imprint: Springer},
	author = {Thyagarajan, K. S.},
	date = {2019},
	doi = {10.1007/978-3-319-76029-2},
	keywords = {Communications Engineering, Networks, Computer Communication Networks, Computer communication systems, Computers, Electrical engineering, Image processing, Information Systems and Communication Service, Signal processing, Signal, Image and Speech Processing, Speech processing systems},
}

@book{veloni_digital_2019,
	location = {Boca Raton, {FL}},
	title = {Digital and statistical signal processing},
	isbn = {978-0-429-01757-5 978-0-429-50752-6 978-0-429-01758-2},
	pagetotal = {1},
	publisher = {{CRC} Press, Taylor \& Francis Group},
	author = {Veloni, Anastasia and Miridakis, Nikolaos and Boukouvala, Erysso},
	date = {2019},
	keywords = {Digital techniques, Signal processing},
}

@misc{liu_separate_2022,
	title = {Separate What You Describe: Language-Queried Audio Source Separation},
	url = {http://arxiv.org/abs/2203.15147},
	shorttitle = {Separate What You Describe},
	abstract = {In this paper, we introduce the task of language-queried audio source separation ({LASS}), which aims to separate a target source from an audio mixture based on a natural language query of the target source (e.g., "a man tells a joke followed by people laughing"). A unique challenge in {LASS} is associated with the complexity of natural language description and its relation with the audio sources. To address this issue, we proposed {LASS}-Net, an end-to-end neural network that is learned to jointly process acoustic and linguistic information, and separate the target source that is consistent with the language query from an audio mixture. We evaluate the performance of our proposed system with a dataset created from the {AudioCaps} dataset. Experimental results show that {LASS}-Net achieves considerable improvements over baseline methods. Furthermore, we observe that {LASS}-Net achieves promising generalization results when using diverse human-annotated descriptions as queries, indicating its potential use in real-world scenarios. The separated audio samples and source code are available at https://liuxubo717.github.io/{LASS}-demopage.},
	number = {{arXiv}:2203.15147},
	publisher = {{arXiv}},
	author = {Liu, Xubo and Liu, Haohe and Kong, Qiuqiang and Mei, Xinhao and Zhao, Jinzheng and Huang, Qiushi and Plumbley, Mark D. and Wang, Wenwu},
	urldate = {2023-09-05},
	date = {2022-03-28},
	eprinttype = {arxiv},
	eprint = {2203.15147 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
}

@misc{tom_m_mitchell_need_1980,
	location = {New Brunswick, {NJ}},
	title = {The Need for Biases in Learning Generalizations},
	url = {https://www.cs.cmu.edu/~tom/pubs/NeedForBias_1980.pdf},
	author = {{Tom M. Mitchell}},
	date = {1980-05},
	langid = {english},
}

@misc{esser_taming_2021,
	title = {Taming Transformers for High-Resolution Image Synthesis},
	url = {http://arxiv.org/abs/2012.09841},
	abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to {CNNs}, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of {CNNs} with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use {CNNs} to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional {ImageNet}. Code and pretrained models can be found at https://github.com/{CompVis}/taming-transformers .},
	number = {{arXiv}:2012.09841},
	publisher = {{arXiv}},
	author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
	urldate = {2023-09-05},
	date = {2021-06-23},
	eprinttype = {arxiv},
	eprint = {2012.09841 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zagoruyko_wide_2017,
	title = {Wide Residual Networks},
	url = {http://arxiv.org/abs/1605.07146},
	abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of {ResNet} blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks ({WRNs}) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on {CIFAR}, {SVHN}, {COCO}, and significant improvements on {ImageNet}. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
	number = {{arXiv}:1605.07146},
	publisher = {{arXiv}},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	urldate = {2023-09-04},
	date = {2017-06-14},
	eprinttype = {arxiv},
	eprint = {1605.07146 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{dhariwal_diffusion_2021,
	title = {Diffusion Models Beat {GANs} on Image Synthesis},
	url = {http://arxiv.org/abs/2105.05233},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an {FID} of 2.97 on {ImageNet} 128\${\textbackslash}times\$128, 4.59 on {ImageNet} 256\${\textbackslash}times\$256, and 7.72 on {ImageNet} 512\${\textbackslash}times\$512, and we match {BigGAN}-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving {FID} to 3.94 on {ImageNet} 256\${\textbackslash}times\$256 and 3.85 on {ImageNet} 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
	number = {{arXiv}:2105.05233},
	publisher = {{arXiv}},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	urldate = {2023-09-04},
	date = {2021-06-01},
	eprinttype = {arxiv},
	eprint = {2105.05233 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{nichol_improved_2021,
	title = {Improved Denoising Diffusion Probabilistic Models},
	url = {http://arxiv.org/abs/2102.09672},
	abstract = {Denoising diffusion probabilistic models ({DDPM}) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, {DDPMs} can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well {DDPMs} and {GANs} cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
	number = {{arXiv}:2102.09672},
	publisher = {{arXiv}},
	author = {Nichol, Alex and Dhariwal, Prafulla},
	urldate = {2023-09-04},
	date = {2021-02-18},
	eprinttype = {arxiv},
	eprint = {2102.09672 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{orland_math_2020,
	location = {Shelter Island, {NY}},
	title = {Math for programmers: 3D graphics, machine learning and simulations with Python},
	isbn = {978-1-61729-535-5},
	shorttitle = {Math for programmers},
	pagetotal = {655},
	publisher = {Manning},
	author = {Orland, Paul},
	date = {2020},
}

@book{deisenroth_mathematics_2020,
	location = {Cambridge ; New York, {NY}},
	title = {Mathematics for machine learning},
	isbn = {978-1-108-67993-0},
	abstract = {"The fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability, and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efficiently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models, and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the first time, the methods help build intuition and practical experience with applying mathematical concepts"--},
	pagetotal = {1},
	publisher = {Cambridge University Press},
	author = {Deisenroth, Marc Peter and Faisal, A. Aldo and Ong, Cheng Soon},
	date = {2020},
	keywords = {Machine learning, Mathematics},
}

@video{machine_learning_at_berkeley_diffusion_2022,
	location = {Berkeley},
	title = {Diffusion Models},
	volume = {12},
	url = {https://www.youtube.com/watch?v=687zEGODmHA},
	shorttitle = {{CS} 198-126},
	abstract = {Lecture 12 - Diffusion Models
{CS} 198-126: Modern Computer Vision and Deep Learning
University of California, Berkele},
	author = {{Machine Learning at Berkeley}},
	urldate = {2023-09-03},
	date = {2022-12-03},
	langid = {english},
}

@misc{neal_annealed_1998,
	title = {Annealed Importance Sampling},
	url = {http://arxiv.org/abs/physics/9803008},
	abstract = {Simulated annealing - moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions - has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers.},
	number = {{arXiv}:physics/9803008},
	publisher = {{arXiv}},
	author = {Neal, Radford M.},
	urldate = {2023-09-03},
	date = {1998-09-04},
	eprinttype = {arxiv},
	eprint = {physics/9803008},
	keywords = {Physics - Computational Physics, Physics - Data Analysis, Statistics and Probability},
}

@article{jarzynski_equilibrium_1997,
	title = {Equilibrium free energy differences from nonequilibrium measurements: a master equation approach},
	volume = {56},
	issn = {1063-651X, 1095-3787},
	url = {http://arxiv.org/abs/cond-mat/9707325},
	doi = {10.1103/PhysRevE.56.5018},
	shorttitle = {Equilibrium free energy differences from nonequilibrium measurements},
	abstract = {It has recently been shown that the Helmholtz free energy difference between two equilibrium configurations of a system may be obtained from an ensemble of finite-time (nonequilibrium) measurements of the work performed in switching an external parameter of the system. Here this result is established, as an identity, within the master equation formalism. Examples are discussed and numerical illustrations provided.},
	pages = {5018--5035},
	number = {5},
	journaltitle = {Physical Review E},
	shortjournal = {Phys. Rev. E},
	author = {Jarzynski, C.},
	urldate = {2023-09-03},
	date = {1997-11-01},
	eprinttype = {arxiv},
	eprint = {cond-mat/9707325},
	keywords = {Condensed Matter - Statistical Mechanics, Physics - Chemical Physics},
}

@misc{goodfellow_generative_2014,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	number = {{arXiv}:1406.2661},
	publisher = {{arXiv}},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2023-08-20},
	date = {2014-06-10},
	eprinttype = {arxiv},
	eprint = {1406.2661 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@online{noauthor_home_nodate,
	title = {Home},
	url = {https://juce.com/},
	abstract = {{JUCE} is the most widely used framework for audio application and plug-in development on Windows, {macOS}, Linux, {iOS}, Android, {VST}, {VST}3, {AU}, {AUv}3, {AAX} and {LV}2 plug-ins.},
	titleaddon = {{JUCE}},
	urldate = {2023-08-19},
	langid = {british},
}

@online{noauthor_juce_nodate,
	title = {{JUCE}: sampler},
	url = {https://docs.juce.com/master/group__juce__audio__formats-sampler.html},
	urldate = {2023-08-19},
}

@misc{raffel_exploring_2020,
	title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing ({NLP}). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for {NLP} by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for {NLP}, we release our data set, pre-trained models, and code.},
	number = {{arXiv}:1910.10683},
	publisher = {{arXiv}},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	urldate = {2023-08-19},
	date = {2020-07-28},
	eprinttype = {arxiv},
	eprint = {1910.10683 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@video{espen_kraft_akai_2018,
	title = {Akai S700 Sampler - Pure magic for next to nothing!},
	url = {https://www.youtube.com/watch?v=uLZ3_XKqYFw},
	abstract = {My first sampler (Akai X7000) in module form.  With one of the more warm and creamy analog filters of any sampler. Even if the filter is not resonant and even if the sampler has no {ADSR} envelopes, just a release, it has a nice deep low end and a magic sound to it.
It can be had for next to nothing these days and well worth picking up and use. In fact it should cost a lot more based on sound only.
I judge a samplers filter based on how it sound on a synth string type of sound so you'll hear no beats in this demo. ;-)},
	author = {{Espen Kraft}},
	urldate = {2023-08-19},
	date = {2018-11-16},
	langid = {english},
}

@misc{kong_panns_2020,
	title = {{PANNs}: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition},
	url = {http://arxiv.org/abs/1912.10211},
	shorttitle = {{PANNs}},
	abstract = {Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classification, music classification, speech emotion classification and sound event detection. Recently, neural networks have been applied to tackle audio pattern recognition problems. However, previous systems are built on specific datasets with limited durations. Recently, in computer vision and natural language processing, systems pretrained on large-scale datasets have generalized well to several tasks. However, there is limited research on pretraining systems on large-scale datasets for audio pattern recognition. In this paper, we propose pretrained audio neural networks ({PANNs}) trained on the large-scale {AudioSet} dataset. These {PANNs} are transferred to other audio related tasks. We investigate the performance and computational complexity of {PANNs} modeled by a variety of convolutional neural networks. We propose an architecture called Wavegram-Logmel-{CNN} using both log-mel spectrogram and waveform as input feature. Our best {PANN} system achieves a state-of-the-art mean average precision ({mAP}) of 0.439 on {AudioSet} tagging, outperforming the best previous system of 0.392. We transfer {PANNs} to six audio pattern recognition tasks, and demonstrate state-of-the-art performance in several of those tasks. We have released the source code and pretrained models of {PANNs}: https://github.com/qiuqiangkong/audioset\_tagging\_cnn.},
	number = {{arXiv}:1912.10211},
	publisher = {{arXiv}},
	author = {Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D.},
	urldate = {2023-08-19},
	date = {2020-08-23},
	eprinttype = {arxiv},
	eprint = {1912.10211 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{chen_hts-at_2022,
	title = {{HTS}-{AT}: A Hierarchical Token-Semantic Audio Transformer for Sound Classification and Detection},
	url = {http://arxiv.org/abs/2202.00874},
	shorttitle = {{HTS}-{AT}},
	abstract = {Audio classification is an important task of mapping audio samples into their corresponding labels. Recently, the transformer model with self-attention mechanisms has been adopted in this field. However, existing audio transformers require large {GPU} memories and long training time, meanwhile relying on pretrained vision models to achieve high performance, which limits the model's scalability in audio tasks. To combat these problems, we introduce {HTS}-{AT}: an audio transformer with a hierarchical structure to reduce the model size and training time. It is further combined with a token-semantic module to map final outputs into class featuremaps, thus enabling the model for the audio event detection (i.e. localization in time). We evaluate {HTS}-{AT} on three datasets of audio classification where it achieves new state-of-the-art ({SOTA}) results on {AudioSet} and {ESC}-50, and equals the {SOTA} on Speech Command V2. It also achieves better performance in event localization than the previous {CNN}-based models. Moreover, {HTS}-{AT} requires only 35\% model parameters and 15\% training time of the previous audio transformer. These results demonstrate the high performance and high efficiency of {HTS}-{AT}.},
	number = {{arXiv}:2202.00874},
	publisher = {{arXiv}},
	author = {Chen, Ke and Du, Xingjian and Zhu, Bilei and Ma, Zejun and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},
	urldate = {2023-08-19},
	date = {2022-02-01},
	eprinttype = {arxiv},
	eprint = {2202.00874 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{agarap_deep_2019,
	title = {Deep Learning using Rectified Linear Units ({ReLU})},
	url = {http://arxiv.org/abs/1803.08375},
	abstract = {We introduce the use of rectified linear units ({ReLU}) as the classification function in a deep neural network ({DNN}). Conventionally, {ReLU} is used as an activation function in {DNNs}, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer \$h\_\{n - 1\}\$ in a neural network, then multiply it by weight parameters \${\textbackslash}theta\$ to get the raw scores \$o\_\{i\}\$. Afterwards, we threshold the raw scores \$o\_\{i\}\$ by \$0\$, i.e. \$f(o) = {\textbackslash}max(0, o\_\{i\})\$, where \$f(o)\$ is the {ReLU} function. We provide class predictions \${\textbackslash}hat\{y\}\$ through argmax function, i.e. argmax \$f(x)\$.},
	number = {{arXiv}:1803.08375},
	publisher = {{arXiv}},
	author = {Agarap, Abien Fred},
	urldate = {2023-08-19},
	date = {2019-02-07},
	eprinttype = {arxiv},
	eprint = {1803.08375 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{drossos_clotho_2019,
	title = {Clotho: An Audio Captioning Dataset},
	url = {http://arxiv.org/abs/1910.09387},
	shorttitle = {Clotho},
	abstract = {Audio captioning is the novel task of general audio content description using free text. It is an intermodal translation task (not speech-to-text), where a system accepts as an input an audio signal and outputs the textual description (i.e. the caption) of that signal. In this paper we present Clotho, a dataset for audio captioning consisting of 4981 audio samples of 15 to 30 seconds duration and 24 905 captions of eight to 20 words length, and a baseline method to provide initial results. Clotho is built with focus on audio content and caption diversity, and the splits of the data are not hampering the training or evaluation of methods. All sounds are from the Freesound platform, and captions are crowdsourced using Amazon Mechanical Turk and annotators from English speaking countries. Unique words, named entities, and speech transcription are removed with post-processing. Clotho is freely available online (https://zenodo.org/record/3490684).},
	number = {{arXiv}:1910.09387},
	publisher = {{arXiv}},
	author = {Drossos, Konstantinos and Lipping, Samuel and Virtanen, Tuomas},
	urldate = {2023-08-19},
	date = {2019-10-21},
	eprinttype = {arxiv},
	eprint = {1910.09387 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@online{noauthor_logic_nodate,
	title = {Logic Pro for Mac},
	url = {https://www.apple.com/logic-pro/},
	abstract = {Logic Pro is a complete professional recording studio on the Mac. And it has everything musicians need to go from first note to final master.},
	titleaddon = {Apple},
	urldate = {2023-08-14},
	langid = {american},
}

@online{noauthor_ableton_nodate,
	title = {Ableton},
	url = {https://www.ableton.com/},
	abstract = {Ableton entwickelt Software, Hardware und kreative Ressourcen für eine weltweite Community von Musikschaffenden.},
	urldate = {2023-08-14},
	langid = {german},
}

@online{noauthor_creative_nodate,
	title = {Creative tools for music makers {\textbar} Ableton},
	url = {https://www.ableton.com/en/},
	abstract = {Ableton makes software, hardware and other creative tools for a global community of music makers.},
	urldate = {2023-08-14},
	langid = {english},
}

@misc{kong_hifi-gan_2020,
	title = {{HiFi}-{GAN}: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis},
	url = {http://arxiv.org/abs/2010.05646},
	shorttitle = {{HiFi}-{GAN}},
	abstract = {Several recent work on speech synthesis have employed generative adversarial networks ({GANs}) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose {HiFi}-{GAN}, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, {MOS}) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 {kHz} high-fidelity audio 167.9 times faster than real-time on a single V100 {GPU}. We further show the generality of {HiFi}-{GAN} to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of {HiFi}-{GAN} generates samples 13.4 times faster than real-time on {CPU} with comparable quality to an autoregressive counterpart.},
	number = {{arXiv}:2010.05646},
	publisher = {{arXiv}},
	author = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
	urldate = {2023-08-14},
	date = {2020-10-23},
	eprinttype = {arxiv},
	eprint = {2010.05646 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-Encoding Variational Bayes},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	number = {{arXiv}:1312.6114},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2023-08-14},
	date = {2022-12-10},
	eprinttype = {arxiv},
	eprint = {1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{rombach_high-resolution_2022,
	title = {High-Resolution Image Synthesis with Latent Diffusion Models},
	url = {http://arxiv.org/abs/2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models ({DMs}) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful {DMs} often consumes hundreds of {GPU} days and inference is expensive due to sequential evaluations. To enable {DM} training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models ({LDMs}) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based {DMs}. Code is available at https://github.com/{CompVis}/latent-diffusion .},
	number = {{arXiv}:2112.10752},
	publisher = {{arXiv}},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	urldate = {2023-08-14},
	date = {2022-04-13},
	eprinttype = {arxiv},
	eprint = {2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
	url = {http://arxiv.org/abs/1907.11692},
	shorttitle = {{RoBERTa}},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of {BERT} pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that {BERT} was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on {GLUE}, {RACE} and {SQuAD}. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	number = {{arXiv}:1907.11692},
	publisher = {{arXiv}},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	urldate = {2023-08-14},
	date = {2019-07-26},
	eprinttype = {arxiv},
	eprint = {1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{dai_why_2023,
	title = {Why Can {GPT} Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers},
	url = {http://arxiv.org/abs/2212.10559},
	shorttitle = {Why Can {GPT} Learn In-Context?},
	abstract = {Large pretrained language models have shown surprising in-context learning ({ICL}) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand {ICL} as follows: {GPT} first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original {GPT} to build an {ICL} model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at {\textbackslash}url\{https://aka.ms/icl\}.},
	number = {{arXiv}:2212.10559},
	publisher = {{arXiv}},
	author = {Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
	urldate = {2023-08-13},
	date = {2023-05-15},
	eprinttype = {arxiv},
	eprint = {2212.10559 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{chung_scaling_2022,
	title = {Scaling Instruction-Finetuned Language Models},
	url = {http://arxiv.org/abs/2210.11416},
	abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes ({PaLM}, T5, U-{PaLM}), prompting setups (zero-shot, few-shot, {CoT}), and evaluation benchmarks ({MMLU}, {BBH}, {TyDiQA}, {MGSM}, open-ended generation). For instance, Flan-{PaLM} 540B instruction-finetuned on 1.8K tasks outperforms {PALM} 540B by a large margin (+9.4\% on average). Flan-{PaLM} 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot {MMLU}. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as {PaLM} 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
	number = {{arXiv}:2210.11416},
	publisher = {{arXiv}},
	author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
	urldate = {2023-08-13},
	date = {2022-12-06},
	eprinttype = {arxiv},
	eprint = {2210.11416 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{kim_audiocaps_2019,
	location = {Minneapolis, Minnesota},
	title = {{AudioCaps}: Generating Captions for Audios in The Wild},
	url = {https://aclanthology.org/N19-1011},
	doi = {10.18653/v1/N19-1011},
	shorttitle = {{AudioCaps}},
	abstract = {We explore the problem of Audio Captioning: generating natural language description for any kind of audio in the wild, which has been surprisingly unexplored in previous research. We contribute a large-scale dataset of 46K audio clips with human-written text pairs collected via crowdsourcing on the {AudioSet} dataset. Our thorough empirical studies not only show that our collected captions are indeed faithful to audio inputs but also discover what forms of audio representation and captioning models are effective for the audio captioning. From extensive experiments, we also propose two novel components that help improve audio captioning performance: the top-down multi-scale encoder and aligned semantic attention.},
	eventtitle = {{NAACL}-{HLT} 2019},
	pages = {119--132},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Chris Dongjoo and Kim, Byeongchang and Lee, Hyunmin and Kim, Gunhee},
	urldate = {2023-08-12},
	date = {2019-06},
}

@inproceedings{gemmeke_audio_2017,
	location = {New Orleans, {LA}},
	title = {Audio Set: An ontology and human-labeled dataset for audio events},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7952261/},
	doi = {10.1109/ICASSP.2017.7952261},
	shorttitle = {Audio Set},
	eventtitle = {2017 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {776--780},
	booktitle = {2017 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	publisher = {{IEEE}},
	author = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
	urldate = {2023-08-12},
	date = {2017-03},
}

@misc{kumar_melgan_2019,
	title = {{MelGAN}: Generative Adversarial Networks for Conditional Waveform Synthesis},
	url = {http://arxiv.org/abs/1910.06711},
	shorttitle = {{MelGAN}},
	abstract = {Previous works (Donahue et al., 2018a; Engel et al., 2019a) have found that generating coherent raw audio waveforms with {GANs} is challenging. In this paper, we show that it is possible to train {GANs} reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. Subjective evaluation metric (Mean Opinion Score, or {MOS}) shows the effectiveness of the proposed approach for high quality mel-spectrogram inversion. To establish the generality of the proposed techniques, we show qualitative results of our model in speech synthesis, music domain translation and unconditional music synthesis. We evaluate the various components of the model through ablation studies and suggest a set of guidelines to design general purpose discriminators and generators for conditional sequence synthesis tasks. Our model is non-autoregressive, fully convolutional, with significantly fewer parameters than competing models and generalizes to unseen speakers for mel-spectrogram inversion. Our pytorch implementation runs at more than 100x faster than realtime on {GTX} 1080Ti {GPU} and more than 2x faster than real-time on {CPU}, without any hardware specific optimization tricks.},
	number = {{arXiv}:1910.06711},
	publisher = {{arXiv}},
	author = {Kumar, Kundan and Kumar, Rithesh and de Boissiere, Thibault and Gestin, Lucas and Teoh, Wei Zhen and Sotelo, Jose and de Brebisson, Alexandre and Bengio, Yoshua and Courville, Aaron},
	urldate = {2023-08-12},
	date = {2019-12-08},
	eprinttype = {arxiv},
	eprint = {1910.06711 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{austin_structured_2023,
	title = {Structured Denoising Diffusion Models in Discrete State-Spaces},
	url = {http://arxiv.org/abs/2107.03006},
	abstract = {Denoising diffusion probabilistic models ({DDPMs}) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on {LM}1B. On the image dataset {CIFAR}-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space {DDPM} model.},
	number = {{arXiv}:2107.03006},
	publisher = {{arXiv}},
	author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and Berg, Rianne van den},
	urldate = {2023-08-11},
	date = {2023-02-22},
	eprinttype = {arxiv},
	eprint = {2107.03006 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
	url = {http://arxiv.org/abs/1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	number = {{arXiv}:1503.03585},
	publisher = {{arXiv}},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	urldate = {2023-08-11},
	date = {2015-11-18},
	eprinttype = {arxiv},
	eprint = {1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@misc{oord_neural_2018,
	title = {Neural Discrete Representation Learning},
	url = {http://arxiv.org/abs/1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational {AutoEncoder} ({VQ}-{VAE}), differs from {VAEs} in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation ({VQ}). Using the {VQ} method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the {VAE} framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	number = {{arXiv}:1711.00937},
	publisher = {{arXiv}},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	urldate = {2023-08-10},
	date = {2018-05-30},
	eprinttype = {arxiv},
	eprint = {1711.00937 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{liu_conditional_2021,
	title = {Conditional Sound Generation Using Neural Discrete Time-Frequency Representation Learning},
	url = {http://arxiv.org/abs/2107.09998},
	abstract = {Deep generative models have recently achieved impressive performance in speech and music synthesis. However, compared to the generation of those domain-specific sounds, generating general sounds (such as siren, gunshots) has received less attention, despite their wide applications. In previous work, the {SampleRNN} method was considered for sound generation in the time domain. However, {SampleRNN} is potentially limited in capturing long-range dependencies within sounds as it only back-propagates through a limited number of samples. In this work, we propose a method for generating sounds via neural discrete time-frequency representation learning, conditioned on sound classes. This offers an advantage in efficiently modelling long-range dependencies and retaining local fine-grained structures within sound clips. We evaluate our approach on the {UrbanSound}8K dataset, compared to {SampleRNN}, with the performance metrics measuring the quality and diversity of generated sounds. Experimental results show that our method offers comparable performance in quality and significantly better performance in diversity.},
	number = {{arXiv}:2107.09998},
	publisher = {{arXiv}},
	author = {Liu, Xubo and Iqbal, Turab and Zhao, Jinzheng and Huang, Qiushi and Plumbley, Mark D. and Wang, Wenwu},
	urldate = {2023-08-10},
	date = {2021-10-06},
	eprinttype = {arxiv},
	eprint = {2107.09998 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{iashin_taming_2021,
	title = {Taming Visually Guided Sound Generation},
	url = {http://arxiv.org/abs/2110.08791},
	abstract = {Recent advances in visually-induced audio generation are based on sampling short, low-fidelity, and one-class sounds. Moreover, sampling 1 second of audio from the state-of-the-art model takes minutes on a high-end {GPU}. In this work, we propose a single model capable of generating visually relevant, high-fidelity sounds prompted with a set of frames from open-domain videos in less time than it takes to play it on a single {GPU}. We train a transformer to sample a new spectrogram from the pre-trained spectrogram codebook given the set of video features. The codebook is obtained using a variant of {VQGAN} trained to produce a compact sampling space with a novel spectrogram-based perceptual loss. The generated spectrogram is transformed into a waveform using a window-based {GAN} that significantly speeds up generation. Considering the lack of metrics for automatic evaluation of generated spectrograms, we also build a family of metrics called {FID} and {MKL}. These metrics are based on a novel sound classifier, called Melception, and designed to evaluate the fidelity and relevance of open-domain samples. Both qualitative and quantitative studies are conducted on small- and large-scale datasets to evaluate the fidelity and relevance of generated samples. We also compare our model to the state-of-the-art and observe a substantial improvement in quality, size, and computation time. Code, demo, and samples: v-iashin.github.io/{SpecVQGAN}},
	number = {{arXiv}:2110.08791},
	publisher = {{arXiv}},
	author = {Iashin, Vladimir and Rahtu, Esa},
	urldate = {2023-08-10},
	date = {2021-10-17},
	eprinttype = {arxiv},
	eprint = {2110.08791 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{radford_learning_2021,
	title = {Learning Transferable Visual Models From Natural Language Supervision},
	url = {http://arxiv.org/abs/2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn {SOTA} image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as {OCR}, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original {ResNet}-50 on {ImageNet} zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/{OpenAI}/{CLIP}.},
	number = {{arXiv}:2103.00020},
	publisher = {{arXiv}},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	urldate = {2023-08-10},
	date = {2021-02-26},
	eprinttype = {arxiv},
	eprint = {2103.00020 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2023-08-10},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{chung_w2v-bert_2021,
	title = {W2v-{BERT}: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training},
	url = {http://arxiv.org/abs/2108.06209},
	shorttitle = {W2v-{BERT}},
	abstract = {Motivated by the success of masked language modeling{\textasciitilde}({MLM}) in pre-training natural language processing models, we propose w2v-{BERT} that explores {MLM} for self-supervised speech representation learning. w2v-{BERT} is a framework that combines contrastive learning and {MLM}, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing {MLM}-based speech pre-training frameworks such as {HuBERT}, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-{BERT} can be optimized in an end-to-end fashion by solving the two self-supervised tasks{\textasciitilde}(the contrastive task and {MLM}) simultaneously. Our experiments show that w2v-{BERT} achieves competitive results compared to current state-of-the-art pre-trained models on the {LibriSpeech} benchmarks when using the Libri-Light{\textasciitilde}60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec{\textasciitilde}2.0 and {HuBERT}, our model shows{\textasciitilde}5{\textbackslash}\% to{\textasciitilde}10{\textbackslash}\% relative {WER} reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-{BERT} outperforms our internal conformer-based wav2vec{\textasciitilde}2.0 by more than{\textasciitilde}30{\textbackslash}\% relatively.},
	number = {{arXiv}:2108.06209},
	publisher = {{arXiv}},
	author = {Chung, Yu-An and Zhang, Yu and Han, Wei and Chiu, Chung-Cheng and Qin, James and Pang, Ruoming and Wu, Yonghui},
	urldate = {2023-08-07},
	date = {2021-09-13},
	eprinttype = {arxiv},
	eprint = {2108.06209 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{zeghidour_soundstream_2021,
	title = {{SoundStream}: An End-to-End Neural Audio Codec},
	url = {http://arxiv.org/abs/2107.03312},
	shorttitle = {{SoundStream}},
	abstract = {We present {SoundStream}, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. {SoundStream} relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3kbps to 18kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone {CPU}. In subjective evaluations using audio at 24kHz sampling rate, {SoundStream} at 3kbps outperforms Opus at 12kbps and approaches {EVS} at 9.6kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.},
	number = {{arXiv}:2107.03312},
	publisher = {{arXiv}},
	author = {Zeghidour, Neil and Luebs, Alejandro and Omran, Ahmed and Skoglund, Jan and Tagliasacchi, Marco},
	urldate = {2023-08-07},
	date = {2021-07-07},
	eprinttype = {arxiv},
	eprint = {2107.03312 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{yang_diffsound_2023,
	title = {Diffsound: Discrete Diffusion Model for Text-to-sound Generation},
	url = {http://arxiv.org/abs/2207.09983},
	shorttitle = {Diffsound},
	abstract = {Generating sound effects that humans want is an important topic. However, there are few studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder ({VQ}-{VAE}), a decoder, and a vocoder. The framework first uses the decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of {VQ}-{VAE}, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the decoder significantly influences the generation performance. Thus, we focus on designing a good decoder in this study. We begin with the traditional autoregressive decoder, which has been proved as a state-of-the-art method in previous sound generation works. However, the {AR} decoder always predicts the mel-spectrogram tokens one by one in order, which introduces the unidirectional bias and accumulation of errors problems. Moreover, with the {AR} decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by {AR} decoders, we propose a non-autoregressive decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained after several steps. Our experiments show that our proposed Diffsound not only produces better text-to-sound generation results when compared with the {AR} decoder but also has a faster generation speed, e.g., {MOS}: 3.56 {\textbackslash}textit\{v.s\} 2.786, and the generation speed is five times faster than the {AR} decoder.},
	number = {{arXiv}:2207.09983},
	publisher = {{arXiv}},
	author = {Yang, Dongchao and Yu, Jianwei and Wang, Helin and Wang, Wen and Weng, Chao and Zou, Yuexian and Yu, Dong},
	urldate = {2023-08-04},
	date = {2023-04-28},
	eprinttype = {arxiv},
	eprint = {2207.09983 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{kreuk_audiogen_2023,
	title = {{AudioGen}: Textually Guided Audio Generation},
	url = {http://arxiv.org/abs/2209.15352},
	shorttitle = {{AudioGen}},
	abstract = {We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose {AaudioGen}, an auto-regressive generative model that generates audio samples conditioned on text inputs. {AudioGen} operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, {AudioGen} outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: https://felixkreuk.github.io/audiogen},
	number = {{arXiv}:2209.15352},
	publisher = {{arXiv}},
	author = {Kreuk, Felix and Synnaeve, Gabriel and Polyak, Adam and Singer, Uriel and Défossez, Alexandre and Copet, Jade and Parikh, Devi and Taigman, Yaniv and Adi, Yossi},
	urldate = {2023-08-04},
	date = {2023-03-05},
	eprinttype = {arxiv},
	eprint = {2209.15352 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{ghosal_text--audio_2023,
	title = {Text-to-Audio Generation using Instruction-Tuned {LLM} and Latent Diffusion Model},
	url = {http://arxiv.org/abs/2304.13731},
	abstract = {The immense scale of the recent large language models ({LLM}) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing ({NLP}) tasks. Inspired by such successes, we adopt such an instruction-tuned {LLM} Flan-T5 as the text encoder for text-to-audio ({TTA}) generation -- a task where the goal is to generate an audio from its textual description. The prior works on {TTA} either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5. Consequently, our latent diffusion model ({LDM})-based approach {TANGO} outperforms the state-of-the-art {AudioLDM} on most metrics and stays comparable on the rest on {AudioCaps} test set, despite training the {LDM} on a 63 times smaller dataset and keeping the text encoder frozen. This improvement might also be attributed to the adoption of audio pressure level-based sound mixing for training set augmentation, whereas the prior methods take a random mix.},
	number = {{arXiv}:2304.13731},
	publisher = {{arXiv}},
	author = {Ghosal, Deepanway and Majumder, Navonil and Mehrish, Ambuj and Poria, Soujanya},
	urldate = {2023-08-04},
	date = {2023-05-29},
	eprinttype = {arxiv},
	eprint = {2304.13731 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@online{team_gradio_gradio_nodate,
	title = {Gradio},
	url = {https://gradio.app},
	abstract = {Build \& Share Delightful Machine Learning Apps},
	author = {Team Gradio},
	urldate = {2023-08-03},
	langid = {english},
}

@software{barney_hill_vroomai_2023,
	title = {{VroomAI}},
	rights = {Apache-2.0},
	url = {https://github.com/vroomai/vst},
	abstract = {Generate sounds from words. Directly in your {DAW}.},
	publisher = {vroomai},
	author = {{Barney Hill}},
	urldate = {2023-07-28},
	date = {2023-07-13},
	note = {original-date: 2023-03-24T23:58:48Z},
	keywords = {audio, generative-art, machine-learning, vst, vst3},
}

@online{qosmo_neutone_nodate,
	title = {Neutone by Qosmo},
	url = {https://neutone.space/},
	abstract = {Neutone makes {AI} technologies accessible for any audio creators to experiment with. Our transformative {AI} audio instruments will spark endless possibilities for creators/performers to discover and produce music of the future.},
	titleaddon = {Neutone by Qosmo},
	author = {{Qosmo}},
	urldate = {2023-07-28},
	langid = {english},
}

@online{google_ai_nsynth_2017,
	title = {{NSynth}: Neural Audio Synthesis},
	url = {https://magenta.tensorflow.org/nsynth},
	shorttitle = {{NSynth}},
	abstract = {One of the goals of Magenta is to use machine learning to develop new avenues of human expression. And so today we are proud to announce {NSynth} (Neural Synth...},
	titleaddon = {Magenta},
	author = {{Google AI}},
	urldate = {2023-07-28},
	date = {2017-04-06},
	langid = {english},
}

@misc{engel_neural_2017,
	title = {Neural Audio Synthesis of Musical Notes with {WaveNet} Autoencoders},
	url = {http://arxiv.org/abs/1704.01279},
	abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new {WaveNet}-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce {NSynth}, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using {NSynth}, we demonstrate improved qualitative and quantitative performance of the {WaveNet} autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
	number = {{arXiv}:1704.01279},
	publisher = {{arXiv}},
	author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
	urldate = {2023-07-28},
	date = {2017-04-05},
	eprinttype = {arxiv},
	eprint = {1704.01279 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound},
}

@online{midi_association_midi_nodate,
	title = {{MIDI}},
	url = {https://www.midi.org/},
	titleaddon = {{MIDI}},
	author = {{MIDI Association}},
	urldate = {2023-07-28},
}

@book{russ_sound_2009,
	location = {Amsterdam},
	edition = {3. ed},
	title = {Sound synthesis and sampling},
	isbn = {978-0-240-52105-3},
	pagetotal = {559},
	publisher = {Elsevier, Focal Press},
	author = {Russ, Martin},
	date = {2009},
}

@online{young_gale_hugh_2013,
	title = {Hugh Le Caine},
	url = {https://www.thecanadianencyclopedia.ca/en/article/hugh-le-caine-emc},
	titleaddon = {The Canadian Encyclopedia},
	author = {{Young Gale} and {Ford Clifford}},
	urldate = {2023-07-25},
	date = {2013-12-16},
}

@video{haohe_liu_audioldm_2023,
	title = {{AudioLDM}: Text-to-Audio Generation with Latent Diffusion Models},
	volume = {40},
	url = {https://www.youtube.com/watch?v=6qtL9_T8m3c},
	abstract = {{AudioLDM}: Text-to-Audio Generation with Latent Diffusion Models [Haohe Liu, University of Surrey]},
	author = {{Haohe Liu}},
	urldate = {2023-07-25},
	date = {2023-03-27},
	langid = {english},
}

@misc{wu_large-scale_2023,
	title = {Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation},
	url = {http://arxiv.org/abs/2211.06687},
	abstract = {Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release {LAION}-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. {LAION}-Audio-630K and the proposed model are both available to the public.},
	number = {{arXiv}:2211.06687},
	publisher = {{arXiv}},
	author = {Wu, Yusong and Chen, Ke and Zhang, Tianyu and Hui, Yuchen and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},
	urldate = {2023-07-18},
	date = {2023-04-07},
	eprinttype = {arxiv},
	eprint = {2211.06687 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@online{dudenredaktion_synthesizer_nodate,
	title = {Synthesizer auf Duden online},
	url = {https://www.duden.de/rechtschreibung/Synthesizer},
	abstract = {Definition, Rechtschreibung, Synonyme und Grammatik von 'Synthesizer' auf Duden online nachschlagen ✔️ Wörterbuch der deutschen Sprache.},
	titleaddon = {Duden online},
	author = {Dudenredaktion},
	urldate = {2023-07-16},
	langid = {german},
}

@online{walz_plugin_nodate,
	title = {Plugin Gui Magic},
	url = {https://foleysfinest.com/developer/pluginguimagic/},
	abstract = {{PluginGuiMagic} allows you to design your audio plugin {GUI} with mouseclicks at runtime},
	author = {Walz, Daniel},
	urldate = {2023-07-15},
	langid = {english},
}

@online{noauthor_huggingface_nodate,
	title = {Huggingface {AudioLDM} Pipeline},
	url = {https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	titleaddon = {Huggingface},
	urldate = {2023-07-16},
}

@online{noauthor_fastapi_nodate,
	title = {{FastAPI}},
	url = {https://fastapi.tiangolo.com/},
	abstract = {{FastAPI} framework, high performance, easy to learn, fast to code, ready for production},
	urldate = {2023-07-16},
	langid = {english},
}

@online{noauthor_torchscript_nodate,
	title = {{TorchScript} — {PyTorch} 2.0 documentation},
	url = {https://pytorch.org/docs/stable/jit.html},
	urldate = {2023-07-16},
}

@video{oli_larkin_machine_2023,
	title = {Machine Learning Audio Plug-ins with {iPlug}2 and {ONNX} Runtime},
	url = {https://www.youtube.com/watch?v=t662qg12f_Y},
	abstract = {Oli Larkin is the lead developer of the {iPlug}2 plug-in framework and a software engineer at Ableton. Oli will introduce the latest {iPlug}2 developments and show how it can be used with Microsoft’s Onnx runtime to perform inference on models exported from {ML} frameworks such as Tensorflow or {PyTorch}.},
	publisher = {The Audio Programmer},
	author = {{Oli Larkin}},
	urldate = {2023-07-16},
	date = {2023-04-12},
	langid = {english},
}

@online{noauthor_onnx_nodate,
	title = {{ONNX} Runtime},
	url = {https://onnxruntime.ai/},
	titleaddon = {{ONNX} Runtime},
	urldate = {2023-07-16},
}

@online{noauthor_onnx_nodate-1,
	title = {{ONNX}},
	url = {https://onnx.ai/},
	abstract = {{ONNX} is an open format built to represent machine learning models. {ONNX} defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable {AI} developers to use models with a variety of frameworks, tools, runtimes, and compilers.},
	titleaddon = {{ONNX}},
	urldate = {2023-07-16},
	langid = {english},
}

@book{meyers_effective_2014,
	location = {Beijing ; Sebastopol, {CA}},
	edition = {First edition},
	title = {Effective modern C++: 42 specific ways to improve your use of C++11 and C++14},
	isbn = {978-1-4919-0399-5},
	shorttitle = {Effective modern C++},
	abstract = {"Coming to grips with C++11 and C++14 is more than a matter of familiarizing yourself with the features they introduce (e.g., auto type declarations, move semantics, lambda expressions, and concurrency support). The challenge is learning to use those features effectively -- so that your software is correct, efficient, maintainable, and portable. That's where this practical book comes in. It describes how to write truly great software using C++11 and C++14 -- i.e. using modern C++ ...Effective Modern C++ follows the proven guideline-based, example-driven format of Scott Meyers' earlier books, but covers entirely new material"--Publisher's website},
	pagetotal = {315},
	publisher = {O'Reilly Media},
	author = {Meyers, Scott},
	date = {2014},
	note = {{OCLC}: ocn884480640},
	keywords = {C++ (Computer program language), Langages de programmation, Programmation informatique},
}

@book{stroustrup_c_1997,
	location = {Reading, Mass},
	edition = {3rd ed},
	title = {The C++ programming language},
	isbn = {978-0-201-88954-3},
	pagetotal = {910},
	publisher = {Addison-Wesley},
	author = {Stroustrup, Bjarne},
	date = {1997},
	keywords = {C++ (Computer program language)},
}

@unpublished{doumler_c_2015,
	location = {{CppCon}},
	title = {C++ in the Audio Industry},
	url = {https://www.youtube.com/watch?v=boPEO2auJj4},
	abstract = {Sound is an essential medium for human-computer interaction and vital for applications such as games and music production software. In the audio industry, C++ is the dominating programming language. This talk provides an insight into the patterns and tools that C++ developers in the audio industry rely on. There are interesting lessons to be learned from this domain that can be useful to every C++ developer.

Handling audio in real time presents interesting technical challenges. Techniques also used in other C++ domains have to be combined: real-time multithreading, lock-free programming, efficient {DSP}, {SIMD}, and low-latency hardware communication. C++ is the language of choice to tie all these requirements together. Clever leveraging of advanced C++ techniques, template metaprogramming, and the new C++11/14 standard makes these tasks more exciting than ever.
— 
Timur Doumler is Senior Software Developer at London-based technology company {ROLI}. He is working with Julian Storer to further develop {JUCE}, the leading cross-platform framework for creating audio applications that is used by hundreds of companies in the audio industry.

After five years of writing high-performance code in Fortran, C, and C++ for numerical simulations of the cosmic structure formation, Timur became committed to audio and music production software. Before joining {ROLI}, he worked on various projects at market-leading company Native Instruments, such as {KONTAKT}, the industry standard sampling platform used by the majority of music producers and composers for film score, games, and contemporary popular music.

Timur holds a {PhD} in astrophysics and is passionate about well-written code, modern C++ techniques, science-fiction, learning languages, and progressive rock music.},
	note = {{CppCon} 2015},
	author = {Doumler, Timur},
	urldate = {2023-06-16},
	date = {2015-09-21},
	langid = {english},
}

@video{noauthor_notitle_nodate,
}

@collection{boulanger_audio_2011,
	location = {Cambridge, Mass},
	title = {The audio programming book},
	isbn = {978-0-262-01446-5},
	pagetotal = {889},
	publisher = {{MIT} Press},
	editor = {Boulanger, Richard Charles and Lazzarini, Victor},
	date = {2011},
	note = {{OCLC}: ocn503654559},
	keywords = {Computer programs, Computer sound processing, Digital techniques, Music, Signal processing},
}

@book{pirkle_designing_2021,
	location = {New York},
	edition = {2nd edition},
	title = {Designing software synthesizer plug-ins in C++ with audio {DSP}},
	isbn = {978-0-367-51048-0 978-0-367-51046-6},
	abstract = {"Designing Software Synthesizer Plugins in C++ provides everything you need to know to start designing and writing your own synthesizer plugins, including theory and practical examples for all of the major synthesizer building blocks from {LFOs} and {EGs} to {PCM} samples and morphing wavetables, along with complete synthesizer example projects. The book and accompanying {SynthLab} projects include scores of C++ objects and functions that implement the synthesizer building blocks, and six synthesizer projects ranging from virtual analog and physical modelling to wavetable morphing and wave-sequencing that demonstrate their use. You can start using the book immediately with the {SynthLab}-{DM} product that allows you to compile and load mini-modules that resemble modular synth components, without needing to maintain the complete synth project code. The C++ objects all run in a standalone mode, so you can incorporate them into your current projects or whip up a quick experiment. All six synth projects are fully documented from the tiny {SynthClock} up to the {SynthEngine} objects allowing you to get the most from the book while working at a level that you feel comfortable with. This book is intended for music technology and engineering students along with {DIY} audio programmers and anyone wanting to understand how synthesizers may be implemented in C++"--},
	publisher = {Routledge},
	author = {Pirkle, William C.},
	date = {2021},
	keywords = {C++ (Computer program language), Plug-ins (Computer programs), Software synthesizers},
}

@article{topfer_granularspectrals_2022,
	title = {{GranularSpectrals} - Spectral manipulation for Advanced Granular Synthesis},
	url = {http://dl.gi.de/handle/20.500.12116/39059},
	doi = {10.18420/MUC2022-MCI-WS03-397},
	abstract = {Spectral manipulation has increasingly established as a common method in audio editing. Based on the analytical visualization of audio material as a spectrogram, it offers interactive methods that are visually comprehensible. Consequently, it can be used to extend methods of music production by discovering novel techniques or by revisiting known ones. Thereby, granular synthesis has high potential in gaining functionality when thought and used in a spectral domain interface. Here, granular spectral synthesis is proposed and how it takes advantage is discussed.},
	author = {Töpfer, Fabian and Engeln, Lars and Groh, Rainer},
	urldate = {2023-07-14},
	date = {2022},
	langid = {english},
	note = {Publisher: Gesellschaft für Informatik e.V.},
	keywords = {granular synthesis, spectrogram, visual audio design},
}

@book{thompson_understanding_2005,
	location = {Boston, Mass},
	title = {Understanding audio: getting the most out of your project or professional recording studio},
	isbn = {978-0-634-00959-4},
	series = {Recording: Audio},
	shorttitle = {Understanding audio},
	pagetotal = {357},
	publisher = {Berklee Press},
	author = {Thompson, Daniel M.},
	date = {2005},
	note = {{OCLC}: ocm58450656},
	keywords = {Handbooks, manuals, etc, Recording and reproducing, Sound, Sound recording industry},
}

@article{shannon_communication_1949,
	title = {Communication in the Presence of Noise},
	volume = {37},
	issn = {0096-8390},
	url = {http://ieeexplore.ieee.org/document/1697831/},
	doi = {10.1109/JRPROC.1949.232969},
	pages = {10--21},
	number = {1},
	journaltitle = {Proceedings of the {IRE}},
	shortjournal = {Proc. {IRE}},
	author = {Shannon, C.E.},
	urldate = {2023-07-02},
	date = {1949-01},
}

@video{hainbac_how_2021,
	title = {How Happy Accidents Can Inspire Your Music},
	url = {https://www.youtube.com/watch?v=Ibajy9A91ls},
	abstract = {In which I show how I go actively looking for "happy accidents" when composing, especially when working with other musicians. I have found it is an easy way to get creative results fast.

This video is part of a series, where I detail my work with https://instagram.com/friday\_musik/ and https://instagram.com/anabambalaelsat... for Amplify Berlin. 

{SUPPORT} {THE} {CHANNEL}:  http://patreon.com/hainbach
{CHANNEL} {MEMBERSHIP}:    / @hainbach  
{MY} {MUSIC}: http://hainbach.bandcamp.com
{FASHION}: http://teespring.com/de/stores/hainbach
{CONNECT}: http://reddit.com/r/hainbach

{MY} {SIGNATURE} {SOFTWARE}: ({BLACK} {FRIDAY} {SPECIAL} {PRICES})
{WIRE} {RECORDER} {ECHO} https://www.audiothing.net/effects/wires
{MORPHING} {ROTOR} https://www.audiothing.net/effects/th...
{TEST} {EQUIPMENT} {LIBRARY} https://www.spitfireaudio.com/shop/a-...
{LOOPER} https://apps.apple.com/us/app/gauss-f...

{BUY} {THE} {GEAR} I {USE} ({EU}):
https://redir.love/thocf/n2lm9oo2ym
{BUY} {THE} {GEAR} I {USE} ({US}):
https://imp.i114863.net/{KejKxe}
{BUY} {THE} {GEAR} I {USE} ({UK}):
https://www.gear4music.com/blog/what-...
(affiliate links, I get a few \% if you buy through them)},
	author = {{HAINBAC}},
	urldate = {2023-06-22},
	date = {2021-11-23},
}

@article{heideman_gauss_1985,
	title = {Gauss and the history of the fast Fourier transform},
	volume = {34},
	issn = {0003-9519, 1432-0657},
	url = {http://link.springer.com/10.1007/BF00348431},
	doi = {10.1007/BF00348431},
	pages = {265--277},
	number = {3},
	journaltitle = {Archive for History of Exact Sciences},
	shortjournal = {Arch. Hist. Exact Sci.},
	author = {Heideman, Michael T. and Johnson, Don H. and Burrus, C. Sidney},
	urldate = {2023-06-22},
	date = {1985},
	langid = {english},
}

@book{raffaseder_audiodesign_2010,
	location = {München},
	edition = {2., aktualisierte und erweiterte Auflage},
	title = {Audiodesign},
	isbn = {978-3-446-41762-5},
	pagetotal = {313},
	publisher = {Hanser},
	author = {Raffaseder, Hannes},
	date = {2010},
}

@online{noauthor_cmake_nodate,
	title = {{CMake}},
	url = {https://cmake.org/},
	urldate = {2023-06-11},
	langid = {american},
}

@online{noauthor_juce_nodate-1,
	title = {Juce},
	url = {https://juce.com/},
	abstract = {{JUCE} is the most widely used framework for audio application and plug-in development on Windows, {macOS}, Linux, {iOS}, Android, {VST}, {VST}3, {AU}, {AUv}3, {AAX} and {LV}2 plug-ins.},
	titleaddon = {{JUCE}},
	urldate = {2023-06-11},
	langid = {british},
}

@book{parker_good_2009,
	location = {Baltimore},
	title = {Good vibrations: the physics of music},
	isbn = {978-0-8018-9264-6},
	shorttitle = {Good vibrations},
	pagetotal = {274},
	publisher = {Johns Hopkins University Press},
	author = {Parker, Barry R.},
	date = {2009},
	note = {{OCLC}: ocn320194527},
	keywords = {Acoustics and physics, Music, Sound, Sound-waves, Vibration, Wave-motion, Theory of},
}

@book{tsuji_physics_2021,
	location = {Cham, Switzerland},
	title = {Physics and music: essential connections and illuminating excursions},
	isbn = {978-3-030-68675-8},
	shorttitle = {Physics and music},
	publisher = {Springer Nature},
	author = {Tsuji, Kinko and Müller, Stefan C.},
	date = {2021},
}

@book{white_physics_2014,
	location = {Mineola, New York},
	title = {Physics and music: the science of musical sound},
	isbn = {978-0-486-79400-6},
	shorttitle = {Physics and music},
	abstract = {"This foundational text is written for students who want to go beyond the perceptual stage of music to learn how musical sound is created and perceived. It surveys a wide range of topics related to acoustics, beginning with a brief history of the art and science of music. Succeeding chapters explore the general principles of sound, musical scales, the primary ways in which sound can be generated, the characteristics of instruments, the use of mechanical and electronic recording devices, hi-fi stereophonic and quadraphonic sound, the design of electronic musical instruments, and architectural acoustics. Comprehensive yet accessible, Physics and Music includes over 300 diagrams, photographs, and tables. Each chapter concludes with questions, problems, and projects, in addition to references for further study. 1980 edition."-- Provided by publisher},
	publisher = {Dover Publications, Inc.},
	author = {White, Harvey Elliott and White, Donald H.},
	date = {2014},
	note = {{OCLC}: 878661301},
}

@book{katz_capturing_2010,
	location = {Berkeley},
	edition = {Rev. ed},
	title = {Capturing sound: how technology has changed music},
	isbn = {978-0-520-26105-1},
	shorttitle = {Capturing sound},
	abstract = {Synopsis: Fully revised and updated, this new edition of Mark Katz's award-winning text adds coverage of mashups and Auto-Tune, explores recent developments in file-sharing, and includes an expanded conclusion and bibliography. Find illustrative sound and film clips www.ucpress.edu/go/capturingsound},
	pagetotal = {320},
	publisher = {University of California Press},
	author = {Katz, Mark},
	date = {2010},
	note = {{OCLC}: ocn610019531},
	keywords = {Music and technology, Sound recording industry},
}

@article{mathews_digital_1963,
	title = {The Digital Computer as a Musical Instrument},
	volume = {142},
	issn = {00368075, 10959203},
	url = {http://www.jstor.org/stable/1712380},
	pages = {553--557},
	number = {3592},
	journaltitle = {Science},
	author = {Mathews, M. V.},
	urldate = {2023-06-06},
	date = {1963},
	note = {Publisher: American Association for the Advancement of Science},
}

@unpublished{mathews_music_2004,
	location = {Computer History Museum},
	title = {Music Meets the Computer},
	url = {https://www.youtube.com/watch?v=Hloic1oBfug},
	abstract = {Computers have revolutionized music making. Two of the most important pioneers of computer music, Max Mathews and John Chowning, stand at the epicenter of this musical revolution. Research led by Mathews at Bell Laboratories, beginning in the 1950s, created a series of programming languages that are the direct precursors of today's software synthesizers. Max Mathew's many contributions to interactive music systems, algorithmic composition, and psychoacoustics (with Jean-Claude Risset) are equally seminal. Stanford's legendary Center for Computer Research in Music and Acoustics ({CCRMA}) led for many years by Chowning, has long been a hotbed of innovation. After groundbreaking research in sound spatialization, Chowning's invention of frequency modulation ({FM}) synthesis led to the most successful synthesizer of all time: the Yamaha {DX}7. In this video, Chowning and Mathews are in conversation with Curtis Roads, composer and music historian. The video also includes a performance by pianist Chryssie Nanou performing "Duet for One Pianist."},
	author = {Mathews, Max and Chowning, John and Roads, Curtis},
	urldate = {2023-06-06},
	date = {2004-12-14},
}

@book{lai_practical_2004,
	location = {London ; Burlington, {MA}},
	title = {Practical digital signal processing for engineers and technicians},
	isbn = {978-0-7506-5798-3},
	pagetotal = {289},
	publisher = {Newnes},
	author = {Lai, Edmund},
	date = {2004},
	note = {{OCLC}: ocm51527224},
	keywords = {Digital techniques, Signal processing, Techniques numériques, Traitement du signal},
}

@book{ruschkowski_elektronische_2019,
	location = {Ditzingen},
	edition = {3., ergänzte Auflage 2019},
	title = {Elektronische Klänge und musikalische Entdeckungen},
	isbn = {978-3-15-019613-7},
	series = {Reclams Universal-Bibliothek},
	abstract = {Die Digitalisierung brachte nicht nur neue Notations-, Speicher- und Übertragungsformate der Musik mit sich, sondern ermöglichte auch die Erzeugung neuer Klänge und Klangeffekte im Tonstudio. Der erste Teil des Bandes widmet sich der analogen Klangsynthese und Klangsteuerung, der zweite Teil bietet einen Überblick über die digitale Klangerzeugung und den Einsatz von Hardware und Software bei Komposition und Klangbearbeitung von den Anfängen bis heute},
	number = {19613},
	publisher = {Reclam},
	author = {Ruschkowski, André},
	date = {2019},
}

@book{robinson_getting_2013,
	location = {Birmingham},
	title = {Getting started with {JUCE}: leverage the power of the {JUCE} framework to start developing applications},
	isbn = {978-1-4619-4968-8},
	shorttitle = {Getting started with {JUCE}},
	abstract = {His book is a fast-paced, practical guide full of step-by-step examples which are easy to follow and implement. This book is for programmers with a basic grasp of C++. The examples start at a basic level, making few assumptions beyond fundamental C++ concepts. Those without any experience with C++ should be able to follow and construct the examples, although you may need further support to understand the fundamental concepts},
	publisher = {Packt Publishing},
	author = {Robinson, Martin},
	date = {2013},
	note = {{OCLC}: 862386437},
}

@book{noauthor_notitle_nodate-1,
}

@misc{long_fully_2015,
	title = {Fully Convolutional Networks for Semantic Segmentation},
	url = {http://arxiv.org/abs/1411.4038},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks ({AlexNet}, the {VGG} net, and {GoogLeNet}) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of {PASCAL} {VOC} (20\% relative improvement to 62.2\% mean {IU} on 2012), {NYUDv}2, and {SIFT} Flow, while inference takes one third of a second for a typical image.},
	number = {{arXiv}:1411.4038},
	publisher = {{arXiv}},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	urldate = {2023-04-18},
	date = {2015-03-08},
	eprinttype = {arxiv},
	eprint = {1411.4038 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{salimans_progressive_2022,
	title = {Progressive Distillation for Fast Sampling of Diffusion Models},
	url = {http://arxiv.org/abs/2202.00512},
	abstract = {Diffusion models have recently shown great promise for generative modeling, outperforming {GANs} on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like {CIFAR}-10, {ImageNet}, and {LSUN}, we start out with state-of-the-art samplers taking as many as 8192 steps, and are able to distill down to models taking as few as 4 steps without losing much perceptual quality; achieving, for example, a {FID} of 3.0 on {CIFAR}-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time.},
	number = {{arXiv}:2202.00512},
	publisher = {{arXiv}},
	author = {Salimans, Tim and Ho, Jonathan},
	urldate = {2023-04-18},
	date = {2022-06-07},
	eprinttype = {arxiv},
	eprint = {2202.00512 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{song_denoising_2022,
	title = {Denoising Diffusion Implicit Models},
	url = {http://arxiv.org/abs/2010.02502},
	abstract = {Denoising diffusion probabilistic models ({DDPMs}) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models ({DDIMs}), a more efficient class of iterative implicit probabilistic models with the same training procedure as {DDPMs}. In {DDPMs}, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that {DDIMs} can produce high quality samples \$10 {\textbackslash}times\$ to \$50 {\textbackslash}times\$ faster in terms of wall-clock time compared to {DDPMs}, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
	number = {{arXiv}:2010.02502},
	publisher = {{arXiv}},
	author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
	urldate = {2023-04-18},
	date = {2022-10-05},
	eprinttype = {arxiv},
	eprint = {2010.02502 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ho_denoising_2020,
	title = {Denoising Diffusion Probabilistic Models},
	url = {http://arxiv.org/abs/2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional {CIFAR}10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art {FID} score of 3.17. On 256x256 {LSUN}, we obtain sample quality similar to {ProgressiveGAN}. Our implementation is available at https://github.com/hojonathanho/diffusion},
	number = {{arXiv}:2006.11239},
	publisher = {{arXiv}},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	urldate = {2023-04-06},
	date = {2020-12-16},
	eprinttype = {arxiv},
	eprint = {2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ronneberger_u-net_2015,
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	url = {http://arxiv.org/abs/1505.04597},
	shorttitle = {U-Net},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the {ISBI} challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and {DIC}) we won the {ISBI} cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent {GPU}. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	number = {{arXiv}:1505.04597},
	publisher = {{arXiv}},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	urldate = {2023-04-06},
	date = {2015-05-18},
	eprinttype = {arxiv},
	eprint = {1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{pasini_musika_2022,
	title = {Musika! Fast Infinite Waveform Music Generation},
	url = {http://arxiv.org/abs/2208.08706},
	abstract = {Fast and user-controllable music generation could enable novel ways of composing or performing music. However, state-of-the-art music generation systems require large amounts of data and computational resources for training, and are slow at inference. This makes them impractical for real-time interactive use. In this work, we introduce Musika, a music generation system that can be trained on hundreds of hours of music using a single consumer {GPU}, and that allows for much faster than real-time generation of music of arbitrary length on a consumer {CPU}. We achieve this by first learning a compact invertible representation of spectrogram magnitudes and phases with adversarial autoencoders, then training a Generative Adversarial Network ({GAN}) on this representation for a particular music domain. A latent coordinate system enables generating arbitrarily long sequences of excerpts in parallel, while a global context vector allows the music to remain stylistically coherent through time. We perform quantitative evaluations to assess the quality of the generated samples and showcase options for user control in piano and techno music generation. We release the source code and pretrained autoencoder weights at github.com/marcoppasini/musika, such that a {GAN} can be trained on a new music domain with a single {GPU} in a matter of hours.},
	number = {{arXiv}:2208.08706},
	publisher = {{arXiv}},
	author = {Pasini, Marco and Schlüter, Jan},
	urldate = {2023-04-03},
	date = {2022-08-18},
	eprinttype = {arxiv},
	eprint = {2208.08706 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{borsos_audiolm_2022,
	title = {{AudioLM}: a Language Modeling Approach to Audio Generation},
	url = {http://arxiv.org/abs/2209.03143},
	shorttitle = {{AudioLM}},
	abstract = {We introduce {AudioLM}, a framework for high-quality audio generation with long-term consistency. {AudioLM} maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, {AudioLM} learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, {AudioLM} generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.},
	number = {{arXiv}:2209.03143},
	publisher = {{arXiv}},
	author = {Borsos, Zalán and Marinier, Raphaël and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and Zeghidour, Neil},
	urldate = {2023-04-03},
	date = {2022-09-07},
	eprinttype = {arxiv},
	eprint = {2209.03143 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{dhariwal_jukebox_2020,
	title = {Jukebox: A Generative Model for Music},
	url = {http://arxiv.org/abs/2005.00341},
	shorttitle = {Jukebox},
	abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale {VQ}-{VAE} to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
	number = {{arXiv}:2005.00341},
	publisher = {{arXiv}},
	author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
	urldate = {2023-04-03},
	date = {2020-04-30},
	eprinttype = {arxiv},
	eprint = {2005.00341 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@misc{oord_wavenet_2016,
	title = {{WaveNet}: A Generative Model for Raw Audio},
	url = {http://arxiv.org/abs/1609.03499},
	shorttitle = {{WaveNet}},
	abstract = {This paper introduces {WaveNet}, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single {WaveNet} can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	number = {{arXiv}:1609.03499},
	publisher = {{arXiv}},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	urldate = {2023-04-03},
	date = {2016-09-19},
	eprinttype = {arxiv},
	eprint = {1609.03499 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
}

@misc{schneider_archisound_2023,
	title = {{ArchiSound}: Audio Generation with Diffusion},
	url = {http://arxiv.org/abs/2301.13267},
	shorttitle = {{ArchiSound}},
	abstract = {The recent surge in popularity of diffusion models for image generation has brought new attention to the potential of these models in other areas of media generation. One area that has yet to be fully explored is the application of diffusion models to audio generation. Audio generation requires an understanding of multiple aspects, such as the temporal dimension, long term structure, multiple layers of overlapping sounds, and the nuances that only trained listeners can detect. In this work, we investigate the potential of diffusion models for audio generation. We propose a set of models to tackle multiple aspects, including a new method for text-conditional latent audio diffusion with stacked 1D U-Nets, that can generate multiple minutes of music from a textual description. For each model, we make an effort to maintain reasonable inference speed, targeting real-time on a single consumer {GPU}. In addition to trained models, we provide a collection of open source libraries with the hope of simplifying future work in the field. Samples can be found at https://bit.ly/audio-diffusion. Codes are at https://github.com/archinetai/audio-diffusion-pytorch.},
	number = {{arXiv}:2301.13267},
	publisher = {{arXiv}},
	author = {Schneider, Flavio},
	urldate = {2023-03-19},
	date = {2023-01-30},
	eprinttype = {arxiv},
	eprint = {2301.13267 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{liu_audioldm_2023,
	title = {{AudioLDM}: Text-to-Audio Generation with Latent Diffusion Models},
	url = {http://arxiv.org/abs/2301.12503},
	shorttitle = {{AudioLDM}},
	abstract = {Text-to-audio ({TTA}) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in {TTA} have limited generation quality with high computational costs. In this study, we propose {AudioLDM}, a {TTA} system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining ({CLAP}) latents. The pretrained {CLAP} models enable us to train {LDMs} with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, {AudioLDM} is advantageous in both generation quality and computational efficiency. Trained on {AudioCaps} with a single {GPU}, {AudioLDM} achieves state-of-the-art {TTA} performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, {AudioLDM} is the first {TTA} system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm.github.io.},
	number = {{arXiv}:2301.12503},
	publisher = {{arXiv}},
	author = {Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D.},
	urldate = {2023-03-19},
	date = {2023-02-16},
	eprinttype = {arxiv},
	eprint = {2301.12503 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
}

@misc{huang_make--audio_2023,
	title = {Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models},
	url = {http://arxiv.org/abs/2301.12661},
	shorttitle = {Make-An-Audio},
	abstract = {Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining ({CLAP}) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation. Moreover, we present its controllability and generalization for X-to-Audio with "No Modality Left Behind", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input. Audio samples are available at https://Text-to-Audio.github.io},
	number = {{arXiv}:2301.12661},
	publisher = {{arXiv}},
	author = {Huang, Rongjie and Huang, Jiawei and Yang, Dongchao and Ren, Yi and Liu, Luping and Li, Mingze and Ye, Zhenhui and Liu, Jinglin and Yin, Xiang and Zhao, Zhou},
	urldate = {2023-03-19},
	date = {2023-01-29},
	eprinttype = {arxiv},
	eprint = {2301.12661 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{peebles_scalable_2023,
	title = {Scalable Diffusion Models with Transformers},
	url = {http://arxiv.org/abs/2212.09748},
	abstract = {We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers ({DiTs}) through the lens of forward pass complexity as measured by Gflops. We find that {DiTs} with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower {FID}. In addition to possessing good scalability properties, our largest {DiT}-{XL}/2 models outperform all prior diffusion models on the class-conditional {ImageNet} 512x512 and 256x256 benchmarks, achieving a state-of-the-art {FID} of 2.27 on the latter.},
	number = {{arXiv}:2212.09748},
	publisher = {{arXiv}},
	author = {Peebles, William and Xie, Saining},
	urldate = {2023-03-19},
	date = {2023-03-02},
	eprinttype = {arxiv},
	eprint = {2212.09748 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2023-03-19},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}
