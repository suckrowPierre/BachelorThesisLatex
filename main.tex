% !TeX spellcheck = en-US
% !TeX encoding = utf8
% !TeX program = pdflatex
% !BIB program = biber
% -*- coding:utf-8 mod:LaTeX -*-


% vv  scroll down to line 200 for content  vv


\let\ifdeutsch\iftrue
\let\ifenglish\iffalse


\input{pre-documentclass}
\documentclass[
  %
  %ngerman, %%% Add if you write in German.
  %
  % fontsize=11pt is the standard
  a4paper,  % Standard format - only KOMAScript uses paper=a4 - https://tex.stackexchange.com/a/61044/9075
  twoside,  % we are optimizing for both screen and two-side printing. So the page numbers will jump, but the content is configured to stay in the middle (by using the geometry package)
  bibliography=totoc,
  %               idxtotoc,   %Index ins Inhaltsverzeichnis
  %               liststotoc, %List of X ins Inhaltsverzeichnis, mit liststotocnumbered werden die Abbildungsverzeichnisse nummeriert
  headsepline,
  cleardoublepage=empty,
  parskip=half,
  %               draft    % um zu sehen, wo noch nachgebessert werden muss - wichtig, da Bindungskorrektur mit drin
  draft=false
]{scrbook}
\input{config}

\usepackage{multicol}

\usepackage[
  title={Text-zu-spielbarem-Klang: Synthesizer basierend auf Latent-Diffusion-Technologie}, % Do not forget to capitalize your title correctly, you may use the following page to help you: https://capitalizemytitle.com/
  author={Pierre-Louis Wolgang Léon Suckrow},
  orcid=0000-0000-0000-0000, % get your own ORCID via https://orcid.org/
  email={suckrowpierre@gmail.com},
  type=bachelor,
  institute={Institut für Informatik}, % or other institute names - or just a plain string using {Demo\\Demo...}
  course={Informatik},
  examiner={Prof.\ Dr.\ Sylvia Rothe},
  supervisor={Christoph Weber},
  startdate={Mai 31, 2023},
  enddate={Oktober 19, 2023},
  % Falls keine Lizenz gewünscht wird bitte auf "none" setzen
  % Die Lizenz erlaubt es zu nichtkommerziellen Zwecken die Arbeit zu
  % vervielfältigen und Kopien zu machen. Dabei muss aber immer der Autor
  % angegeben werden. Eine kommerzielle Verwertung ist für den Autor
  % weiter möglich.
  copyright=ccbysa, % ccbysa, ccbynosa, cc0, none
  language=german
]{lmu-thesis-cover}

\input{acronyms}

\makeindex

\begin{document}

%tex4ht-Konvertierung verschönern
\iftex4ht
  % tell tex4ht to create picures also for formulas starting with '$'
  % WARNING: a tex4ht run now takes forever!
  \Configure{$}{\PicMath}{\EndPicMath}{}
  %$ % <- syntax highlighting fix for emacs
  \Css{body {text-align:justify;}}

  %conversion of .pdf to .png
  \Configure{graphics*}
  {pdf}
  {\Needs{"convert \csname Gin@base\endcsname.pdf
      \csname Gin@base\endcsname.png"}%
    \Picture[pict]{\csname Gin@base\endcsname.png}%
  }
\fi

%\VerbatimFootnotes %verbatim text in Fußnoten erlauben. Geht normalerweise nicht.

\input{commands}
\pagenumbering{arabic}
\Coverpage
\Copyright
%Eigener Seitenstil fuer die Kurzfassung und das Inhaltsverzeichnis
\deftriplepagestyle{preamble}{}{}{}{}{}{\pagemark}
%Doku zu deftriplepagestyle: scrguide.pdf
\pagestyle{preamble}
\renewcommand*{\chapterpagestyle}{preamble}



%Kurzfassung / abstract
%auch im Stil vom Inhaltsverzeichnis
\section*{Kurzfassung}

\todo{Short summary of the thesis. Here, the following questions should be answered:}
\todo{What is the specific problem addressed?}
\todo{What have you done?}
\todo{What did you find out?}
\todo{What are the implications on a larger scale?}
\todo{Should be around 0.5 pages. Not longer than 1 page.}

\cleardoublepage

\section*{Abstract}

\todo{Short summary of the thesis. Here, the following questions should be answered:}
\todo{What is the specific problem addressed?}
\todo{What have you done?}
\todo{What did you find out?}
\todo{What are the implications on a larger scale?}
\todo{Should be around 0.5 pages. Not longer than 1 page.}

\cleardoublepage


% BEGIN: Verzeichnisse

\iftex4ht
\else
  \microtypesetup{protrusion=false}
\fi

%%%
% Literaturverzeichnis ins TOC mit aufnehmen, aber nur wenn nichts anderes mehr hilft!
% \addcontentsline{toc}{chapter}{Literaturverzeichnis}
%
% oder zB
%\addcontentsline{toc}{section}{Abkürzungsverzeichnis}
%
%%%

%Produce table of contents
%
%In case you have trouble with headings reaching into the page numbers, enable the following three lines.
%Hint by http://golatex.de/inhaltsverzeichnis-schreibt-ueber-rand-t3106.html
%
%\makeatletter
%\renewcommand{\@pnumwidth}{2em}
%\makeatother
%
\tableofcontents

% Bei einem ungünstigen Seitenumbruch im Inhaltsverzeichnis, kann dieser mit
% \addtocontents{toc}{\protect\newpage}
% an der passenden Stelle im Fließtext erzwungen werden.

\listoffigures
\listoftables

% Control List of Listings
\let\iflistings\iffalse
%Wird nur bei Verwendung von der lstlisting-Umgebung mit dem "caption"-Parameter benoetigt
%\lstlistoflistings
%ansonsten:
\iflistings
  \ifdeutsch
    \listof{Listing}{Verzeichnis der Listings}
  \else
    \listof{Listing}{List of Listings}
  \fi
\fi

% Control List of Algorithms
\let\ifalgorithms\iffalse
\ifalgorithms
  %mittels \newfloat wurde die Algorithmus-Gleitumgebung definiert.
  %Mit folgendem Befehl werden alle floats dieses Typs ausgegeben
  \ifdeutsch
    \listof{Algorithmus}{Verzeichnis der Algorithmen}
  \else
    \listof{Algorithmus}{List of Algorithms}
  \fi
  %\listofalgorithms %Ist nur für Algorithmen, die mittels \begin{algorithm} umschlossen werden, nötig
\fi

% Control Glossary
\let\ifglossary\iffalse
\ifglossary
  \printnoidxglossaries
\fi

\iftex4ht
\else
  %Optischen Randausgleich und Grauwertkorrektur wieder aktivieren
  \microtypesetup{protrusion=true}
\fi

% END: Verzeichnisse


% Headline and footline
\renewcommand*{\chapterpagestyle}{scrplain}
\pagestyle{scrheadings}
\pagestyle{scrheadings}
\ihead[]{}
\chead[]{}
\ohead[]{\headmark}
\cfoot[]{}
\ofoot[\usekomafont{pagenumber}\thepage]{\usekomafont{pagenumber}\thepage}
\ifoot[]{}


%% vv  scroll down for content  vv %%































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Main content starts here
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Einleitung}
\label{sec:introduction}

\glqq Es gibt keine theoretischen Grenzen für den Computer als Quelle für musikalische Klänge, im Gegensatz zu herkömmlichen Instrumenten\grqq \footnote{"There are no theoretical limitations to the performance of the computer as a source of musical sounds, in contrast to the performance of ordinary instruments"} \cite{mathews_digital_1963}. Mit diesen wegweisenden Worten publizierte Max Vernon Mathews im Jahr 1963 seinen Artikel \glqq The Digital Computer as a Musical Instrument\grqq \, und legte durch seine Arbeit an den Bell Laboratories den Grundstein für die heutige Musikproduktion und Klangsynthese am Computer \cite{mathews_music_2004}.

In Anlehnung an die Überlegungen von M. V. Mathews konzentriert sich die vorliegende Arbeit auf die Erforschung von Implementierungsmöglichkeiten eines Synthesizers unter Verwendung der neuartigen \emph{Latenten Diffusionstechnologie}. Es werden sowohl das resultierende digitale Instrument als auch die potenziellen Vorteile und Limitationen dieser Implementierung betrachtet. Zudem wird das Potenzial der \emph{Diffusions}-Modelle im Kontext musikalischer Anwendungen diskutiert. Die vorgestellte Realisierung beabsichtigt, Nutzern Klangbilder gemäß individueller Präferenzen zu generieren und diese folglich zu modifizieren und spielbar zu gestalten. Das zentrale Ziel dieser Studie besteht darin, die verschiedenen Implementierungsansätze sowie die Ergebnisse des digitalen Instruments und der eingesetzten \emph{Diffusions}-Modelle systematisch zu analysieren, ihre jeweiligen Stärken und Schwächen herauszuarbeiten und die Tauglichkeit als innovatives Musikinstrument zu bewerten. Ein weiteres Ziel ist es, das Instrument so zu gestalten, dass es nahtlos in den zeitgenössischen Musikproduktionsprozess integriert werden kann, wodurch Musikern der Zugang zu neuen Klangwelten und Syntheseverfahren ermöglicht wird. Die Erzeugung eigens definierter Klänge könnte darüber hinaus rechtliche Problematiken, die durch den Einsatz von Samples auftreten, obsolet machen. Im weiteren Verlauf dieser Arbeit wird die Machbarkeit eines solchen Instruments unter Berücksichtigung von zwei spezifischen Diffusionsmodellen für die Audiosynthese erörtert.

Vereinzelte Projekte, darunter der von Google im Jahr 2017 entwickelte \emph{NSynth} \cite{google_ai_nsynth_2017} oder das Audio-Plugin \emph{Neutone} \cite{qosmo_neutone_nodate}, haben bereits die Konzeption von Instrumenten mithilfe von künstlicher Intelligenz in Angriff genommen. Diese Ansätze erlernen auditive Charakteristika und wenden diese auf ein Signal an bzw. übertragen diese. Die jüngsten Fortschritte in der Entwicklung generativer Modelle haben nun das Potenzial eröffnet, vollkommen neue Klangbilder zu erschaffen. Durch die Möglichkeit, das Ergebnis dieser Modelle präzise durch eine Texteingabe zu bestimmen, erhalten Musiker, Musikproduzenten und Sounddesigner eine neuartige Kontrollmethode zur Definition gewünschter Klangattribute. Ein ähnlicher Ansatz, mit Verwendung desselben Modells wie in dieser Arbeit findet Anwendung in \emph{VroomAI} \cite{barney_hill_vroomai_2023}.

In der vorliegenden Arbeit werden die Text-zu-Audio-Diffusionsmodelle \emph{Audio Latent Diffusion Model} \emph{AudioLDM}\cite{liu_audioldm_2023} und  \emph{Audio Latent Diffusion Model 2} \emph{AudioLDM2}\cite{liu_audioldm2_2023} verwendet, um das Potenzial von künstlicher Intelligenz im Bereich der Audiosynthese in einem musikalischen Kontext aufzuzeigen. Der Fokus liegt dabei auf der Untersuchung dieser neuen Methode zur Klangerzeugung und der Bewertung ihres performativen Potenzials. Insbesondere die Möglichkeit einer interaktiven Bereitstellung von Einstellungsmöglichkeiten für Parameter des jeweiligen Modells und die Klangwiedergabe, welche in vergleichbaren Projekten bisher fehlte, soll dem Instrumentalist zusätzliche Kontrolle über die gewünschten Eigenschaften verleihen.

% Fourth paragraph
% CORE MESSAGE OF THIS PARAGRAPH:
\todo{P4.1. What did you find out? What are the concrete results?}
\todo{P4.2. What are the implications? What does this mean for the bigger picture?}

\chapter{Related Work}


Die Entwicklung eines Instruments, das die Soundsynthese durch künstliche Intelligenz ermöglicht, wurde im Jahr 2023 im Rahmen des von \emph{Juce} \cite{noauthor_juce_nodate} organisierten Wettbewerbs „Neural Audio Plugin“ verfolgt. Insbesondere zeichnete sich das Projekt \emph{VroomAi} \cite{barney_hill_vroomai_2023} durch die Nutzung desselben C++ Audio Frameworks wie im vorliegenden Projekt aus. In \emph{VroomAi} wurde ein digitales Instrument entwickelt, welches über eine Benutzeroberfläche die Eingabe eines benutzerspezifischen Prompts zur Klangerzeugung ermöglicht. Anschließend kann dieser Klang durch den Einsatz eines Samplers spielbar gemacht werden.

\emph{VroomAi} stellt zwei Modelle zur Verfügung: Das Modell \emph{AudioLDM} \cite{liu_audioldm_2023}, welches in dieser Arbeit implementiert ebenfalls benutzt wird, und das Modell \emph{AudioLM}\cite{borsos_audiolm_2022}. Das Modell \emph{AudioLDM2} wurde bis zum Veröffentlichungsdatum dieser Arbeit noch nicht publiziert und findet daher keine Anwendung. Die in \emph{VroomAi} implementierte Architektur ähnelt jener der in dieser Arbeit vorgestellten Umsetzung. Ihr Hauptzweck besteht darin, die Modelle auszuführen und Klänge zu generieren. Hierfür wird auf einen \emph{Gradio} Server \cite{team_gradio_gradio_nodate} zurückgegriffen, der das Modell lokal ausführt.

Das lokale Ausführen des Modells weist bestimmte Schwachstellen auf (siehe Kapitel \ref{sec:api}). Insbesondere bei der Nutzung des von den \emph{AudioLDM} Entwicklern bereitgestellten \emph{Gradio}-Servers können Komplikationen entstehen. Das \emph{AudioLDM} Modell lässt sich, ohne zusätzliche Anpassungen, nicht auf Computern ohne GPU betreiben. Dies kann für Anwender ohne Vorkenntnisse im Bereich ML-Modelle oder bei nicht geeigneter Hardware zu Herausforderungen führen. Ziel dieser Arbeit ist es, solche Hürden so weit wie möglich zu reduzieren und ein intuitiv zu bedienendes digitales Instrument zu entwickeln. Zusätzlich soll dem Anwender die Möglichkeit gegeben werden, modellspezifische sowie Audio-Parameter einzugeben – eine Funktion, die in \emph{VroomAi} bisher nicht vorhanden ist.

Neben \emph{AudioLDM} \cite{liu_audioldm_2023} bieten sich folgende \emph{Machine-Learning} Modelle an, um Klangerzeugung zu betreiben.

\emph{AudioLM} \cite{borsos_audiolm_2022} ermöglicht Audio-Synthese durch eine Kombination von \emph{adversarial neural networks} \cite{goodfellow_generative_2014}, \emph{self-supervised representation learning} \cite{chung_w2v-bert_2021} und \emph{language modeling} \cite{roberts_scaling_2022} besteht. Es wurde auf der \emph{tokenisierten} Darstellung von Wellenformen trainiert. Obwohl die Hauptfunktion von \emph{AudioLM} in der Stimmgenerierung liegt, zeigte es, durch Training auf Klavieraufnahmen, ebenfalls Potenzial in der musikalischen Synthese.  Das Modell besteht aus drei Kernelementen: einem \emph{Tokenizer-Modell}, dem Decoder eines \emph{Transformers-Sprachmodells} und einem \emph{Detokenizer-Modell}. Der Tokenizer wandelt kontinuierliche Audiowellenformen in eine diskrete Repräsentation um. Dabei verwendet es ein hybrides Tokenisierungsverfahren (siehe Abbildung \ref{fig:tokenizerAudioLM}), das sowohl akustische als auch semantische Charakteristika (wie Sprachinhalte für Sprache oder Melodie und Rhythmus für Musik) der Wellenform erfasst. Während \emph{SoundStream} \cite{zeghidour_soundstream_2021} die akustischen Token generiert, stammen die semantischen Token von den Repräsentationen einer Zwischenschicht des \emph{w2v-BERT} \cite{chung_w2v-bert_2021}. Der Decoder eines \emph{Transformers-Sprachmodells}\cite{vaswani_attention_2017} lernt, Tokens basierend auf ihrem vorherigen Kontext zu antizipieren, indem die Abhängigkeiten und Struktur innerhalb der benutzen Sprache erfasst werden. Während der Inferenz prognostiziert das Modell aus einem gegebenen \emph{Prompt} autoregressiv die Token-Sequenz. Dies impliziert, dass Tokens nacheinander generiert werden, wobei jeweils die zuvor erzeugten Tokens als Kontext dienen. Ein hierarchisches Vorgehen wird dabei angewendet (siehe Abbildung \ref{fig:hierarAudioLM}): Zuerst werden semantische Token für die komplette Sequenz modelliert, welche dann als Konditionen zur Vorhersage der akustischen Token dienen. Abschließend konvertiert das Detokenizer-Modell die Token-Sequenz zurück in ein Audiosignal. \cite{borsos_audiolm_2022}

\begin{figure}[h]
\centering
\begin{subfigure}{1.0\textwidth}
  \centering
  \includegraphics[width=0.5\textwidth]{tokenizerAudioLM.png}
  \caption[AudioLM Tokenisierungsverfahren]{Benutzte Tokenizer}
  \label{fig:tokenizerAudioLM}
\end{subfigure}

\vspace{1em} % Optional: Add some vertical space between the subfigures

\begin{subfigure}{1.0\textwidth}
  \centering
  \includegraphics[width=0.7\textwidth]{hierarAudioLM.png}
  \caption[AudioLM Hierarchische Modellierung der Tokens]{Hierarchische Modellierung der Tokens}
  \label{fig:hierarAudioLM}
\end{subfigure}
\caption[AudioLM Architektur]{AudioLM Architektur \cite{borsos_audiolm_2022}}
\label{fig:test}
\end{figure}

Das Modell \emph{Diffsound} \cite{yang_diffsound_2023} wurde mit der Absicht entwickelt, Soundeffekte zu generieren. Die Erzeugung eines Audiosignals erfolgt anhand eines \emph{Prompts}, unter Zuhilfenahme eines \emph{Text Encoders}, eines \emph{Vector Quantized Variational Autoencoders (VQ-VAE)}, eines \emph{Token Decoders} und eines \emph{Vocoders} (siehe Abbildung \ref{fig:DiffsoundArchitecture}). Der \emph{Text Encoder} extrahiert relevante Audioinformationen aus dem eingegebenen Text und eliminiert dabei unwesentliche Daten. Dabei kommen ein vorab trainiertes \emph{BERT} \cite{devlin_bert_2019} und der \emph{Text Encoder} eines bereits trainierten \emph{Contrastive Language-Image Pre-Training (CLIP)} \cite{radford_learning_2021} zum Einsatz. Aus diesen Token wird ein \emph{Mel-Spektrogramm} generiert, welches das künftige Audiosignal repräsentiert. Da Mel-Spektrogramme in eine Token-Sequenz überführt werden können, erstellt der \emph{Token Decoder} diese basierend auf den vom \emph{Text Encoder} generierten Token. Die Qualität des resultierenden Audiosignals hängt maßgeblich vom \emph{Token Decoder} ab. Frühere Arbeiten \cite{liu_conditional_2021, iashin_taming_2021} setzten auf einen \emph{autoregressiven} Ansatz, welcher aufeinanderfolgende Token basierend auf ihren Vorgängern vorhersagte. Ein solcher Ansatz kann allerdings zu kumulierten Fehlern führen, die sowohl die Leistungszeit als auch die Qualität beeinträchtigen. Zur Überwindung dieser Schwäche präsentiert das Paper einen \emph{nicht autoregressiven Token Decoder}, der auf \emph{Diskreter Diffusion} \cite{sohl-dickstein_deep_2015, austin_structured_2023} basiert. Anstatt die Mel-Spektrogramm-Token sequenziell vorherzusagen, prognostiziert \emph{Diffsound} alle Token simultan und optimiert sie iterativ. Der \emph{VQ-VAE} \cite{oord_neural_2018} lernt mithilfe eines \emph{Discriminators} das Vokabular der Spektrogramm-Token, um ein Spektrogramm zu generieren (siehe Abbildung \ref{fig:VQVAE}). In einem abschließenden Schritt konvertiert der \emph{Vocoder} das erstellte Spektrogramm in ein Audiosignal. Hierzu wurde der \emph{Vocoder MelGAN} \cite{kumar_melgan_2019} mit dem \emph{AudioSet} \cite{gemmeke_audio_2017} Datensatz trainiert. Zusätzlich kam der Datensatz \emph{AudioCaps} \cite{kim_audiocaps_2019} sowohl für Training als auch Validierung zum Einsatz. Trotz der Fortschritte besitzt diese Arbeit Limitierungen: So ist das Generierungssystem nicht vollständig End-to-End und sowohl der VQ-VAE, der Token-Decoder als auch der Vocoder werden separat trainiert. \cite{yang_diffsound_2023}

\begin{figure}[h]
\centering
\begin{subfigure}{1.0\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{Diffsound1.png}
  \caption[Diffsound Architektur]{Diffsound Architektur}
  \label{fig:DiffsoundArchitecture}
\end{subfigure}

\vspace{1em} % Optional: Add some vertical space between the subfigures

\begin{subfigure}{1.0\textwidth}
  \centering
  \includegraphics[width=0.7\textwidth]{Diffsound2.png}
  \caption[Diffsound VQ-VAE]{Diffsound Vector Quantized Variational Autoencoder}
  \label{fig:VQVAE}
\end{subfigure}
\caption[Diffsound Architektur]{Diffsound Architektur \cite{yang_diffsound_2023}}
\label{fig:test}
\end{figure}

Ebenfalls einen \emph{autoregressiven} Ansatz verfolgend, verspricht \emph{AUDIOGEN}\cite{kreuk_audiogen_2023} eine verbesserte Audiogenerierung auf der Grundlage textueller Eingaben im Vergleich zu \emph{Diffsound}. \emph{AUDIOGEN} operiert auf einer gelernten diskreten Audio-Repräsentation und ist in der Lage daraus Audiosignale zu generieren. Die Architektur dieses Modells besteht aus zwei Hauptkomponenten (siehe Abbildung \ref{fig:audiogen}). Die erste Komponente kodiert Audio mittels \emph{neural audio compression}\cite{zeghidour_soundstream_2021} in eine diskrete Tokensequenz. Das hierfür verwendete Modell wurde darauf trainiert, komprimierte Audiosignale zu rekonstruieren. Auf die durch diese erste Komponente erzeugten Audio-Tokens versucht ein \emph{autoregressiven} \emph{Transformer-decoder} \emph{Sprachmodell}, das dem \emph{GPT2} \cite{alec_radford_jeff_wu_rewon_child_david_luan_dario_amodei_ilya_sutskever_language_2019} ähnelt, unter Berücksichtigung von Textinformationen abzubilden. Während der Inferenz extrahiert das \emph{Transformer}-Modell die zur Eingabe passenden Tokens, die anschließend durch den Decoder der ersten Komponente in Audio umgewandelt werden können. Das entwickelte Modell zeigt jedoch Schwierigkeiten, temporale Umformulierungen korrekt zu verarbeiten. \cite{kreuk_audiogen_2023}

\begin{figure}[h]
  \centering
  \includegraphics[width=.5\textwidth]{graphics/Audiogen.png}
  \caption[AUDIOGEN Architektur]{AUDIOGEN Architektur \cite{kreuk_audiogen_2023}}
  \label{fig:audiogen}
\end{figure}

Das Modell \emph{Tango} \cite{ghosal_text--audio_2023} zieht Inspiration aus \emph{AudioLDM} und verfolgt das Ziel, Text in Audio mittels latenter Diffusion zu transformieren. Es hebt sich in bestimmten Aspekten von \emph{AudioLDM} ab und strebt an, überlegene Resultate trotz reduziertem Datensatz zu erziehlen. Die Konstruktionsweise setzt sich aus drei zentralen Bestandteilen zusammen (siehe Abbildung \ref{fig:tango}): einem \emph{Text Encoder} zur Verarbeitung der Eingabeprompts, einem \emph{Latenten Diffusionsmodell} (\emph{LDM})\cite{rombach_high-resolution_2022}, und einem \emph{Variational Autoencoder} (\emph{VAE})\cite{kingma_auto-encoding_2022}. Der \emph{Text Encoder}, angereichert durch das \emph{Large Language Model} (\emph{LLM}) \emph{FLAN-T5-LARGE} \cite{chung_scaling_2022}, formt eine textbasierte Darstellung des gegebenen Inputs. Untersuchungen \cite{dai_why_2023} zufolge vermögen \emph{FLAN-T5-Modelle} durch ihr extensives Vortraining - reich an Gedankenstrukturen, Argumentationslinien und Anweisungen - rasch und wirkungsvoll neue Aufgaben zu erlernen. Dieses intensive Vortraining, das älteren Textmodellen wie \emph{RoBERTa}\cite{liu_roberta_2019} fehlte, könnte die Betonung essenzieller Details vereinfachen, was eine optimierte Transformation von Texten in ihre auditiven Pendants begünstigen könnte. Das \emph{LDM}, adaptiert von \emph{AudioLDM}, wandelt die textbasierte Darstellung des Prompts in eine latente Audio-Abbildung mittels Spektogramm-Tokens um. Dieser Vorgang beruht auf dem synchronisierten Wechselspiel von Vorwärts- und Rückwärtsdiffusion: Dem Audio wird Rauschen beigemischt, welches anschließend unter Anleitung des Textes wieder entfernt wird. Bei der Auswertung wird der trainierte Rückwärtsdiffusionsablauf genutzt, um die Audio-Darstellung zu formen. Der \emph{VAE}-Decoder wandelt dies dann in ein Mel-Spektogramm um. Der \emph{Vocoder HiFi-GAN}\cite{kong_hifi-gan_2020}, identisch zu dem in \emph{AudioLDM}, konvertiert das Spektogramm letztlich in ein Audiosignal. Eine Schwäche des Modells liegt in der Erstellung nuancierter Audio-Outputs für ähnliche Texte, besonders wenn es nur auf kleineren Datensätzen trainiert wurde. Die Autoren empfehlen daher, dieses Hindernis durch Training auf umfangreicheren Datensätzen zu überbrücken, um die Kapazitäten des Modells in Bezug auf Differenzierung und Generierung von diversen Text-Audio-Kombinationen zu steigern. \cite{ghosal_text--audio_2023}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\textwidth]{graphics/Tango.png}
  \caption[Tango Architektur]{Tangos Architektur \cite{liu_roberta_2019}}
  \label{fig:tango}
\end{figure}

Ein zusätzliches Modell, das sich der Methode der \emph{Latenten Diffusion}\cite{rombach_high-resolution_2022} bedient, um Text in Audio zu konvertieren, ist \emph{Make-An-Audio} \cite{huang_make--audio_2023}. Parallel zur Entwicklung dieses Modells (siehe Abbildung \ref{fig:Make-An-Audio_Architecture}) wurde eine Methode konzipiert, um ungelabelten Audiosignalen eine Beschreibung zuzuweisen (siehe Abbildung \ref{fig:Make-An-Audio_DestillATIon}). Diese Methode soll den Mangel an gelabelten Audiosignalen adressieren. Der Prozess zur Umschreibung ungelabelter Audiosignale gliedert sich in zwei Stufen. In der ersten Stufe kommen vortrainierte Expertenmodelle \cite{xu_crnn-gru_2020, deshmukh_audio_2022, koepke_audio_2023} zum Einsatz, um Wissen zu destillieren. Diese Modelle generieren Texte, die das Audiosignal beschreiben. Der Text mit der höchsten \emph{CLAP}-Bewertung \cite{elizalde_clap_2022} wird als finaler Text ausgewählt. Zur Vermeidung von Overfitting im Modell wird in der zweiten Stufe aus einer Datenbank mit gelabelten Audioereignissen bis zu zwei Signale extrahiert und mit dem aus dem ersten Schritt generierten Paar kombiniert. Der \emph{Diffusions}-Prozess arbeitet auf einer latenten Repräsentation des Audios, die durch einen Spektrogramm-\emph{Autoencoder} erlernt wurde. Ein \emph{Encoder}-Netz empfängt ein Spektrogramm und komprimiert dieses in den \emph{Latentenraum}. Ein \emph{Decoder} zielt darauf ab, das komprimierte Audiosignal aus dem latenten Raum zu rekonstruieren. Das Modell wird daraufhin trainiert, den durch die Rekonstruktion entstandenen Verlust zu minimieren und mit einem zusätzlichen \emph{Diskriminator}, der das Unterscheiden zwischen echten und generierten Audiosignalen erlernt. Ein \emph{Vocoder} kann schließlich das rekonstruierte Spektrogramm in ein Audiosignal umwandeln. Abseits von textbasierten Eingaben können Audiosignale auch aus anderen Quellen, wie Bildern und Videos, generiert werden.
 

\begin{figure}[h]
\centering
\begin{subfigure}{1.0\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{graphics/Make-An_Audio.png}
  \caption[Make-An-Audio Architektur]{Make-An-Audio Architektur\cite{huang_make--audio_2023}. Die Parameter der mit einem Schloss gekennzeichnete Module bleiben während Trainings konstant}
  \label{fig:Make-An-Audio_Architecture}
\end{subfigure}

\vspace{1em} % Optional: Add some vertical space between the subfigures

\begin{subfigure}{1.0\textwidth}
  \centering
  \includegraphics[width=0.7\textwidth]{graphics/Make-An-Audio-Destillation.png}
  \caption[Make-An-Audio Destillation]{ake-An-Audio Destillation \cite{huang_make--audio_2023}}
  \label{fig:Make-An-Audio_DestillATIon}
\end{subfigure}
\caption[Make-An-Audio Module]{Make-An-Audio Module}
\label{fig:test}
\end{figure}

Die musikalische Anwendung neuronaler Netze beschränkt sich nicht ausschließlich auf die konditionierte Synthese von Klängen. Vielmehr ermöglichen solche Netzwerke auch eine zielgerichtete musikalische Generierung. Modelle wie \emph{MusicLM} \cite{agostinelli_musiclm_2023} und \emph{MusicGen} \cite{copet_simple_2023} stützen sich auf bewährte Modelle und Ansätze der akustischen Klangsynthese. Sie erweitern diese jedoch durch die Fähigkeit, vollständige musikalische Audiosignale zu generieren, die kohärente Strukturen über längere Zeiträume beibehalten. 

Das Modell \emph{MusicLM} basiert auf den Konzepten von \emph{AudioLM}. Es integriert akustische Informationen über \emph{Soundstream} \cite{zeghidour_soundstream_2021}, semantische Informationen mittels \emph{w2v-Bert} \cite{chung_w2v-bert_2021-1} und Audio-Informationen mithilfe von \emph{Mulan} \cite{huang_mulan_2022}. Im Training dieses Modells erfolgte eine Abbildung des latenten Raums von \emph{MuLan} zum latenten Raum von \emph{w2v-Bert} und ebenso eine Abbildung von \emph{w2v-Bert} zu \emph{Soundstream} unter Verwendung von \emph{Transformers}. Während der Inferenz können die erlernten Abbildungen genutzt werden. Eine von \emph{MuLan} generierte Texteingabe wird hierbei in den latenten Raum von \emph{SoundStream} transformiert, um anschließend in ein Audiosignal kodiert zu werden. \cite{agostinelli_musiclm_2023}


Im Gegensatz zu \emph{MusicLM} setzt \emph{MusicGen} nicht auf mehrere kaskadierende Modelle, sondern verwendet einen \emph{autoregressiven Transformer-basierten Decoder} \cite{vaswani_attention_2017}, der auf Text oder Melodien, wie beispielsweise Summen, konditioniert werden kann. Dieses Modell operiert auf einer komprimierten Darstellung von \emph{gesampelten} Audiosignalen, die durch \emph{EnCodec} \cite{defossez_high_2022} generiert wird. \emph{EnCodec} ist ein \emph{convolutional auto-encoder}, der einen \emph{Latentenraum} mithilfe von \emph{Residual Vector Quantization} \cite{zeghidour_soundstream_2021} erstellt. Abhängig von der Anzahl der in dem Quantifizierungsprozess verwendeten \emph{codebooks} werden entsprechend viele Tokens generiert, die speziell angeordnet und anschließend dem \emph{Decoder} von \emph{EnCodec} übergeben werden. Dieser kann unter der gegebenen Konditionierung eine Audiodatei erstellen. Für die Textkonditionierung wurden Modelle wie \emph{T5} \cite{raffel_exploring_2020}, \emph{FLAN-T5} \cite{roberts_scaling_2022} und \emph{CLAP} evaluiert, während für Melodien eine Konditionierung über ein \emph{Chromagramm} erfolgt.







\chapter{Methoden}

\section{Klangsynthese und Musikproduktion}

\subsection{Mathematische und physikalische Modellierung von Musik und Klang}\label{sec:music_math}
\glqq Musik ist eine Kunstform und kulturelle Aktivität, deren Medium der in Zeit organisierte Klang ist\grqq \footnote{"Music is an art form and cultural activity whose medium is sound organized in time"} \cite{tsuji_physics_2021}. Im Gegensatz zu Rauschen weisen die Klänge, die Musik aufbauen, Strukturen und Zusammenhänge auf, welche für das menschliche Gehör als angenehm wahrgenommen werden \cite{parker_good_2009}. 

\emph{Klang} stellt ein intrinsisches Zusammenspiel aus physikalischen und perzeptiven Elementen dar. Auf physikalischer Ebene handelt es sich bei Klang um eine durch einen schwingenden Körper erzeugte Welle, die sich von einem Ort zum anderen propagiert. Diese Welle besteht aus einem Grundton und mehreren resonierenden Einzeltönen. Der resultierende Klang besitzt eine Vielzahl von Obertönen, die die charakteristischen klanglichen Eigenschaften oder \emph{Klangfarben} hervorbringen. \cite{tsuji_physics_2021, parker_good_2009}

Diese verschiedenen \emph{Töne} sind periodische Schwingungen, definiert durch eine \emph{Tonhöhe/Frequenz} $\omega$ (in $Hz$), welche als die Anzahl der Kompressionen an einem bestimmten Punkt pro Sekunde interpretiert werden kann. Der Kehrwert der Frequenz wird als \emph{Periode} $T=\frac{1}{\omega}$ (in $s$) bezeichnet und beschreibt die Zeit, die eine Kompression benötigt, um zwei identische Punkte zu passieren. Unsere Wahrnehmung lässt Töne mit niedriger Frequenz tief und dumpf klingen, während hohe Frequenzen als leicht, schwebend und durchdringend wahrgenommen werden. Die \emph{Amplitude} der Schwingung beschreibt die transportierte Energie und somit die Lautstärke eines Tones. Aufgrund des Abstandsgesetzes wird diese von einer Schallquelle über die logarithmische \emph{Dezibel-Skala} (dB) angegeben.  \cite{tsuji_physics_2021, parker_good_2009}

Die Untersuchung der Struktur eines Klangs befasst sich mit den Relationen und Zusammenhängen der verschiedenen Töne, aus denen der Klang besteht. Die einfachste Form eines Tones ist der Sinuston, dessen Schwingung durch eine Sinuskurve dargestellt wird. Gemäß dem Fouriertheorem setzt sich jeder andere Ton aus verschiedenen Sinustönen zusammen (Abbildung \ref{fig:evolution}), wobei die tiefste dominante Frequenz als \emph{Grundton} und die höheren Frequenzen als \emph{Obertöne} bezeichnet werden. Die Obertöne unterscheiden sich in Frequenz, Amplitude und zeitlichem Auf- und Abbau. Wenn die Frequenz eines Obertones ein ganzzahliges Vielfaches des Grundtones ist, wird dieser als \emph{harmonischer} Oberton bezeichnet. Im Falle, dass der Oberton kein ganzzahliges Vielfaches des Grundtones ist, spricht man von einem \emph{inharmonischen} Oberton. Daher erzeugen unterschiedliche Instrumente, die die gleiche Note spielen, die gleiche Grundschwingung, weisen jedoch unterschiedliche harmonische und inharmonische Obertöne auf. \cite{parker_good_2009, white_physics_2014, ruschkowski_elektronische_2019}

\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{evolution.png}
  \caption[Fourier Reihe]{Zusammensetzung Sägezahn- und Rechtecksschwingungen aus harmonischen Oberschwingungen einer Sinusschwingung}
  \label{fig:evolution}
\end{figure}

Die Struktur eines Klangs kann durch die Amplitudenfaktoren und zugehörigen Frequenzen der einzelnen Töne beschrieben und in einem \emph{Frequenzspektrum} (Abbildung \ref{fig:spectro}) visualisiert werden. Dies erlaubt die Bestimmung, welche Frequenzen oder Frequenzbereiche im Signal besonders stark enthalten sind und ermöglicht eine Darstellung der Klangfarbe bzw. der Charakteristik. Mit einer \emph{Fourier-Transformation} kann für jedes beliebige Signal das entsprechende Spektrum ermittelt und durch die \emph{Inverse Fourier-Transformation} das Signal eines Spektrums berechnet werden. \cite{raffaseder_audiodesign_2010}

Zur Berechnung einer \emph{Diskreten Fourier-Transformation (DFT)} hat sich die \emph{Fast Fourier-Transformation (FFT)} als effizienter Algorithmus etabliert \cite{heideman_gauss_1985}. Um die Foruier-Transformation in Echtzeit durchzuführen wird die \emph{Short Time Fourier-Transformation} angewandt \cite{thyagarajan_introduction_2019}. 

Ein \emph{Spektrogramm} ermöglicht die Betrachtung eines Signals sowohl im Zeit- als auch im Frequenzbereich (Abbildung \ref{fig:spectro}), allerdings nicht in beliebiger Genauigkeit. Da hier das Signal in kurze zeitliche Abschnitte unterteilt und für diese das Spektrum berechnet wird, verlieren längere Abschnitte Informationen über die zeitliche Auflösung, weisen jedoch eine bessere Frequenzauflösung auf. Kurze Abschnitte hingegen besitzen eine bessere zeitliche Auflösung, lösen jedoch weniger genau im Frequenzbereich auf. \cite{raffaseder_audiodesign_2010}

Ein Klang wird als \emph{Rauschen} bezeichnet, wenn er ein kontinuierliches Frequenzspektrum aufweist, das viele Arten von Geräuschen aus unterschiedlichen Quellen umfasst
\cite{tsuji_physics_2021}. 

Der durch das menschliche Gehör erfassbare Frequenzbereich ($20 Hz$ -  $20kHz$) wird in drei Bereiche (\emph{Bässe, Mitten, Höhen}) unterteilt. Bässe sind in einem Bereich von $20Hz$ bis $250Hz$ auffindbar, während tiefe Mitten den Bereich von 250$Hz$ bis $2000Hz$ umschließen. Hohen Mitten erstrecken sich von $2kHZ$ bis $4kHz$. Die Höhen liegen oberhalb von $4kHz$. \cite{raffaseder_audiodesign_2010}

\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{spectrums_flat.png}
  \caption[Fourier Reihe]{Spektren, Spektrogramme und Mel-Spektrogramme verschiedener Klänge}
  \label{fig:spectro}
\end{figure}


\subsection{Digitale Audiorepräsentation und Sampling}

Bedingt durch die binäre Natur digitaler Computer ist für die Repräsentation und Verarbeitung von Audiosignalen eine Transformation des Signals von einem kontinuierlichen zu einem diskreten Wertebereich erforderlich. Das \emph{Abtasttheorem} besagt, dass analoge Signale überflüssige Informationen enthalten, die bei einer Übertragung weggelassen werden können. Es reicht somit aus, das Audiosignal zu reproduzieren, indem Amplitudenwerte in regelmäßigen Zeitabständen entnommen und übertragen werden. Um das Signal vollständig beschreiben zu können, muss die Abtastrate mindestens das Zweifache der höchsten Frequenz des Signals betragen. Dieser Grenzwert wird als \emph{Nyquist-Frequenz} definiert und entspricht $2 \omega \mathrm{Hz}$. Da das menschliche Gehör nur Frequenzen bis zu $20kHz$ wahrnimmt, wird in der Praxis häufig eine \emph{Abtastrate} von $44.1kHz$ verwendet. Fällt die Abtastrate unter die Nyquist-Frequenz, gehen Informationen verloren und die wiederhergestellte Wellenform kann von der originalen Form abweichen. In einem solchen Fall ist der Prozess der Signalerneuerung nicht mehr deterministisch, was als \emph{Unterabtastung} bezeichnet wird. \cite{lai_practical_2004, shannon_communication_1949, ruschkowski_elektronische_2019}

Die \emph{Samplingtiefe (engl. Bitdepth)} gibt den Wertebereich an, in dem die ausgelesenen Amplitudenwerte abgebildet werden. Je größer dieser ist, desto treuer kann das original Signal repräsentiert werden, jedoch wird hierfür auch mehr Speicher benötigt. Fällt der Wertebereich klein aus, so müssen die Amplitudenwerte stärker gerundet werden und daher Artefakte und Fehler im Signal entstehen. \cite{thompson_understanding_2005}

\emph{Sound-Sampling} oder \emph{Sampling} als Kombination mit digitaler/analoger Klangmanipulation ist eine eigenständige Klangsynthesetechnik, obwohl es im Kern nur eine Wiedergabe des entnommen Signals ist, und somit nicht nur eine einfache Speicherung. Die Prinzipen und Methoden aus der analogen Synthetisierung und Klangformung können auf die Wiedergabe des Klanges angewandt werden. Das entstandene Instrument wird als \emph{Sampler} bezeichnet. Je nach Komplexität des Instruments erlaubt es den Spieler, das Signal in einem gewünschten Bereich zu wiederholen, es zu verlangsamen, verschnellern, umzudrehen, oder bestimmte Intervalle neu zu arrangieren. Dadurch eröffnet sich ebenfalls die Möglichkeit des musikalischen Zitierens und damit auch das Problem des klanglichen Diebstahls für Musiker, Verlage und Plattenfirmen. \cite{russ_sound_2009, ruschkowski_elektronische_2019, katz_capturing_2010} 

Die Wiederverwendung und Aufarbeitung von Ausschnitten oder kompletten schon produzierten Werken ist in der Industrie Gang und Gebe und etwas was mit Hilfe von generativer Küntslichen Intelligenz vermieden werden kann. 

\subsection{Synthetisierung und Klangformung} \label{sec:synth+envelope}

Instrumente, die zur Generierung und Manipulation von musikalischen Klängen durch elektronische Spannung dienen, werden als Synthesizer bezeichnet \cite{dudenredaktion_synthesizer_nodate, pirkle_designing_2021}. Die Komponenten eines solchen Instruments können in drei Kategorien unterteilt werden (Abbildung \ref{fig:synth}): \emph{Erzeuger} (eng. \emph{source}), \emph{Modifikatoren} und \emph{Kontrollinstanzen}. Erzeuger wie Oszillatoren generieren den ursprünglichen Klang, der in diesem spezifischen Projekt mittels Diffusion erzeugt wird. Modifikatoren, darunter Filter und Effekte, manipulieren diesen erzeugten Klang. Kontrollinstanzen bieten dem Anwender die Möglichkeit, die Parameter der Erzeuger und Modifikatoren einzustellen und dynamisch anzupassen. Das Manipulieren eines Parameters wird als \emph{Modulieren} bezeichnet. Die Spezifischen Einstellungen der Parameters als \emph{Patch}. \cite{pirkle_designing_2021}   

Die für das Umsetzen des Projektes benötigten Komponenten werden im Folgenden erklärt. 

\begin{figure}[h]
  \centering
  \includegraphics[width=.5\textwidth]{graphics/synthstruc.png}
  \caption[Synth]{Komponenten eines Synthesizers \cite{russ_sound_2009}}
  \label{fig:synth}
\end{figure}

In dem Bestreben, sein erworbenes wissenschaftliches Verständnis und seine Expertise in die Konstruktion musikalischer Instrumente einzubringen, entwickelte der Physiker und Komponist Hugh Le Caine erste Geräte zur elektronischen Klangsynthese, 20 Jahre vor der Kommerzialisierung der ersten analogen Synthesizer \cite{young_gale_hugh_2013}. Le Caine formuliert in diesem Kontext eine Reihe von Prinzipien, die ein Instrumentenbauer seiner Ansicht nach anstreben sollte. Er vertrat die Überzeugung, dass ein Spieler die maximale Kontrolle über die wichtigsten Parameter eines Klangs besitzen sollte. Er erkannte, dass sich die Expressivität eines Instruments insbesondere durch die Kontrolle der \emph{Anschlagsdynamik} über den gesamten Lautstärkebereich eines Tones, durch die Kontrolle seiner \emph{An- und Ausschwingszeiten} sowie der Klangfarbe definiert. \cite{ruschkowski_elektronische_2019}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\textwidth]{graphics/ADSR.png}
  \caption[ADSR]{Beispiel einer Hüllkurve \cite{russ_sound_2009}}
  \label{fig:adsr}
\end{figure}

Generell wird danach gestrebt, Klänge zu erzeugen, die sich sowohl in der Zeit als auch in der Frequenz entwickeln, verändern oder morphieren, um klanglich interessante Ereignisse zu schaffen \cite{pirkle_designing_2021}. Die Kontrolle über Anschlagsdynamik sowie An- und Ausschwingszeiten wird mittels eines Hüllkurvengenerators (engl. envelope generator) erreicht. Ein solcher Generator setzt sich typischerweise aus vier Komponenten zusammen. Die Anschwellzeit (engl. attack time) definiert das Intervall, welches nach Betätigen einer Taste verstreicht, bis ein Ton seine maximale Lautstärke erreicht. Analog dazu kennzeichnet die Abschwellzeit (engl. decay time) den Zeitraum, in dem der Ton bis zu einem vorab festgelegten Lautstärkelevel (engl. sustain level) absinkt. Dieses Level wird solange aufrechterhalten, bis die Taste wieder freigegeben wird. Nach dieser Freigabe kann ein weiteres Zeitintervall festgelegt werden, in dem der Ton vollständig ausklingt (engl. release time). Aufgrund der Nomenklatur dieser Komponenten wird der Generator häufig als ADSR-Generator(engl. \emph{attack, decay sustain, release}) (Abbidlung \ref{fig:adsr})  bezeichnet. Instrumente, wie beispielsweise mit einem Bogen gespielte Streichinstrumente, weisen lange Anschlag-, Abschwell- und Ausschwingzeiten haben. Im Gegensatz dazu haben Zupfinstrumente kürzere Angriffszeiten. Klaviere und Schlaginstrumente haben sehr schnelle Angriffszeiten. \cite{ruschkowski_elektronische_2019, russ_sound_2009}

Die durch den spezifischen Erzeuger entstandene Klangfarbe, kann mittels Filtern weiterhin modifiziert werden. In der Elektrotechnik dienen diese dazu, bestimmte Frequenzbereiche zu eliminieren. Dabei spielen vor allem der sogenannte \emph{Tiefpassfilter} (engl. \emph{low-pass}) und \emph{Hochpassfilter} (engl. \emph{high-pass}) eine zentrale Rolle, welche jeweils die höheren oder tieferen Bereiche des Spektrums unterdrücken. Der Grenzwert, an dem diese Unterdrückung initiiert wird, ist als \emph{Beschneidungsfrequenz} (engl. \emph{cut-off-frequency}) bekannt. Mithilfe der genannten Filter besteht die Möglichkeit, gewünschte oder ungewünschte Obertöne zu dämpfen oder zu isolieren. Eine sequenzielle Anordnung dieser Filter resultiert in einem \emph{Bandpassfilter}. Oftmals bietet sich zudem die Gelegenheit, Obertöne im Bereich der Beschneidungsfrequenz zu intensivieren, wodurch eine \emph{Resonanz} (engl. \emph{resonance}) simuliert wird. \cite{ruschkowski_elektronische_2019}

Kontrollinstanzen lassen sich in zwei Kategorien untergliedern. Die erste Kategorie umfasst Performance-gesteuerte Steuerungselemente, wie beispielsweise ein Klaviaturkeyboard, welches dem Musiker ermöglicht, die Tonhöhe nach seinen individuellen Präferenzen anzupassen und somit das Instrument zu manipulieren sowie zu beherrschen. Die zweite Kategorie besteht aus festen, synthesizerspezifischen Parametern, die dazu dienen, den Charakter des gewünschten Klangs einzustellen und zu formen. Hierzu zählen beispielsweise die Regler und Knöpfe eines Synthesizers. \cite{russ_sound_2009}

Für die Vereinheitlichung der Parametersteuerung und Kommunikation zwischen elektronischen Geräten wurde das Kommunikationsprotokoll MIDI (Musical Instrument Digital Interface)\cite{midi_association_midi_nodate} entwickelt . Dieses hat sich als internationaler Standard in der Welt der elektronischen Musik durchgesetzt und ermöglicht es MIDI-Instrumenten, Informationen zu empfangen und zu senden. \cite{ruschkowski_elektronische_2019}

Die Einführung dieses Protokolls beeinflusste signifikant die Konzeption und Gestaltung von Synthesizern. Durch die Einheitlichkeit vieler Aspekte des Synthesizer-Designs, bedingt durch das MIDI-Protokoll, rückte die Methode der Klangerzeugung stärker in den Vordergrund, während das funktionale Design des Instruments in den Hintergrund trat. \cite{russ_sound_2009}

\section{Synthetisierung mittels Diffusion}

\glqq Die Methoden der Klangerzeugung mit traditionellen mechanischen Musikinstrumenten waren seit ihrer Entstehung nur unwesentlichen Veränderungen unterworfen. [...] Der Instrumentenbau wandelte sich lediglich deshalb im Laufe der Zeit, weil man die Spielbarkeit der Instrumente zu verbessern und ihren Klangcharakter den wechselnden musikalischen Idealvorstellungen anzupassen wünschte. Anders dagegen im Bereich elektronischer Klangerzeugung. Hier werden ständig neue Methoden zur Synthese von Klängen entwickelt.\grqq \, \cite{ruschkowski_elektronische_2019}

Im Folgenden soll zunächst dargelegt werden, welche Vorteile die neuartige Technik der Soundsynthese durch Diffusion mit sich bringen kann. Bei Erzielung von hochwertigen Ergebnissen könnten beispielsweise die Notwendigkeit und der Zugang zu aufgezeichneten Klängen entfallen. Der Aufwand für die Aufnahme solcher Klänge ist oft erheblich und führt daher in der Regel zu kostenpflichtigen Angeboten. Die Anwendung künstlicher Intelligenz in der Synthese von Sound könnte jedoch dazu beitragen, diesen Bereich zu demokratisieren und einem breiteren Publikum die benötigten Ressourcen zur Verfügung zu stellen. Darüber hinaus könnte diese Technik den Nutzern eine umfassendere Kontrolle über die gewünschten Eigenschaften und Klangfarben verleihen. Auf kreativer Ebene ermöglicht die Soundsynthese durch Diffusion die Erzeugung einzigartiger, zuvor nicht existierender Klänge, die potenziell als neue Inspirationsquelle für Künstler und Musiker dienen könnten. \cite{haohe_liu_audioldm_2023}


\subsection{Diffusion}
Unter der Prämisse, dass die untersuchten Daten einer bestimmten Verteilung entstammen, verfolgen generative Machine-Learning-Modelle das Hauptziel, diese Verteilung zu identifizieren und zu approximieren. Die Generierung neuer Daten basiert auf der Entnahme einer Probe aus der approximierten Verteilung.\cite{machine_learning_at_berkeley_diffusion_2022}

Der im Kontext des Maschinellen Lernens als \emph{Diffusion} bekannte Prozess\cite{sohl-dickstein_deep_2015, ho_denoising_2020, nichol_improved_2021, dhariwal_diffusion_2021} schöpfte seine Inspiration aus der statistischen Thermodynamik\cite{jarzynski_equilibrium_1997} und der sequenziellen Monte-Carlo-Methodik\cite{neal_annealed_1998}. Hierbei zielt man darauf ab, iterative korrumpierte Strukturen einer Datenverteilung $x_0\sim q(\mathbf{x}_0)$ (\emph{forward diffusion}) durch einen Rückwärtsprozess zu korrigieren (\emph{reverse diffusion}). Dies resultiert in einem generativen Modell, welches effizient trainiert und bewertet werden kann als vergleichbare Methoden.\cite{sohl-dickstein_deep_2015, nichol_improved_2021}

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\textwidth]{forwardDiffusion.png}
  \caption[Forward diffusion]{Vorwärtsprozess Diffusion \cite{machine_learning_at_berkeley_diffusion_2022}}
  \label{fig:forwardDiffusion}
\end{figure} 

Der Vorwärtsprozess (Abbildung \ref{fig:forwardDiffusion}) wurde als Markov-Kette konzipiert\cite{sohl-dickstein_deep_2015, ho_denoising_2020}. Einem Datenpunkt $x_0$ wird eine minimale Menge an Rauschen hinzugefügt, welches durch den Hyperparameter $\left\{\beta_t \in(0,1)\right\}_{t=1}^T$, als \emph{noise schedule} bekannt, definiert wird\cite{ho_denoising_2020, machine_learning_at_berkeley_diffusion_2022}. Dabei wird \emph{Gauss'sches Rauschen}\cite{shannon_communication_1949} verwendet, welches durch eine Normalverteilung $\mathcal{N}$ beschrieben wird. Wird dieser Prozess über $T$-Schritte wiederholt, verliert man nach und nach Informationen, bis letztlich nur isotropisches Rauschen zurückbleibt\cite{machine_learning_at_berkeley_diffusion_2022}.

\begin{align}
   & q\left(\mathbf{x}^{(t)} \mid \mathbf{x}^{(t-1)}\right) = \mathcal{N}\left(\mathbf{x}^{(t)} ; \mathbf{x}^{(t-1)} \sqrt{1-\beta_t}, \mathbf{I} \beta_t\right) \\
   & q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)=\prod_{t=1}^T q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)
\end{align}

Mit den Beziehungen \( \alpha_t:=1-\beta_t \) und \( \bar{\alpha}_t:=\prod_{s=1}^t \alpha_s \) ergibt sich für den Vorwärtsprozess die Gleichung:

\begin{equation}
    q\left(\mathbf{x}_t \mid \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_t ; \sqrt{\bar{\alpha}_t} \mathbf{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)
\end{equation}

\begin{figure}[h]
  \centering
  \includegraphics[width=.8\textwidth]{reverseDiffusion.png}
  \caption[Reverse diffusion]{Rückwertsprozess Diffusion \cite{machine_learning_at_berkeley_diffusion_2022}}
  \label{fig:reverseDiffusion}
\end{figure} 

Im Rückwärtsprozess (Abbildung \ref{fig:reverseDiffusion}) wird angestrebt, das hinzugefügte Rauschen $\epsilon \sim \mathcal{N}(0,1)$ iterativ zu eliminieren \cite{machine_learning_at_berkeley_diffusion_2022}. Dadurch erscheint es, als würde der inverse Prozess neue Daten aus Rauschen generieren \cite{machine_learning_at_berkeley_diffusion_2022}. Bei geringem $\beta$ entspricht die Rauschverteilung des Rückwärtsschritts \( q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right) \) dem Vorwärtsprozess \cite{sohl-dickstein_deep_2015}. Der erlernte Rückwärtsprozess \( p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right) \) kann daher wie folgt approximiert werden \cite{ho_denoising_2020, machine_learning_at_berkeley_diffusion_2022, nichol_improved_2021}. Der Lernprozess konzentriert sich darauf, geringfügige Abweichungen zu schätzen, anstatt den gesamten Prozess in einem Schritt durch eine Funktion darzustellen \cite{sohl-dickstein_deep_2015}.


\begin{align}
& p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_\theta\left(\mathbf{x}_t, t\right), \boldsymbol{\Sigma}_\theta\left(\mathbf{x}_t, t\right)\right)\\
& p_\theta\left(\mathbf{x}_{0: T}\right)=p\left(\mathbf{x}_T\right) \prod_{t=1}^T p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)
\end{align}

Das Erlernen des Rückwärtsprozesses erfolgt durch ein neuronales Netzwerk, wobei in jedem Schritt der Erwartungswert $\boldsymbol{\mu}_\theta$, das ursprüngliche Bild $\boldsymbol{x}_0$ oder das eingefügte Rauschen $\boldsymbol{\epsilon}$ vorhergesagt werden können \cite{ho_denoising_2020, nichol_improved_2021}. Es wurde festgestellt, dass Bildvorhersagen schlechtere Ergebnisse erzielten und daher sich für die Vorhersage des eingefügten Rauschens $\boldsymbol{\epsilon}$ mit der vereinfachten Verlustfunktion 3.6 entschieden wurde \cite{ho_denoising_2020}.

\begin{equation}
    L_{\text {simple }}=E_{t, x_0, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(x_t, t\right)\right\|^2\right]
\end{equation}

Aus dem vorhergesagten Rauschen $\boldsymbol{\epsilon}$ kann der Erwartungswert $\boldsymbol{\mu}_\theta$ mit folgender Gleichung hergeleitet werden:

\begin{equation}
    \mu_\theta\left(x_t, t\right)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta\left(x_t, t\right)\right)
\end{equation}

Die in \cite{ho_denoising_2020} präsentierte Architektur des neuronalen Netzwerks basiert auf einem \emph{U-Net} \cite{ronneberger_u-net_2015}, welches auf einem \emph{Wide ResNet} \cite{zagoruyko_wide_2017} aufbaut \cite{ho_denoising_2020}.

\cite{nichol_improved_2021} führte bedeutende Verbesserungen gegenüber den Arbeiten von \cite{sohl-dickstein_deep_2015} und \cite{ho_denoising_2020} ein. Anstatt mit einer festen Varianz zu trainieren, wurde diese nun ebenfalls gelernt. Ein weiterer bedeutender Fortschritt war die Anpassung des \emph{schedule} Parameters. Anstatt ihn konstant zu halten, was dazu führte, dass Datenpunkte am Ende zu stark verrauscht waren und die initialen Schritte zu viele Informationen verloren, wurde eine \emph{cosinus schedule} eingeführt \cite{nichol_improved_2021}.

\subsection{Latente Diffusion}

Um Bildgenerierung basierend auf einer Eingabe zu ermöglichen, erweiterten \cite{rombach_high-resolution_2022} mit dem vorgestellten  \emph{Latenten Diffusionsmodell} (\emph{LDM}) den Diffusionsprozesses \cite{sohl-dickstein_deep_2015, ho_denoising_2020, nichol_improved_2021, dhariwal_diffusion_2021}. Im Gegensatz zu vorhergehenden Diffusionsprozessen arbeitet dieser Ansatz nicht auf die Pixelwerte der Bilder, sondern auf ihre latente Repräsentation. Dies führt zu einer erheblichen Reduzierung des Rechenaufwands und erlaubt sowohl Training als auch Inferenz auf beschränkter Hardware. Die Architektur, dargestellt in Abbildung \ref{fig:LDM}, setzt sich aus drei Hauptkomponenten zusammen: Einem \emph{Variational Autoencoder}, der die Ein- und Ausgabe im Latentenraum kodiert bzw. dekodiert, dem Diffusionsteil, sowie einem weiteren Modul zur Konditionierung des Diffusionsteils mittels Text, Bildern oder anderen Eingaben \cite{rombach_high-resolution_2022}.

\begin{figure}[h]
  \centering
  \includegraphics[width=.8\textwidth]{LDM.png}
  \caption[LDM Architektur]{LDM Architektur\cite{rombach_high-resolution_2022}}
  \label{fig:LDM}
\end{figure} 

Der latente Raum für den Diffusionsprozess wird durch vorausgehendes Training eines \emph{Variational Autoencoders} \cite{kingma_auto-encoding_2022} gemäß \cite{esser_taming_2021} erzeugt. Der resultierende \emph{Encoder} $\mathcal{E}$ transformiert ein RGB-Bild $x \in \mathbb{R}^{H \times W \times 3}$ in die latente Repräsentation $z \in \mathbb{R}^{h \times w \times c}$, so dass $z=\mathcal{E}(x)$. Ein \emph{Decoder} $\mathcal{D}$ generiert nach dem Rückwärtsdiffusionsprozess das Bild $\tilde{x}=\mathcal{D}(z)=\mathcal{D}(\mathcal{E}(x))$ \cite{rombach_high-resolution_2022}.

Der Diffusionsprozess profitiert von der niedrigen Dimensionalität und Kompression des latenten Raumes und zeigte sich gegenüber der Nutzung der originellen Pixelwerten als geeigneter für \emph{likelihood} basierte Generative Modelle, da mehr die semantischen wichtigen Information mehr zur Geltung kommen und der Rechenaufwand effizienter gehalten wird. Das zugrunde liegende \emph{U-Net} \cite{ronneberger_u-net_2015} besteht, um den Umgang mit Bildern gerecht zu werden, primär aus \emph{2D-Konvolutionsschichten}. \cite{rombach_high-resolution_2022}

Um die von einer Eingabe $y$ bedingten Generierung zu ermöglichen ist eine Konditionierung des Netzes $\epsilon_\theta\left(z_t, t, y\right)$ notwendig. Hierfür wird das benutze \emph{U-Net} um \emph{Cross Attention}\cite{vaswani_attention_2017} erweitert. Um die verschiedenen Eingabmodalitäten zu unterstützen verarbeiten zu können, schafft eine je nach Medium spezifischer Encoder $\tau_\theta$ eine Repräsentation $T_\theta(y) \in \mathbb{R}^{M \times d_\tau}$, welche mittels \emph{Cross Attention} $\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d}}\right)$, mit $Q=W_Q^{(i)} \cdot \varphi_i\left(z_t\right), K=W_K^{(i)} \cdot \tau_\theta(y), V=W_V^{(i)} \cdot \tau_\theta(y)$ auf die Schichten des \emph{U-Net} übertragen wird. Die Verlustfunktion definert sich folgendermaßen. \cite{rombach_high-resolution_2022}

\begin{equation}
L_{L D M}:=\mathbb{E}_{\mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0,1), t}\left[\left\|\epsilon-\epsilon_\theta\left(z_t, t, \tau_\theta(y)\right)\right\|_2^2\right]
\end{equation}


\subsection{Clap}

\emph{Large-Scale Contrastive Language-Audio Pretraining} (\emph{Clap})\cite{wu_large-scale_2023} präsentierte ein Modell, um Vortraining mittels \emph{Contrastive Learning} auf Sprach-Audio-Daten durchzuführen, mit dem Ziel, eine latente Repräsentation für Audio zu schaffen. Dieses Modell orientierte sich an der Architektur von \emph{Contrastive Language-Image Pretraining} (\emph{CLIP})\cite{radford_learning_2021}, bei dem in ähnlicher Weise ein Zusammenhang zwischen Bild und Textdaten erlernt wird. Analog zum bildlichen Kontext weisen Audio und Text überlappende Informationen auf. \cite{wu_large-scale_2023}

Das \emph{Contrastive Learning}-Modell wurde auf dem speziell veröffentlichten Datensatz \emph{LAION-Audio-630K} trainiert. Dieser Datensatz besteht aus $633,526$ Text- und Audio-Paaren ($X_i^a$, $X_i^t$), zusammengesetzt aus verschiedenen Internetquellen, einschließlich Klängen wie menschlichen Stimmen, Naturgeräuschen und Audioeffekten. Weitere verwendete Datensätze sind \emph{AudioCaps+Clotho} \cite{kim_audiocaps_2019} \cite{drossos_clotho_2019}, sowie \emph{AudioSet} \cite{gemmeke_audio_2017}. Alle Audiodaten werden zu einem \emph{Mono}-Signal konvertiert und besitzen eine Abtastrate von 48kHz. \cite{wu_large-scale_2023}

\begin{figure}[h]
  \centering
  \includegraphics[width=.6\textwidth]{Clap.png}
  \caption[Clap Architektur]{CLAP Architektur \cite{wu_large-scale_2023}}
  \label{fig:Clap}
\end{figure} 

Die Architektur des Modells (Abbildung \ref{fig:Clap}) erzeugt Embeddings $E_i^a$, $E_i^t$ der Audio- $X_i^a$ und Texteingabe $X_i^t$, indem zuerst jeweils ein Encoder $f(\cdot)$ verwendet wird, dessen Ergebnis durch ein 2-Layer großes \emph{Multi-Layer-Perceptron} (\emph{MLP}) mit \emph{ReLU}\cite{agarap_deep_2019} als Aktivierungsfunktion verarbeitet wird. \cite{wu_large-scale_2023}

\begin{align}
  E_i^a & = M L P_{\text{audio}}\left(f_{\text{audio}}\left(X_i^a\right)\right) \\
  E_i^t & = M L P_{\text{text}}\left(f_{\text{text}}\left(X_i^t\right)\right)
\end{align}

Das gesamte Audiosignal zu encodieren, würde bei Signalen mit längerer Dauer zu viel Rechenzeit in Anspruch nehmen. Stattdessen wurde ein Ansatz verfolgt, bei dem verschiedene Längen von Audioeingaben in konstanter Rechenzeit trainiert und inferiert wurden. Hierzu wurden sowohl globale als auch lokale Informationen kombiniert. Falls das Signal kürzer als 10 Sekunden ist, wird es dreimal wiederholt und mit Nullen aufgefüllt, bis die 10 Sekunden erreicht sind. Wenn das Signal länger als 10 Sekunden ist, werden daraus vier Eingabeparameter generiert. Das Signal wird einmal komplett zu einem 10 Sekunden globalen Parameter \emph{downgesampled} und einmal werden drei zufällige 10 Sekunden Ausschnitte aus Anfang, Mitte und Ende des Signals als lokale Parameter entnommen. Der Audio-Encoder extrahiert anschließend die wichtigen Informationen und verschmilzt globale und lokale Informationen.

Da einige der verwendeten Datensätze keine Beschreibungen, sondern nur Keywords oder Tags für das jeweilige Audiosignal besitzen, wurde das \emph{Language}-Modell \emph{T5}\cite{raffel_exploring_2020} eingesetzt, um vollständige Beschreibungen aus den Keywords und Tags zu generieren. Auch wurden die Beschreibungen \emph{de-biased}, beispielsweise durch \emph{gender de-biasing}. \cite{wu_large-scale_2023}

Die gleiche Verlustfunktion mit einem Temperaturparameter $\tau$, wie in \emph{Clip}\cite{radford_learning_2021}, wurde in den MLPs verwendet \cite{wu_large-scale_2023}.

\begin{equation}
L=\frac{1}{2 N} \sum_{i=1}^N\left(\log \frac{\exp \left(E_i^a \cdot E_i^t / \tau\right)}{\sum_{j=1}^N \exp \left(E_i^a \cdot E_j^t / \tau\right)}+\log \frac{\exp \left(E_i^t \cdot E_i^a / \tau\right)}{\sum_{j=1}^N \exp \left(E_i^t \cdot E_j^a / \tau\right)}\right)
\end{equation}

Als Audio-Encoder wurden die Modelle \emph{PANN}\cite{kong_panns_2020}, basierend auf einem \emph{Convolutional Neural Network} (CNN), und \emph{HTSAT}\cite{chen_hts-at_2022}, basierend auf einem \emph{Transformer}-Modell, evaluiert. Für den Text-Encoder wurden der Text-Encoder von \emph{Clip}\cite{radford_learning_2021}, \emph{Bert}\cite{devlin_bert_2019}, oder \emph{RoBERTa}\cite{liu_roberta_2019} ebenfalls ausgewertet. \cite{wu_large-scale_2023}

Das trainierte Modell kann für Audio-Klassifikation verwendet werden oder ein passendes Audiosignal für einen Text bestimmen $(T\rightarrow A)$ \cite{wu_large-scale_2023}. Für die Klangsynthese ist der Prozess von Text zu Audio von Bedeutung. Die Ergebnisse der Kombination der verschiedenen Encoder aus Tabelle \ref{tab:Clap} zeigen, dass \emph{HTSAT}\cite{chen_hts-at_2022} als Audio-Encoder und je nach Datensatz \emph{Bert}\cite{devlin_bert_2019} oder \emph{RoBERTa}\cite{liu_roberta_2019} als Text-Encoder die besten Resultate lieferten. \cite{wu_large-scale_2023}

\begin{table}[h]
  \centering
\begin{tabular}{lcc|cc}
\hline \multirow{2}{*}{ Model } & \multicolumn{2}{c|}{ AudioCaps $(\mathrm{mAP} @ 10)$} & \multicolumn{2}{c}{ Clotho (mAP@ 10) } \\
\cline { 2 - 5 } & $\mathrm{A} \rightarrow \mathrm{T}$ & $\mathrm{T} \rightarrow \mathrm{A}$ & $\mathrm{A} \rightarrow \mathrm{T}$ & $\mathrm{T} \rightarrow \mathrm{A}$ \\
\hline PANN+CLIP Trans. & 4.7 & 11.7 & 1.9 & 4.4 \\
PANN+BERT & 34.3 & 44.3 & 10.8 & 17.7 \\
PANN+RoBERTa & 37.5 & 45.3 & 11.3 & 18.4 \\
HTSAT+CLIP Trans. & 2.4 & 6.0 & 1.1 & 3.2 \\
HTSAT+BERT & 43.7 & 49.2 & $\mathbf{1 3 . 8}$ & $\mathbf{2 0 . 8}$ \\
HTSAT+RoBERTa & $\mathbf{4 5 . 7}$ & $\mathbf{5 1 . 3}$ & $\mathbf{1 3 . 8}$ & 20.4 \\
\hline
\end{tabular}
\caption[Encoder CLAP]{Evaluation der Encoder \cite{wu_large-scale_2023}}
  \label{tab:Clap}
\end{table}


\subsection{AudioLDM}
\label{sec:AudioLDM}

Das Modell \emph{AudioLDM}\cite{liu_audioldm_2023} dient zur Synthese von Text zu Audio sowie zur textgesteuerten Audio-Manipulation mittles \emph{Latenten Diffusion}\cite{rombach_high-resolution_2022}. Es verspricht qualitativ hochwertige Ergebnisse bei geringem Rechenaufwand. Im Gegensatz zu \emph{Diffsound}\cite{yang_diffsound_2023}, das Audio-Text-Paare für das Training verwendet, hat \emph{AudioLDM} gezeigt, dass mithilfe einer \emph{CLAP}-Repräsentation\cite{wu_large-scale_2023} hochwertige Resultate erzielt werden können. \cite{liu_audioldm_2023}

Die Wahl, Natürliche Sprache anstelle von Labels für die Eingabe und Konditionierung zu verwenden, erfolgte aufgrund der Möglichkeit, akustische Merkmale flexibler, deskriptiver und präziser darzustellen. Frühere Ansätze im Bereich Text-zu-Audio (TTA) stießen auf die Grenze, dass oft keine großen Mengen qualitativ hochwertiger Audio-Text-Daten verfügbar waren\cite{liu_separate_2022}. Methoden der Textvorverarbeitung\cite{gemmeke_audio_2017, yang_diffsound_2023} konnten dieses Problem nicht gänzlich lösen, da sie komplexe Zusammenhänge zwischen Klangereignissen vereinfachten, was die Generierungsleistung beeinträchtigte. Das Modell \emph{AudioLDM} adressierte diese Herausforderung, indem es ausschließlich auf Audiodaten im Training setzte und so bessere Ergebnisse erzielte als mit gepaarten Audio-Text-Daten. \cite{liu_audioldm_2023}


\begin{figure}[h]
  \centering
  \includegraphics[width=.6\textwidth]{AudioLDM.png}
  \caption[AudioLDM Architektur]{Architektur für die Text-zu-Audio Generierung von AudioLDM\cite{liu_audioldm_2023}}
  \label{fig:AudioLDM}
\end{figure} 

Die Architektur (Abbildung \ref{fig:AudioLDM}) kombiniert mehrere Komponenten: Einen \emph{VAE}\cite{kingma_auto-encoding_2022}, der Mel-Spektrogramme in einen latenten Raum kodiert; einen nach \emph{CLAP}\cite{wu_large-scale_2023} konzipierten Audio- und Text-Encoder, der Embeddings für die Konditionierung des \emph{LDMs} erstellt; einen \emph{LDM}, der die zugrunde liegende Verteilung erlernt; und einen \emph{Vocoder}, der das generierte Spektrogramm in ein Audiosignal umwandelt. \cite{liu_audioldm_2023}

Die \emph{CLAP}-Module generieren Embeddings für Text $\boldsymbol{E}^y \in \mathbb{R}^L$ und Audio $\boldsymbol{E}^x \in \mathbb{R}^L$. Hierbei repräsentiert $x$ eine Audioprobe und $y$ eine Textbeschreibung, wobei jeweils $f_{\text {text }}(\cdot)$ und $f_{\text {audio }}(\cdot)$ als Encoder dienen. Für den Audioencoder wurde \emph{HTSAT}\cite{chen_hts-at_2022} und für den Textencoder \emph{RoBERTa}\cite{liu_roberta_2019} verwendet. Beim Training des \emph{LDMs} diente $\boldsymbol{E}^x$ aus dem jeweiligen Trainingsdatenpunkt als Konditionierung, während $\boldsymbol{E}^y$ zur Generierung des gewünschten Signals Verwendung findet. \cite{liu_audioldm_2023}

Für den \emph{LDM}-genutzten Latentraum $\boldsymbol{z} \in \mathbb{R}^{C \times \frac{T}{r} \times \frac{F}{r}}$ fand ein Training mithilfe eines \emph{VAE} auf Spektrogrammen statt. Dabei repräsentiert $r$ das \emph{Kompressionslevel}, während $T$ und $F$ für Zeit- und Frequenzdimensionen stehen. $C$ gibt die Anzahl der Kanäle an. Ein Spektrogramm $\boldsymbol{X} \in \mathbb{R}^{T \times F}$ wird durch \emph{STFT} (Referenz \ref{sec:music_math}) aus einem gegebenen Audiosignal generiert. Im Generierungsverfahren wird aus der latenten Variable $\hat{\boldsymbol{z}}_o$ ein Spektrogramm $\hat{\boldsymbol{X}}$ erstellt. Dieses Spektrogramm wird schließlich durch den \emph{Vocoder}, basierend auf \emph{HiFi-GAN}\cite{kong_hifi-gan_2020}, in ein Audiosignal $\hat{x}$ transformiert. \cite{liu_audioldm_2023}

Das konzeptionelle \emph{LDM} hat das Ziel, die grundlegende Verteilung $q\left(\boldsymbol{z}0 \mid \boldsymbol{E}^y\right)$ mithilfe von $p\theta\left(\boldsymbol{z}_0 \mid \boldsymbol{E}^y\right)$ anzunähern. Dabei wurden Verlustfunktion und der Rückwärtsprozess wie folgt spezifiziert: \cite{liu_audioldm_2023}

\begin{equation}
    L_n(\theta)=\mathbb{E}_{\boldsymbol{z}_0, \boldsymbol{\epsilon}, n}\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{z}_n, n, \boldsymbol{E}^x\right)\right\|_2^2
\end{equation}
\begin{equation}
    p_\theta\left(\boldsymbol{z}_{n-1} \mid \boldsymbol{z}_n, \boldsymbol{E}^y\right)=\mathcal{N}\left(\boldsymbol{z}_{n-1} ; \boldsymbol{\mu}_\theta\left(\boldsymbol{z}_n, n, \boldsymbol{E}^y\right), \boldsymbol{\sigma}_n^2 \boldsymbol{I}\right)
\end{equation}
\begin{equation}
    p_\theta\left(\boldsymbol{z}_{0: N} \mid \boldsymbol{E}^y\right)=p\left(\boldsymbol{z}_N\right) \prod^N p_\theta\left(\boldsymbol{z}_{n-1} \mid \boldsymbol{z}_n, \boldsymbol{E}^y\right) \\
\end{equation}

Der Erwartungswert ist folgendermaßen parametrisiert und berechnet sich aus dem im Rückwärtsprozess ermittelten Rauschen $\boldsymbol{\epsilon}_\theta\left(\boldsymbol{z}_n, n, \boldsymbol{E}^y\right)$ \cite{liu_audioldm_2023}.

\begin{equation}
    \boldsymbol{\mu}_\theta\left(\boldsymbol{z}_n, n, \boldsymbol{E}^y\right)=\frac{1}{\sqrt{\alpha_n}}\left(\boldsymbol{z}_n-\frac{\beta_n}{\sqrt{1-\bar{\alpha}_n}} \boldsymbol{\epsilon}_\theta\left(\boldsymbol{z}_n, n, \boldsymbol{E}^y\right)\right)
\end{equation}

Des weiterem wurde gezeigt, dass der \emph{Cross-Attention}-Mechanismus aus \cite{rombach_high-resolution_2022} weggelassen werden kann \cite{liu_audioldm_2023}. 

\subsection{AudioLDM2}

\emph{AudioLDM2}\cite{liu_audioldm2_2023} greifte erarbeitete Konzepte aus \emph{AudioLDM}\cite{liu_audioldm_2023} auf und erziehlte durch eine neue Architektur (Abildung \ref{fig:AudioLDM2}) und neuen Konzepten, verbesserte Ergebnisse für die Audiogenerierung mittels neuronalen Netzen. Motivation hinter der Entwicklung, war die Beobachtung, dass im Bereich der Audiosynthese verschiedene Modelle für verschiedene Aufgaben, wie Spracherzeugung, Musikgenerierung und Klangysnthese existieren, jedoch diese für eine speziellen Aufgabe oder Domaine entwickelt wurden und somit zu spezifisch und einschränkend sind, das sie in einem übergeordneten Kontext verwendet werden können. Das vorgestellte Modell soll ermöglichen, all diese spezifischen Problemstellungen in einem Modell zu vereinen. Hierfür wurde eine Repräsentation von Audio namens \emph{language of audio} (\emph{LOA}) entwickelt, welche gut auf verschiedenen Domainen und Modalitäten, wie Text, Audio, und diversen Medien generalisieren kann. In dieser Repräsentation wurde ein \emph{Latente-Diffusion}-Model trainiert, um Audiosynthese zu betreiben. \cite{liu_audioldm2_2023}

\begin{figure}[h]
  \centering
  \includegraphics[width=.8\textwidth]{AudioLDM2.png}
  \caption[AudioLDM2 Architektur]{Architektur von AudioLDM2\cite{liu_audioldm2_2023}}
  \label{fig:AudioLDM2}
\end{figure} 

Die Aufgabe der Synthese eines Audiosignals $x \in \mathbb{R}^{L_s}$, wobei $L_s$ die Dauer des Signals in Sekunden repräsentiert, lässt sich als Funktion $\mathcal{H}: C \mapsto x$ im Hypothesenraum $\mathcal{H}$ beschreiben. In dieser Funktion stellt $C$ die Eingabe dar, auf deren Basis das Signal generiert wird. Diese Eingabe kann sowohl Text als auch diverse Medienformate umfassen. Das Erlernen dieser Funktion stellt aufgrund der signifikanten Unterschiede in den Verteilungen von $C$ und $x$ eine besondere Herausforderung dar. Als Lösungsansatz wurde eine Abstraktion mittels \emph{LOA} vorgeschlagen. Eine Repräsentation eines Audiosignals im \emph{LOA}-Format ist als $Y=\mathcal{A}(x)$ definiert, wobei $\mathcal{A}$ einen Audio-zu-LOA-Encoder darstellt. $\mathcal{A}$ kann entweder durch festgelegte Regeln definiert oder mittels \emph{self-supervised learning} \cite{tan_regeneration_2023} trainiert werden. Um unterschiedlichste Eingabeformate zu akzeptieren und in \emph{LOA} zu konvertieren, wurde der Übersetzer \emph{any modality to LOA translator} $\mathcal{M}$ mit $\hat{Y}=\mathcal{M}(C)$ definiert. Demzufolge kann die Signalgenerierungsfunktion wie folgt ausgedrückt werden. \cite{liu_audioldm2_2023}

\begin{equation}
    \mathcal{H}_0=\mathcal{G} \circ \mathcal{M}: C \mapsto \hat{Y} \mapsto x
\end{equation}

Hierbei extrahiert $\mathcal{G}$ ein Audiosignal aus der \emph{LOA}-Repräsentation. Falls $\mathcal{M}$ durch $\mathcal{A}$ ersetzt wird  (wie in Gleichung \ref{eqn:h1} gezeigt), tritt nur $x$ als Trainingsdaten für $\mathcal{G}$ auf. Dementsprechend kann $\mathcal{G}$ \emph{self-supervised} ohne den Einsatz von Labels trainiert werden, wodurch das Problem einer geringen Anzahl an gelabelten Audiopaaren umgangen wird. \cite{liu_audioldm2_2023}

\begin{equation}
\label{eqn:h1}
    \mathcal{H}_0=\mathcal{G} \circ \mathcal{A}: C \mapsto Y \mapsto x
\end{equation}

Um eine diverse Audio-Repräsentation $Y$ für Sprache, Musik und Soundeffekte zu erstellen, muss diese sowohl semantische als auch akustische Informationen abbilden können. Um diesen Anforderungen gerecht zu werden, wurde für den \emph{Audio zu LOA}-Encoder ein im \emph{self-supervised} Verfahren vortrainiertes Modell \emph{AudioMAE} \cite{huang_masked_2023} verwendet. Das Modell trainiert eine Repräsentation von ungelabelten Audiosignalen mithilfe eines Encoder-Decoder-Ansatzes. Hierbei werden dem Encoder \emph{Mel-Spektrogramme} mit maskierten Bereichen übergeben, welche der Decoder anschließend zu rekonstruieren versucht. Nachdem die Repräsentation $Y$ generiert wurde, kann diese durch den Einsatz von \emph{average-max pooling} \cite{liu_simple_2023} weiter zu $Y_\lambda$ verfeinert werden. \cite{liu_audioldm2_2023}

Das Erlernen von $\mathcal{G}$ wurde in Anlehnung an \emph{AudioLDM}\cite{liu_audioldm_2023} mittels \emph{Latenter Diffusion}\cite{rombach_high-resolution_2022} durchgeführt (siehe \ref{sec:AudioLDM}). Der Diffusionsprozess vollzieht sich in einem durch einen \emph{VAE}\cite{kingma_auto-encoding_2022} erlernten Latentenraum. Dabei wird keine direkte Diffusion auf $Y$ angewendet, da \emph{AudioMAE} sich nicht primär auf die Erhaltung der Qualität während des Rekonstruktionsprozesses fokussiert. Ein \emph{VAE} zeichnet sich durch eine bessere Rekonstruktionskapazität und ein höheres Kompressionsniveau aus. Während \emph{AudioMAE} stärker semantische Informationen betont, fokussiert sich der \emph{VAE} vermehrt auf akustische Informationen. \cite{liu_audioldm2_2023}

Die mittels \emph{VAE} erstellte latente Repräsentation $z$ wird durch einen Encoder mit Downsampling und einen Decoder mit Upsampling erlernt. Ein Spektrogramm $X$ wird in $z$ transformiert und zu $\mathcal{V}: X \mapsto z \mapsto \hat{X}$ rekonstruiert. Aus $\hat{X}$ lässt sich mit einem vortrainierten \emph{HiFiGAN}\cite{kong_hifi-gan_2020} ein Audiosignal generieren. Die Optimierung des \emph{VAE} erfolgt basierend auf den Unterschieden zwischen $X$ und $\hat{X}$.

Der Vorwärtsprozess des \emph{LDM} im Latentenraum mit dem Hyperparameter $\beta_t \in[0,1]$ ist wie folgt definiert:
\begin{equation}
    q\left(z_t \mid z_{t-1}\right)=\sqrt{1-\beta_t} z_{t-1}+\sqrt{\beta_t} \epsilon_t, t \in 2, \ldots, T
\end{equation}

Der Rückwärtsprozess hingegen ist definiert als:
\begin{equation}
q\left(z_t \mid z_0\right)=\sqrt{\alpha_t} z_0+\sqrt{1-\alpha_t} \epsilon_t
\end{equation}

Die zugehörige Verlustfunktion lautet:
\begin{equation}
\operatorname{argmin}_\phi\left[\mathbb{E}_{z_0, Y, t \sim\{1, \ldots, T\}}\left\|\mathcal{G}\left(\sqrt{\alpha_t} z_0+\sqrt{1-\alpha_t} \epsilon_t, t, Y ; \phi\right)-\epsilon_t\right\|\right]
\end{equation}

Für $\mathcal{G}$ kam ein \emph{Transformer-UNet} (\emph{T-UNET}) zum Einsatz. Dieses Netzwerk kombiniert einen Encoder mit Downsampling und einen Decoder mit Upsampling. Nach einem \emph{Konvolutions}-Block werden zusätzlich \emph{Transformer}-Blöcke integriert.

Das Modell $\mathcal{M}$ ist von Bedeutung, da bei der Inferenz $\mathcal{A}$ nicht verfügbar ist. Für die Generierung von $\hat{Y}$ wird ein alternatives Modell $\mathcal{M}_\theta: C \rightarrow \hat{Y}$ benötigt. Hierbei symbolisiert $\theta$ die lernbaren Parameter. Um der Ausgabe des \emph{AudioMAE} bestmöglich zu entsprechen, wurde dieses Problem als Sprachmodellierungsaufgabe konzipiert. Als Modell hierfür dient \emph{Generative Pre-trained Transformer 2} (\emph{GPT-2})\cite{alec_radford_jeff_wu_rewon_child_david_luan_dario_amodei_ilya_sutskever_language_2019}. Dieses wurde in einem \emph{unsupervised} Verfahren auf einem umfangreichen Textdatensatz trainiert und zielt darauf ab, Aufgaben im Bereich des \emph{Natural Language Processing} (\emph{NLP}), etwa Textvervollständigung, Beantwortung von Fragen oder Sprachübersetzung, zu erfüllen. \emph{GPT-2} wurde speziell für die Verwendung in \emph{AudioLDM2} mittels \emph{teacher forcing}\cite{kolen_field_2001} verfeinert. Gegeben ein Datensatz $C$, wird das Modell durch die Maximierung der Likelihood optimiert. $C$ kann diverse Daten repräsentieren, darunter Audio, Text, Phoneme oder visuelle Daten. Ein \emph{Mischung von Experten}-Ansatz (engl. \emph{mixture of experts})\cite{masoudnia_mixture_2014} dient der Extraktion essentieller Merkmale, wodurch ein breites Spektrum an Informationen erschlossen werden kann. Ein \emph{Linearer Adapter} (engl. \emph{linear adaptor}) wurde eingesetzt, um alle Merkmale auf eine einheitliche Dimension $D$ zu normieren. Verschiedene Systeme können zur Merkmalextraktion herangezogen werden.

Für die Nutzung von Text als Kondition $C$ wurden sowohl \emph{CLAP}\cite{wu_large-scale_2023} als auch \emph{FLAN-T5}\cite{chung_scaling_2022} herangezogen. \emph{FLAN-T5} diente zusätzlich als weiterer Encoder, weil \emph{CLAP} Schwierigkeiten zeigte, temporale und semantische Informationen adäquat zu verarbeiten. \emph{CLAP} wurde zudem verwendet, um Umschreibungen für Audiosignale zu generieren, die solche nicht besitzen, wie es bei Text-zu-Sprach-Aufgaben der Fall ist. Ein weiterer implementierter Encoder ist der \emph{Phoneme Encoder}, der dazu dient, Informationen über Phoneme – die kleinsten sprachlichen Einheiten – zu kodieren. Für die Kodierung von Bildern wurde \emph{ImageBind}\cite{girdhar_imagebind_2023} eingesetzt.

Während des Finetunings bestimmte ein probabilistischer Schalter über das Konditionierungssignal. Mit einer Wahrscheinlichkeit von $25\%$ wurden Informationen, die von \emph{AudioMAE} extrahiert wurden, als \emph{Ground Truth} herangezogen. Hingegen wurden in $75\%$ der Fälle die von \emph{GPT} generierten Informationen verwendet.

\section{Implementierung des Neuronalen Synthesizers}
\subsection{Audioprogrammierung}
Die Geschwindigkeit der Datenverarbeitung stellt ein maßgebliches Kriterium bei der Wahl der Programmiersprache für die Entwicklung und Implementierung von digitalen Audioprozessen dar. Für Echtzeitanwendungen gilt insbesondere, dass der Code so effizient wie möglich gestaltet sein sollte, um jegliche Latenz zu minimieren. Unter Berücksichtigung dieser Prämissen fällt die Wahl häufig auf eine Realisierung in \emph{C/C++}. \cite{doumler_c_2015, boulanger_audio_2011}

C++ entstand als Erweiterung der Sprache C und behält dabei C als seine Untermenge. Es stützt sich auf die Grundprinzipien von C, insbesondere die hardwarenahe Programmiermöglichkeit und die Tatsache, dass C auf den meisten Systemen laufen kann. Darüber hinaus bereichert C++ das Spektrum um Aspekte der Datenabstraktion sowie objektorientierte und generische Programmierparadigmen. \cite{stroustrup_c_1997}
\subsection{JUCE Framework}
\glqq\emph{JUCE} ist das am häufigsten verwendete Framework für die Entwicklung von Audioanwendungen und -Plugins. Es handelt sich dabei um eine Open-Source-C++-Codebasis, die zur Erstellung eigenständiger Software auf Windows, macOS, Linux, iOS und Android sowie VST-, VST3-, AU-, AUv3-, AAX- und LV2-Plugins verwendet werden kann.\grqq \cite{noauthor_juce_nodate-1}

Es bietet eine Abstraktion für die Verarbeitung von Audiosamples und MIDI von den nativen Audiogeräten auf jeder Plattform oder einer Host-DAW. Mit der Bibliothek von \emph{digitalen Signalverarbeitungs-(DSP)-Bausteinen}, die JUCE bereitstellt, können unterschiedliche Audioeffekte, Filter, Instrumente und Generatoren schnell prototypisiert und eingesetzt werden. \cite{noauthor_juce_nodate-1} So umfasst wa eine breite Palette von Klassen, die häufig auftretende Probleme bei der Entwicklung von Audioprojekten lösen. Dazu gehören die Behandlung von Grafiken, Sound, Benutzerinteraktion und Netzwerken. \cite{robinson_getting_2013}

In dieser Arbeit wird das JUCE-Framework mittels \emph{CMake}, \glqq eine Open-Source-, plattformübergreifende Werkzeugfamilie, die zur Erstellung, zum Testen und zum Verpacken von Software entwickelt wurde\grqq \cite{noauthor_cmake_nodate} benutzt, um die durch das Diffusionsnetz erstellten Klänge spielbar und manipulierbar zu gestalten. 

Das Juce Module \emph{PluginGuiMagic} \cite{walz_plugin_nodate} ermöglicht die Erstellung einer Benutzeroberfläche für die zu entwickelnde Anwendung zu vereinfachen und zu beschleunigen. 


\subsection{AudioLDM-Anbindung} \label{sec:api}
Um die Wiedergabe eines vom Diffusionsmodell generierten Samples im Synthesizer zu realisieren, ist es notwendig, das Modell in den C++-Quellcode zu integrieren. Hierfür existieren prinzipiell drei mögliche Methoden zur Implementierung von AudioLDM. Eine optimal anmutende Herangehensweise wäre die Konvertierung des trainierten Modells in ein \emph{ONNX}-Modell \cite{noauthor_onnx_nodate-1}, wodurch eine binäre Repräsentation des Modells entstehen würde. Mit Hilfe der \emph{ONNX-Runtime} \cite{noauthor_onnx_nodate} könnte diese dann in C++ eingebettet und geladen werden, um Inferenzoperationen durchzuführen und somit die Generierung von Sound zu ermöglichen. Diese Vorgehensweise würde besonders das Zusammenstellen und die Verbreitung des fertiggestellten Instruments inklusive des trainierten Modells begünstigen. Eine alternative Option wäre die Konvertierung des Modells in ein \emph{Torchscript}-Modell \cite{noauthor_torchscript_nodate} und das anschließende Laden in C++. \cite{oli_larkin_machine_2023} 

Aktuell konnte jedoch weder ein ONNX- noch ein Torchscript-Modell für das AudioLDM-Modell generiert werden, da dieses aus einem komplexen Zusammenspiel verschiedener Modelle und Module besteht und einige der eingesetzten Modelle und Module derzeit noch nicht von ONNX oder Torchscript unterstützt werden. Ausdiesem Grund wurde für die Umsetzung dieses Projekts die Entwicklung einer \emph{API (Application Programming Interface)} gewählt, durch die über definierte Endpunkte eine lokale Instanz des Modells erzeugt und zur Generierung der benötigten Samples verwendet werden kann. Als Framework für diese Implementierung kommt \emph{FastAPI} \cite{noauthor_fastapi_nodate} zum Einsatz. Auf dem Python-Server wird eine Instanz des AudioLDM-Modells ausgeführt. Diese wird als \emph{Pipeline} durch die \emph{Diffusers} Bibliothek von \emph{Huggingface} \cite{noauthor_huggingface_nodate} verwendet. Der Einsatz dieser Pipeline ermöglicht die Durchführung von Inferenzen mit einem gewissen Grad an Abstraktion und eine Anwendung auf verschiedenen Systemen. So war das veröffentlichte Modell ursprünglich nicht auf Computern ohne GPU, wie den M-Modellen von Apple, lauffähig. Durch die Möglichkeit, den zu verwendenden Prozessor innerhalb der Pipeline festzulegen, konnte dieses Problem jedoch behoben werden. Des Weiteren erlaubt die Nutzung einer API, dass bei mehreren Instanzen des Synthesizers nicht für jede ein neues Modell initialisiert werden muss, was das parallele Betreiben mehrerer Modelle vermeidet.

Die implementierten Endpunkte umfassen einen zur Initialisierung eines Modells, der das \emph{Gerät} (auf welchem Prozessor die Berechnung ausgeführt werden soll) und das \emph{Modell} (bestimmt, welches vortrainierte Modell verwendet werden soll) benötigt. Der Endpunkt zur Generierung eines Klangs erfordert einen \emph{Prompt}, \emph{negativen Prompt}, die \emph{Audiolänge}, \emph{Anzahl der Inferenzschritte} sowie eine \emph{Leitskala} und liefert ein in \emph{Base64} kodiertes Audiosignal zurück.

\begin{figure}
  \centering
  \includegraphics[width=.4\textwidth]{graphics/Server.png}
  \caption[Infrastruktur]{Anbindung an den AudioLDM Server}
  \label{fig:server}
\end{figure}

\subsection{Sampler}

Für die Implementierung des Samplers wurde der bereits von \emph{Juce}\cite{noauthor_juce_nodate-1} zur Verfügung gestellte Sampler \cite{noauthor_juce_nodate} verwendet. Zusätzlich wurde die Funktionalität erweitert, um eine spezifische Tonverschiebung vornehmen zu können und den Pitch-Bender an einem Keyboard einsetzen zu können.

\chapter{Ergebnisse}
Das Zusammenspiel der vorgestellten Komponenten und Elemente führte zu einem digitalen Instrument, welches als Standalone oder als Plugin in einem VST3 oder AU-Format in einer DAW wie \emph{Ableton} \cite{noauthor_ableton_nodate}, \emph{Logic} \cite{noauthor_logic_nodate} und anderen, unter Windows und Mac, verwendet werden kann.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{synth.png}
  \caption[Benutzeroberfläche]{Benutzeroberfläche}
  \label{fig:synth}
\end{figure} 

Die daraus entstandene Benutzeroberfläche (Abbildung \ref{fig:synth}) ermöglicht es einem Nutzer, den Server für die API (siehe \ref{sec:api}) auf einem gewünschten Port  zu starten und ein Modell zu initiieren, indem aus einer Auswahl von vorab trainierten Modellen gewählt wird. Die Verwendung des Modells auf verschiedenen Hardwarekonfigurationen (mit und ohne GPU) und Betriebssystemen wird durch die Auswahl an unterstützten Geräten ermöglicht. Nach der Initialisierung eines Modells kann ein gewünschter Klang, der durch die Texteingabefelder \emph{Prompt} und \emph{Negative Prompt} definiert wird, synthetisiert werden. Auch die Länge des zu erzeugenden Klangs in Sekunden (\emph{audio length}), die Anzahl der Inferenzschritte (\emph{number of inference steps}) und die Leitskala (\emph{guidance scale}) lassen sich einstellen. Um die Erzeugung determiniert zu machen, kann auch ein \emph{Seed} in Form eines 8-Byte großen Integers im Wertebereich von $0$ bis $2^{64}-1$ mit angegeben werden.  

Die \emph{Hüllkurve} des \emph{Samplers} (siehe \ref{sec:synth+envelope}) lässt sich über vier Parameter modifizieren. Hierbei sind die Zeitspannen in Sekunden für die Anschwell- (engl. \emph{attack}), Abschwell- (engl. \emph{decay}) und Ausschwingphasen (engl. \emph{release}) sowie das Level, auf das die Lautstärke absinken soll (engl. \emph{sustain}), festlegbar. Der von dem Instrument generierte Klang kann mithilfe des Sliders "Verstärkung" (engl. \emph{gain}) auf eine bestimmte Lautstärke angepasst werden.

Da die Tonhöhe des generierten Klangs nicht im Voraus bestimmt werden kann und der Klang standardmäßig der \emph{Midi}-Note $C4$ zugeordnet wird, besteht die Möglichkeit, das Instrument so zu stimmen, dass die Tonhöhe des Klangs der gespielten Note entspricht. Hierbei kann der Klang kontinuierlich um bis zu zwölf Halbtöne, also einer Oktave, nach oben oder unten angepasst werden.

Das produzierte Signal wird durch eine Wellenform- und Spektrogramm-Anzeige visualisiert. Dies soll dem Nutzer dabei helfen, die Ergebnisse des Modells besser nachzuvollziehen und das erfolgreiche Generieren zu erkennen. Zudem lassen sich aus dieser Visualisierung bestimmte Klangstrukturen herauslesen.

Die Soundqualität des Instrumentes und die Fähigkeit die gewünschten musikalischen Merkmale aus der textuellen Eingabe zu generieren hängt von dem zur Verwendung kommenden Modell, und der Nutzer spezifischen Eingabe der Parameter und des Promptes ab. 

Eine Sammlung des Verhalten der verschiedene Modelle auf speziellen Parameter und Eingaben wird auf \url{https://suckrowpierre.github.io/TtPS.github.io/}\cite{pierre-louis_suckrow_text-zu-spielbarem-klang_nodate}  visualisiert und ermöglicht eine Evaluierung dieser (siehe \ref{chap:prompts}).

Es lässt sich entnehmen, dass bei gleichen Parametern und Prompt für unterschiedliche \emph{Torch}-Geräte unterschiedliche Audiosignale erzeugt werden. Eine großen unterschied in der Qualität zwischen den Geräten lässt sich nicht eindeutig festlegen. Man kann jedoch feststellen, dass wenn eine Prompt auf den Geräten \glqq\emph{mps}\grqq und \glqq\emph{cuda}\grqq eine schlechtes Ergebnis liefert, dass mittels \glqq\emph{cpu}\grqq generierten Audiosignal für den gleichen Prompt, noch mehr ungewünschte Artefakte und unangenehme Geräusche auftreten. //TODO generate for cuda and show example

Aus \cite{noauthor_audioldm_nodate-1, noauthor_audioldm_nodate} geht hervor, welchen erwarteten Einfluss der Wert der \glqq Leitskala\grqq (eng. \glqq guidance-scale\grqq) auf die Signalqualität hat. Obgleich höhere Werte das beschriebene Prompt präziser darstellen, resultieren sie in einer verminderten Audioqualität. Zur Untersuchung dieses Sachverhalts wurden Audiosignale der Prompts (\glqq Gentle guitar strum, Tribal African drum circle, vocal harmonies\grqq) mittels sämtlicher verfügbarer vorab trainierter Modelle und \glqq Leitskala\grqq-Werten von $1,2,3,4,5$ generiert. Es zeigt sich, dass Audiosignale, die mit einem Wert von $1$ erzeugt wurden, die geringste Qualität aufweisen. Werte zwischen $2-4$ scheinen hingegen eine bessere Audioqualität zu gewährleisten. Die wichtigen Audiocharachteristika scheinen sich in diesem Bereich heraus zu kristallisieren und tiefe Frequenzen kommen zur Prägung. Weiterhin bestätigt sich, dass höhere Werte den gegebenen Prompt detaillierter repräsentieren. Ein exemplarischer Fall hierfür ist der Prompt \glqq Gentle guitar strum\grqq, generiert mit \emph{audioldm-m-full}\cite{noauthor_cvsspaudioldm-m-full_nodate}. Bei diesem zeigen Signale mit einem Wert unter $4$ diverse Audioereignisse, die an eine Gitarre erinnern. Erst bei einem Wert von $4$ oder höher ist ein ein einzelnes eindeutiges Gitarrenzupfen zu identifizieren.

Die \emph{Inferenzschritte} (engl. \emph{Inferencesteps}) geben die Anzahl der Entrauschungsschritte im Diffusionsprozess an. Eine höhere Anzahl dieser Schritte soll zu verbesserten Ergebnissen bei gleichzeitig längerer Inferenzzeit führen \cite{noauthor_audioldm_nodate-1, noauthor_audioldm_nodate}. Um die Auswirkungen der \emph{Inferenzschritte} zu analysieren, wurden wieder Audiosignale aus drei unterschiedlichen Prompts (\glqq ambient texture\grqq, \glqq techno kickdrum\grqq, \glqq Long evolving drone\grqq) mithilfe aller vorab trainierten Modelle und für Inferenzschritte-Anzahlen von $5,10,20,50,100,200,400$ generiert. Hierbei zeigten die Werte $5$ und $400$ die ungünstigsten Ergebnisse: Signale mit $5$ Schritten resultierten in dumpfen und wenig strukturierten Klängen. Hingegen schienen Werte im Bereich von $10-200$ adäquate Resultate zu liefern. Es wurden oftmals nur marginale Qualitätsveränderungen pro Schritt beobachtet. Die Ergebnisse weisen zudem darauf hin, dass \emph{AudioLDM2} im Vergleich zu \emph{AudioLDM} eine größere Anzahl an Schritten benötigt, um qualitativ hochwertige Signale zu produzieren. Bei $400$ Schritten hingegen erscheint die Anzahl überdimensioniert, da das resultierende Signal verstärkt abstrakte Klangereignisse und unerwünschte Artefakte aufweist.

Um die Auswirkungen der gewünschten Audiosignallänge auf die Resultate zu untersuchen, wurden für die drei Prompts (\glqq Chirping birds at dawn\grqq, \glqq A choir pad\grqq, \glqq An ambient electronic pad\grqq) mittels der Modelle \emph{audioldm-m-full}\cite{noauthor_cvsspaudioldm-m-full_nodate} und \emph{audioldm2}\cite{noauthor_cvsspaudioldm2_nodate} Signale mit den Längen $5,10,20,30$ generiert. Bei ausgedehnteren Signalen scheint die Qualität leicht abzunehmen. Offenbar haben die Modelle Schwierigkeiten, die Konsistenz über längere Zeiträume hinweg beizubehalten. In einigen dieser Beispiele manifestiert sich ein Phänomen, das als \emph{Pitchdrifting} bezeichnet wird, wodurch die wahrgenommene Tonhöhe variiert. Diese leicht reduzierte Qualität könnte jedoch gezielt als stilistisches Mittel verwendet werden.

Die Einwirkung verschiedener \emph{Negative-Prompts} (engl. \emph{negative prompt}) auf die Prompts (\glqq Soft flute note\grqq, \glqq Choir\grqq, \glqq A muted trumpet\grqq) wurde analysiert. Mittels der Modelle \emph{audioldm-m-full}\cite{noauthor_cvsspaudioldm-m-full_nodate} und \emph{audioldm2-music}\cite{noauthor_cvsspaudioldm2-music_nodate} wurden Signale unter Verwendung der \emph{Negativen Prompts} (\glqq low quality\grqq, \glqq average quality\grqq, \glqq harsh noise\grqq, \glqq dissonant chords\grqq, \glqq distorted sounds\grqq, \glqq clashing frequencies\grqq, \glqq feedback loop\grqq, \glqq clattering\grqq, \glqq inharmonious\grqq, \glqq noise\grqq, \glqq high pitch\grqq, \glqq artefacts\grqq) generiert. Die Beobachtungen zeigen, dass einige \emph{Negative-Prompts} das Ergebnis positiv beeinflussen, während andere das Signal mit zusätzlichem Rauschen beeinträchtigen. Insbesondere \glqq low quality\grqq, \glqq noise\grqq, \glqq harsh noise\grqq und \glqq average quality\grqq schienen das Resultat am meisten zu verbessern. Der \emph{Negative-Prompt} \glqq distorted sounds\grqq zeigte in einigen Fällen positive Effekte, bewirkte jedoch im Zusammenhang mit dem \emph{Prompt} \glqq Soft flute note\grqq und dem Modell \emph{audioldm-m-full} eine Verschlechterung. \glqq high pitch\grqq scheint hohe Obertöne zu unterdrücken und tiefere Töne bekommen mehr Präsenz, was eine bewusste Entscheidung seien sollte. Andere \emph{Negative-Prompts}, wie \glqq dissonant chords\grqq, \glqq distorted sounds\grqq, \glqq clashing frequencies\grqq, \glqq feedback loop\grqq, \glqq clattering\grqq, \glqq inharmonious\grqq und \glqq artefacts\grqq, scheinen das Resultat zu beeinträchtigen. Die anfängliche Annahme, dass \emph{AudioLDM2} aufgrund von \emph{LOLA} effizienter mit \emph{Negative-Prompts} interagieren könnte, wurde durch die ungünstigen Ergebnisse von \emph{audioldm2-music} in den gegebenen Tests nicht bestätigt.


Prompts, die nur ein einzelnes kurzes nicht wiederholendes Audioereigniss umschreiben,wie \glqq A single kickdrum\grqq; 
\glqq A kickdrum\grqq; \glqq A clap\grqq; \glqq A snare\grqq; etc.., erzeugen nicht das erwarte Ergebniss, sonder Signale, in denen Audioereignisse mehrfach vorkommen. Die Modelle versuchen nicht das gewünschte einzelne Ereigniss zu synthetisieren, sondern versuchen das gwünschte Ergebniss in einem musikalischen Rhytmus zu erzeugen 

\chapter{Diskussion}

Das Aufzeigen, der Möglichkeit Instrumente basierend auf künstlicher Intelligenz zu entwickeln stellt eine beachtliche Erweiterung der musikalischen Klangerzeugung dar. Die Untersuchung des digitalen Instruments und die Evaluation der anwendbaren Modelle legen nahe, dass die Generierung, Manipulation und Reproduktion von durch Künstliche Intelligenz synthetisierten Klängen ein erhebliches kreatives Potential besitzen. Dennoch zeigen sich bestimmte limitierende Schwachstellen. Insbesondere die identifizierten Mängel, das gelegentlich unvorhersehbare Verhalten der Modelle und die eingeschränkte Umsetzung des Samplers gehören dazu.

Es wird deutlich, dass die untersuchten Modelle bisher nicht adäquat auf Eingabeaufforderungen im Kontext der Musikproduktion reagieren. Die Unfähigkeit, einzelne Klangereignisse zu generieren, das Versäumnis, erwartete Klangmerkmale von spezifischen Instrumenten zu modellieren, sowie einige verrauschte Ergebnisse, deuten auf vorhandene Defizite hin. Diese könnten möglicherweise durch das Training mit mehr Audiodaten, die in der Musikproduktion verwendet werden, behoben werden. Trainingsdaten hierfür liegen bei Synthesizer- oder Samplermanufaktoren, Audiobibliothken wie Splice etc.., Musikstudios oder Produzenten selber. Privatpersonen, welche Musikproduktion betreiben besitzen meist selber eine Bibliothek an Klängen, welche Verwendung in deren Kreationen findet. Basierend auf diesen könnte ein gewünschtes Modell weiter gefinedtuned werden um dieses Individueller und persönlicher zu gestalten. Ein weiterer Idee, wäre diese verschiedenen Biblitoheken von verschiedenen Privat Personen zu einer großen nutzbaren Datenmenge zusammenzuführen, welche anschließend für Finetuning oder Training genutzt werden. Dieser Community-Driven Ansatz wäre ein demokratisierter Weg den Mangeln an Datensätze in diesem Bereich zu überwinden. 

Das Scheitern des Erstellen eines ONNX, bzw einer TorchScript Repräsentation des Modells um Inferenz in C++ zu betreiben, bringt die Schwierigkeit mit sich das Modell samt nötiger Verknüpfung mit der Python API zu bündeln und verteilen. Eine gesamte Implementation in einer Sprache wäre deutlich wünschenswerter und einfacherer. Ein Server und eine Api wären somit nicht mehr von Nöten und 

Die rudimentäre Implementierung des Samplers verhindert aktuell die Festlegung von Start- und Endpunkten für die Wiedergabe des generierten Signals. Solche Parameter könnten eine intuitive Methode darstellen, um aus mehreren oder wiederholten Klangereignissen das gewünschte auszuwählen. Auch fehlt die Option, bestimmte Klangbereiche zu loopen, um einen kontinuierlichen Klang zu erzeugen, ein Feature, das in industriell genutzten Samplern seit den 1990er Jahren Standard ist. In diesem Kontext erhebt sich die Frage, ob die Entwicklung eines eigenen Samplers tatsächlich erforderlich ist. Alternativ könnte eine Benutzeroberfläche ausreichen, die sich in bestehende Musikproduktionssoftware integrieren lässt, um Klänge zu generieren und mittels der Werkzeuge des jeweiligen Software-Ökosystems zu manipulieren. Ein solches Instrument könnte Anwendungen wie Splice potenziell obsolet machen, vorausgesetzt, das Modell ist ausreichend leistungsfähig. Die in AudioLDM vorgestellten Generierungsoptionen, wie Inpainting oder Style-Transfer, könnten in einem derartigen Tool integriert werden, sodass es beispielsweise einem Nutzer ermöglicht wird, bestimmte unerwünschte Abschnitte eines Spektrogramms zu entfernen oder Audiodateien zu importieren und mittels Eingabeaufforderungen zu modifizieren.

Falls jedoch die Entwicklung eines eigens entwickelten Samplers weiterverfolgt wird könnten diesem auch Aspekte des maschinellen Lernens zugeführt werden. Wenn auf einem Klavier unterschiedliche Noten gespielt werden habe diese unterschiedliche Klangfarben und somit unterschiedlichen Spektren \cite{parker_good_2009}. Es reicht nicht aus den Klang in die gewünschte Frequenz zu bringen. Um ein realisitischeres Ergebniss zu erziehlen müssen die kleinen Unterschiede der Oberschwingungen für jede Tonhöhe modulliert werden. Genau dies könnte mit einem ML-Modell erlernt werden. Als Eingabe würde ein Audiosignal dienen, aus dem das Modell jeweils eine Signal für alle möglichen Tonhöhen erzeugt. Um dieses zu trainieren wäre eine große Datenmenge an Audiosignalen von nöten, die alle Tonhöhen von verschiedenen Instrumenten umfassen. 

Weitere Mängel des Instrumentes welche adressiert und verbessert werden könnten, wären zum Beispiel das manuelle Stimmen des Instruments, welches sich automatisierten ließ. Eine weitere große Schwäche, ist das bisher generierte Klänge und Einstellungen sich nicht wie üblich bei digitalen Instrumenten speichern und abrufen lassen. 

Dem Prinzip der Additivensynthese, folgend, könnten dem Instrument mehr als eine Soundquelle beigeführt werden, und diese mit Mixern und Filtern zusammen führen. 


TODO Ref Roland sytnh with multiple sound sources  

Schwächen:

AudioLDM

Zu wenig Training über Musikproduktions Keywords

Nervige Obertöne

Keine zuordnung zu Genres, Künstler, Epochen

Keine Automatische Pitch Erkennung

16.000 khz 

--------------

Stärken:

Neue Soundsynthese

Ambiend Pads

Sound Experimentierung

Happy Little Accidents


--------------

Verbesserung:

Automatische Pitch Zuordnung

Mehr Zuordnung zu Emotionen 

AudioLDM 2

Schneller Inferenz

Spezielle Hardware 

ML-Sampler

Spektogramm Manipulation 

Loop Points

Modulare Aufbau 

Finetuning auf mehr musikalischen Daten

Indivduelles Finetuning 

Eine Programmiersprache

ONNX

Binäredaten

Envelope Visualiesieren

Additive Synth

Convelution Mixer

Presets speichern 

Schon Präsente Spuren Analysieren und Lücken finden 

ML-Effekte

Mehre Modell Auswahl 

Modell zusammenbauen ? 





\chapter{Schlussfolgerung}
\label{sec:conclusion}


\printbibliography

All links were last followed on \today{}.

\appendix

\chapter{Prompts}
\label{chap:prompts}

Die auf \url{https://suckrowpierre.github.io/TtPS.github.io/}\cite{pierre-louis_suckrow_text-zu-spielbarem-klang_nodate} vorgestellten Ergebnisse wurden folgender maßen erstellt.

Mittels den folgenden Prompts wurden für jedes der Modelle \emph{audioldm-m-full, audioldm-l-full, audioldm2-large, audioldm2-music} \cite{liu_audioldm_2023,liu_audioldm2_2023} jeweils ein Audiosignal mittels dem Quellcode \ref{lst:generator}\cite{pierre-louis_suckrow_thesismodelsresultsgenerator_2023} generiert. 

\begin{multicols}{4}
\fontsize{8pt}{11pt}\selectfont
\begin{itemize}
    \item A single kickdrum
    \item A kickdrum
    \item A distored kickdrum
    \item A kickdrum with a lot of reverb
    \item Deep house kick
    \item Techno kick
    \item A clap
    \item A weird clap
    \item A snare
    \item A snare with reverb tail
    \item A string orchestra
    \item A analog synth
    \item A distorted analog synth
    \item A string synth
    \item dreamy nostalgic strings
    \item Ambient pads
    \item A guitar string
    \item A distorted guitar string
    \item A tight snare
    \item 808 bass
    \item A sub-bass
    \item Jazz ride cymbal
    \item A realistic hi-hat pattern
    \item A blues harmonica
    \item A smooth saxophone
    \item A jazz trumpet solo
    \item Electric piano chords
    \item Grand piano arpeggios
    \item Harpsichord playing Baroque
    \item A sitar drone
    \item A tabla rhythm
    \item Fast-paced conga drums
    \item A marimba melody
    \item An upright bass groove
    \item Lo-fi beats
    \item A vocoder voice
    \item A children's choir
    \item An operatic soprano
    \item A flute ensemble
    \item An oboe solo
    \item Bassoon in a chamber setting
    \item Steel drums playing Calypso
    \item A bagpipe melody
    \item A didgeridoo drone
    \item Djembe in a world music context
    \item A triangle ting
    \item Cowbell in a funk setting
    \item A gong hit
    \item Wind chimes in C major
    \item A celesta playing a lullaby
    \item A Hammond organ in gospel style
    \item A Moog synthesizer lead
    \item Square wave arpeggiator
    \item FM synthesis bells
    \item Noise sweep
    \item White noise with a filter sweep
    \item Vinyl crackle
    \item Tape hiss
    \item Reverse cymbal
    \item Side-chained pad
    \item Glitched vocal sample
    \item Auto-tuned vocals
    \item A theremin playing a melody
    \item Beatboxing sequence
    \item Finger snaps in a jazz setting
    \item A full orchestral crescendo
    \item A muted trumpet
    \item A slide guitar in a blues context
    \item Clawhammer banjo
    \item Double-bass pizzicato
    \item A haunting choir pad
    \item Ambient rainforest sounds
    \item Foley footsteps
    \item Synth brass in an '80s style
    \item Harp glissando
    \item Ticking clock sample
    \item Ambient ocean waves
    \item Jazz brush kit groove
    \item A detuned square wave synth
    \item An aggressive dubstep wobble bass
    \item Shrill piccolo solo
    \item Melodic pan flute
    \item Sustained pipe organ chord
    \item Choir singing in Latin
    \item A fingerstyle ukulele
    \item Flamenco guitar playing fast runs
    \item Hard rock power chords
    \item Acoustic guitar with a capo on 5th fret
    \item A sultry bossa nova rhythm
    \item Swing band with a prominent double bass
    \item Spooky theremin melody
    \item A pulsing trance bassline
    \item Crunchy breakbeat loop
    \item Reggae offbeat chords
    \item A barbershop quartet
    \item Bowed double bass drone
    \item Glissando on a grand harp
    \item Layered vocal harmonies
    \item A slow, haunting Gregorian chant
    \item A distorted metal guitar solo
    \item A mellow Rhodes piano groove
    \item A detuned radio frequency scan
    \item Spacey sci-fi effects
    \item A mandolin playing bluegrass
    \item Chorus of kazoos playing a melody
    \item Melodica in a reggae context
    \item Pop punk palm-muted guitar
    \item Bright and shimmering glockenspiel
    \item High-pitched bat squeaks
    \item Traditional Japanese koto melody
    \item Tribal African drum circle
    \item A drone from a tanpura
    \item Cajun accordion riff
    \item Complex polyrhythmic hand percussion
    \item A twangy banjo in a bluegrass setting
    \item Psychedelic sitar in a raga style
    \item A punchy techno kick
    \item Rich choir aahs and ooohs
    \item Massive cinematic drum hits
    \item A fluid slap bass line
    \item Balkan brass band upbeat tune
    \item Tense suspense string section
    \item Shruti box drone in C\#
    \item Chirping birds at dawn
    \item Tuvan throat singing melody
    \item A haunting reverb-drenched ebow guitar
    \item Energetic punk rock drum fill
    \item Middle Eastern oud improvisation
    \item Polka accordion and tuba duet
    \item Smooth and soulful Motown bassline
    \item An eerie wind howling effect
    \item Traditional Chinese guzheng melody
    \item A slapstick comedy woodblock
    \item Bouncy reggaeton beat with dembow rhythm
    \item Echoing canyon yodel
\end{itemize}
\end{multicols}

Die Liste an musikalisch und instrumentalen Prompts wurden unteranderem mittels \emph{GPT-4}\cite{openai_gpt-4_2023} auf \url{https://chat.openai.com/} mittels folgendem Prompt am 8. September 2023 generiert. 

\lstdefinestyle{gpt}{
    basicstyle=\ttfamily\small,
    breaklines=true
}

\begin{lstlisting}[style=gpt]
For a scientific work of mine, I'm evaluating the music abilities of diffusion models that can generate audio. For that, I have a script that generates audio for different models based on a list of prompts. My list looks like the following. Please add more prompts to test the musical and instrumental abilities of the models. Keep in mind the people using the models would be musicians, studio engineers, and music producers. 


A single kickdrum
A kickdrum
A distored kickdrum
A kickdrum with a lot of reverb
A clap
A weird clap
A snare
A string orchestra
A analog synth
A distorted analog synth
A string synth
dreamy nostalgic strings
Ambient pads
A guitar string
A distorted guitar string
\end{lstlisting}

\begin{Listing}
\lstinputlisting[language=Python, frame=single,basicstyle=\scriptsize]{code/generator.py}
 \caption{Python code zur Generierung der Ergebnisse}
  \label{lst:generator}
\end{Listing}
%\input{latexhints/latexhints-english}
\pagestyle{empty}
\renewcommand*{\chapterpagestyle}{empty}
\Affirmation
\end{document}
