% !TeX spellcheck = en-US
% !TeX encoding = utf8
% !TeX program = pdflatex
% !BIB program = biber
% -*- coding:utf-8 mod:LaTeX -*-


% vv  scroll down to line 200 for content  vv


\let\ifdeutsch\iftrue
\let\ifenglish\iffalse


\input{pre-documentclass}
\documentclass[
  %
  %ngerman, %%% Add if you write in German.
  %
  % fontsize=11pt is the standard
  a4paper,  % Standard format - only KOMAScript uses paper=a4 - https://tex.stackexchange.com/a/61044/9075
  twoside,  % we are optimizing for both screen and two-side printing. So the page numbers will jump, but the content is configured to stay in the middle (by using the geometry package)
  bibliography=totoc,
  %               idxtotoc,   %Index ins Inhaltsverzeichnis
  %               liststotoc, %List of X ins Inhaltsverzeichnis, mit liststotocnumbered werden die Abbildungsverzeichnisse nummeriert
  headsepline,
  cleardoublepage=empty,
  parskip=half,
  %               draft    % um zu sehen, wo noch nachgebessert werden muss - wichtig, da Bindungskorrektur mit drin
  draft=false
]{scrbook}
\input{config}

\usepackage{multicol}

\usepackage[
  title={Text-zu-spielbarem-Klang: Synthesizer basierend auf Latent-Diffusion-Technologie}, % Do not forget to capitalize your title correctly, you may use the following page to help you: https://capitalizemytitle.com/
  author={Pierre-Louis Wolgang Léon Suckrow},
  orcid=0000-0000-0000-0000, % get your own ORCID via https://orcid.org/
  email={suckrowpierre@gmail.com},
  type=bachelor,
  institute={Institut für Informatik}, % or other institute names - or just a plain string using {Demo\\Demo...}
  course={Informatik},
  examiner={Prof.\ Dr.\ Sylvia Rothe},
  supervisor={Christoph Weber},
  startdate={Mai 31, 2023},
  enddate={Oktober 19, 2023},
  % Falls keine Lizenz gewünscht wird bitte auf "none" setzen
  % Die Lizenz erlaubt es zu nichtkommerziellen Zwecken die Arbeit zu
  % vervielfältigen und Kopien zu machen. Dabei muss aber immer der Autor
  % angegeben werden. Eine kommerzielle Verwertung ist für den Autor
  % weiter möglich.
  copyright=ccbysa, % ccbysa, ccbynosa, cc0, none
  language=german
]{lmu-thesis-cover}

\input{acronyms}

\makeindex

\begin{document}

%tex4ht-Konvertierung verschönern
\iftex4ht
  % tell tex4ht to create picures also for formulas starting with '$'
  % WARNING: a tex4ht run now takes forever!
  \Configure{$}{\PicMath}{\EndPicMath}{}
  %$ % <- syntax highlighting fix for emacs
  \Css{body {text-align:justify;}}

  %conversion of .pdf to .png
  \Configure{graphics*}
  {pdf}
  {\Needs{"convert \csname Gin@base\endcsname.pdf
      \csname Gin@base\endcsname.png"}%
    \Picture[pict]{\csname Gin@base\endcsname.png}%
  }
\fi

%\VerbatimFootnotes %verbatim text in Fußnoten erlauben. Geht normalerweise nicht.

\input{commands}
\pagenumbering{arabic}
\Coverpage
\Copyright
%Eigener Seitenstil fuer die Kurzfassung und das Inhaltsverzeichnis
\deftriplepagestyle{preamble}{}{}{}{}{}{\pagemark}
%Doku zu deftriplepagestyle: scrguide.pdf
\pagestyle{preamble}
\renewcommand*{\chapterpagestyle}{preamble}



%Kurzfassung / abstract
%auch im Stil vom Inhaltsverzeichnis
\section*{Kurzfassung}

\todo{Short summary of the thesis. Here, the following questions should be answered:}
\todo{What is the specific problem addressed?}
\todo{What have you done?}
\todo{What did you find out?}
\todo{What are the implications on a larger scale?}
\todo{Should be around 0.5 pages. Not longer than 1 page.}

\cleardoublepage

\section*{Abstract}

\todo{Short summary of the thesis. Here, the following questions should be answered:}
\todo{What is the specific problem addressed?}
\todo{What have you done?}
\todo{What did you find out?}
\todo{What are the implications on a larger scale?}
\todo{Should be around 0.5 pages. Not longer than 1 page.}

\cleardoublepage


% BEGIN: Verzeichnisse

\iftex4ht
\else
  \microtypesetup{protrusion=false}
\fi

%%%
% Literaturverzeichnis ins TOC mit aufnehmen, aber nur wenn nichts anderes mehr hilft!
% \addcontentsline{toc}{chapter}{Literaturverzeichnis}
%
% oder zB
%\addcontentsline{toc}{section}{Abkürzungsverzeichnis}
%
%%%

%Produce table of contents
%
%In case you have trouble with headings reaching into the page numbers, enable the following three lines.
%Hint by http://golatex.de/inhaltsverzeichnis-schreibt-ueber-rand-t3106.html
%
%\makeatletter
%\renewcommand{\@pnumwidth}{2em}
%\makeatother
%
\tableofcontents

% Bei einem ungünstigen Seitenumbruch im Inhaltsverzeichnis, kann dieser mit
% \addtocontents{toc}{\protect\newpage}
% an der passenden Stelle im Fließtext erzwungen werden.

\listoffigures
\listoftables

% Control List of Listings
\let\iflistings\iffalse
%Wird nur bei Verwendung von der lstlisting-Umgebung mit dem "caption"-Parameter benoetigt
%\lstlistoflistings
%ansonsten:
\iflistings
  \ifdeutsch
    \listof{Listing}{Verzeichnis der Listings}
  \else
    \listof{Listing}{List of Listings}
  \fi
\fi

% Control List of Algorithms
\let\ifalgorithms\iffalse
\ifalgorithms
  %mittels \newfloat wurde die Algorithmus-Gleitumgebung definiert.
  %Mit folgendem Befehl werden alle floats dieses Typs ausgegeben
  \ifdeutsch
    \listof{Algorithmus}{Verzeichnis der Algorithmen}
  \else
    \listof{Algorithmus}{List of Algorithms}
  \fi
  %\listofalgorithms %Ist nur für Algorithmen, die mittels \begin{algorithm} umschlossen werden, nötig
\fi

% Control Glossary
\let\ifglossary\iffalse
\ifglossary
  \printnoidxglossaries
\fi

\iftex4ht
\else
  %Optischen Randausgleich und Grauwertkorrektur wieder aktivieren
  \microtypesetup{protrusion=true}
\fi

% END: Verzeichnisse


% Headline and footline
\renewcommand*{\chapterpagestyle}{scrplain}
\pagestyle{scrheadings}
\pagestyle{scrheadings}
\ihead[]{}
\chead[]{}
\ohead[]{\headmark}
\cfoot[]{}
\ofoot[\usekomafont{pagenumber}\thepage]{\usekomafont{pagenumber}\thepage}
\ifoot[]{}


%% vv  scroll down for content  vv %%































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Main content starts here
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Einleitung}
\label{sec:introduction}

\glqq Es gibt keine theoretischen Grenzen für den Computer als Quelle für musikalische Klänge, im Gegensatz zu herkömmlichen Instrumenten\grqq \footnote{"There are no theoretical limitations to the performance of the computer as a source of musical sounds, in contrast to the performance of ordinary instruments"} \cite{mathews_digital_1963}. Mit diesen wegweisenden Worten publizierte Max Vernon Mathews im Jahr 1963 seinen Artikel \glqq The Digital Computer as a Musical Instrument\grqq \, und legte durch seine Arbeit an den Bell Laboratories den Grundstein für die heutige Musikproduktion und Klangsynthese am Computer \cite{mathews_music_2004}.

In Anlehnung an die Überlegungen von M. V. Mathews konzentriert sich die vorliegende Arbeit auf die Erforschung von Implementierungsmöglichkeiten eines Synthesizers unter Verwendung der neuartigen \emph{Latenten Diffusionstechnologie}. Es werden sowohl das resultierende digitale Instrument als auch die potenziellen Vorteile und Limitationen dieser Implementierung betrachtet. Zudem wird das Potenzial der \emph{Diffusions}-Modelle im Kontext musikalischer Anwendungen diskutiert. Die vorgestellte Realisierung beabsichtigt, Nutzern Klangbilder gemäß individueller Präferenzen zu generieren und diese folglich zu modifizieren und spielbar zu gestalten. Das zentrale Ziel dieser Studie besteht darin, die verschiedenen Implementierungsansätze sowie die Ergebnisse des digitalen Instruments und der eingesetzten \emph{Diffusions}-Modelle systematisch zu analysieren, ihre jeweiligen Stärken und Schwächen herauszuarbeiten und die Tauglichkeit als innovatives Musikinstrument zu bewerten. Ein weiteres Ziel ist es, das Instrument so zu gestalten, dass es nahtlos in den zeitgenössischen Musikproduktionsprozess integriert werden kann, wodurch Musikern der Zugang zu neuen Klangwelten und Syntheseverfahren ermöglicht wird. Die Erzeugung eigens definierter Klänge könnte darüber hinaus rechtliche Problematiken, die durch den Einsatz von Samples auftreten, obsolet machen. Im weiteren Verlauf dieser Arbeit wird die Machbarkeit eines solchen Instruments unter Berücksichtigung von zwei spezifischen Diffusionsmodellen für die Audiosynthese erörtert.

Vereinzelte Projekte, darunter der von Google im Jahr 2017 entwickelte \emph{NSynth} \cite{google_ai_nsynth_2017} oder das Audio-Plugin \emph{Neutone} \cite{qosmo_neutone_nodate}, haben bereits die Konzeption von Instrumenten mithilfe von künstlicher Intelligenz in Angriff genommen. Diese Ansätze erlernen auditive Charakteristika und wenden diese auf ein Signal an bzw. übertragen diese. Die jüngsten Fortschritte in der Entwicklung generativer Modelle haben nun das Potenzial eröffnet, vollkommen neue Klangbilder zu erschaffen. Durch die Möglichkeit, das Ergebnis dieser Modelle präzise durch eine Texteingabe zu bestimmen, erhalten Musiker, Musikproduzenten und Sounddesigner eine neuartige Kontrollmethode zur Definition gewünschter Klangattribute. Ein ähnlicher Ansatz, mit Verwendung desselben Modells wie in dieser Arbeit findet Anwendung in \emph{VroomAI} \cite{barney_hill_vroomai_2023}.

In der vorliegenden Arbeit werden die Text-zu-Audio-Diffusionsmodelle \emph{Audio Latent Diffusion Model} \emph{AudioLDM}\cite{liu_audioldm_2023} und  \emph{Audio Latent Diffusion Model 2} \emph{AudioLDM2}\cite{liu_audioldm2_2023} verwendet, um das Potenzial von künstlicher Intelligenz im Bereich der Audiosynthese in einem musikalischen Kontext aufzuzeigen. Der Fokus liegt dabei auf der Untersuchung dieser neuen Methode zur Klangerzeugung und der Bewertung ihres performativen Potenzials. Insbesondere die Möglichkeit einer interaktiven Bereitstellung von Einstellungsmöglichkeiten für Parameter des jeweiligen Modells und die Klangwiedergabe, welche in vergleichbaren Projekten bisher fehlte, soll dem Instrumentalist zusätzliche Kontrolle über die gewünschten Eigenschaften verleihen.

% Fourth paragraph
% CORE MESSAGE OF THIS PARAGRAPH:
\todo{P4.1. What did you find out? What are the concrete results?}
\todo{P4.2. What are the implications? What does this mean for the bigger picture?}

\chapter{Related Work}


Die Entwicklung eines Instruments, das die Soundsynthese durch künstliche Intelligenz ermöglicht, wurde im Jahr 2023 im Rahmen des von \emph{Juce} \cite{noauthor_juce_nodate} organisierten Wettbewerbs „Neural Audio Plugin“ verfolgt. Insbesondere zeichnete sich das Projekt \emph{VroomAi} \cite{barney_hill_vroomai_2023} durch die Nutzung desselben C++ Audio Frameworks wie im vorliegenden Projekt aus. In \emph{VroomAi} wurde ein digitales Instrument entwickelt, welches über eine Benutzeroberfläche die Eingabe eines benutzerspezifischen Prompts zur Klangerzeugung ermöglicht. Anschließend kann dieser Klang durch den Einsatz eines Samplers spielbar gemacht werden.

\emph{VroomAi} stellt zwei Modelle zur Verfügung: Das Modell \emph{AudioLDM} \cite{liu_audioldm_2023}, welches in dieser Arbeit implementiert ebenfalls benutzt wird, und das Modell \emph{AudioLM}\cite{borsos_audiolm_2022}. Das Modell \emph{AudioLDM2} wurde bis zum Veröffentlichungsdatum dieser Arbeit noch nicht publiziert und findet daher keine Anwendung. Die in \emph{VroomAi} implementierte Architektur ähnelt jener der in dieser Arbeit vorgestellten Umsetzung. Ihr Hauptzweck besteht darin, die Modelle auszuführen und Klänge zu generieren. Hierfür wird auf einen \emph{Gradio} Server \cite{team_gradio_gradio_nodate} zurückgegriffen, der das Modell lokal ausführt.

Das lokale Ausführen des Modells weist bestimmte Schwachstellen auf (siehe Kapitel \ref{sec:api}). Insbesondere bei der Nutzung des von den \emph{AudioLDM} Entwicklern bereitgestellten \emph{Gradio}-Servers können Komplikationen entstehen. Das \emph{AudioLDM} Modell lässt sich, ohne zusätzliche Anpassungen, nicht auf Computern ohne GPU betreiben. Dies kann für Anwender ohne Vorkenntnisse im Bereich ML-Modelle oder bei nicht geeigneter Hardware zu Herausforderungen führen. Ziel dieser Arbeit ist es, solche Hürden so weit wie möglich zu reduzieren und ein intuitiv zu bedienendes digitales Instrument zu entwickeln. Zusätzlich soll dem Anwender die Möglichkeit gegeben werden, modellspezifische sowie Audio-Parameter einzugeben – eine Funktion, die in \emph{VroomAi} bisher nicht vorhanden ist.

Neben \emph{AudioLDM} \cite{liu_audioldm_2023} bieten sich folgende \emph{Machine-Learning} Modelle an, um Klangerzeugung zu betreiben.

\emph{AudioLM} \cite{borsos_audiolm_2022} ermöglicht Audio-Synthese durch eine Kombination von \emph{adversarial neural networks} \cite{goodfellow_generative_2014}, \emph{self-supervised representation learning} \cite{chung_w2v-bert_2021} und \emph{language modeling} \cite{roberts_scaling_2022} besteht. Es wurde auf der \emph{tokenisierten} Darstellung von Wellenformen trainiert. Obwohl die Hauptfunktion von \emph{AudioLM} in der Stimmgenerierung liegt, zeigte es, durch Training auf Klavieraufnahmen, ebenfalls Potenzial in der musikalischen Synthese.  Das Modell besteht aus drei Kernelementen: einem \emph{Tokenizer-Modell}, dem Decoder eines \emph{Transformers-Sprachmodells} und einem \emph{Detokenizer-Modell}. Der Tokenizer wandelt kontinuierliche Audiowellenformen in eine diskrete Repräsentation um. Dabei verwendet es ein hybrides Tokenisierungsverfahren (siehe Abbildung \ref{fig:tokenizerAudioLM}), das sowohl akustische als auch semantische Charakteristika (wie Sprachinhalte für Sprache oder Melodie und Rhythmus für Musik) der Wellenform erfasst. Während \emph{SoundStream} \cite{zeghidour_soundstream_2021} die akustischen Token generiert, stammen die semantischen Token von den Repräsentationen einer Zwischenschicht des \emph{w2v-BERT} \cite{chung_w2v-bert_2021}. Der Decoder eines \emph{Transformers-Sprachmodells}\cite{vaswani_attention_2017} lernt, Tokens basierend auf ihrem vorherigen Kontext zu antizipieren, indem die Abhängigkeiten und Struktur innerhalb der benutzen Sprache erfasst werden. Während der Inferenz prognostiziert das Modell aus einem gegebenen \emph{Prompt} autoregressiv die Token-Sequenz. Dies impliziert, dass Tokens nacheinander generiert werden, wobei jeweils die zuvor erzeugten Tokens als Kontext dienen. Ein hierarchisches Vorgehen wird dabei angewendet (siehe Abbildung \ref{fig:hierarAudioLM}): Zuerst werden semantische Token für die komplette Sequenz modelliert, welche dann als Konditionen zur Vorhersage der akustischen Token dienen. Abschließend konvertiert das Detokenizer-Modell die Token-Sequenz zurück in ein Audiosignal. \cite{borsos_audiolm_2022}

\begin{figure}[h]
\centering
\begin{subfigure}{1.0\textwidth}
  \centering
  \includegraphics[width=0.5\textwidth]{tokenizerAudioLM.png}
  \caption[AudioLM Tokenisierungsverfahren]{Benutzte Tokenizer}
  \label{fig:tokenizerAudioLM}
\end{subfigure}

\vspace{1em} % Optional: Add some vertical space between the subfigures

\begin{subfigure}{1.0\textwidth}
  \centering
  \includegraphics[width=0.7\textwidth]{hierarAudioLM.png}
  \caption[AudioLM Hierarchische Modellierung der Tokens]{Hierarchische Modellierung der Tokens}
  \label{fig:hierarAudioLM}
\end{subfigure}
\caption[AudioLM Architektur]{AudioLM Architektur \cite{borsos_audiolm_2022}}
\label{fig:test}
\end{figure}

Das Modell \emph{Diffsound} \cite{yang_diffsound_2023} wurde mit der Absicht entwickelt, Soundeffekte zu generieren. Die Erzeugung eines Audiosignals erfolgt anhand eines \emph{Prompts}, unter Zuhilfenahme eines \emph{Text Encoders}, eines \emph{Vector Quantized Variational Autoencoders (VQ-VAE)}, eines \emph{Token Decoders} und eines \emph{Vocoders} (siehe Abbildung \ref{fig:DiffsoundArchitecture}). Der \emph{Text Encoder} extrahiert relevante Audioinformationen aus dem eingegebenen Text und eliminiert dabei unwesentliche Daten. Dabei kommen ein vorab trainiertes \emph{BERT} \cite{devlin_bert_2019} und der \emph{Text Encoder} eines bereits trainierten \emph{Contrastive Language-Image Pre-Training (CLIP)} \cite{radford_learning_2021} zum Einsatz. Aus diesen Token wird ein \emph{Mel-Spektrogramm} generiert, welches das künftige Audiosignal repräsentiert. Da Mel-Spektrogramme in eine Token-Sequenz überführt werden können, erstellt der \emph{Token Decoder} diese basierend auf den vom \emph{Text Encoder} generierten Token. Die Qualität des resultierenden Audiosignals hängt maßgeblich vom \emph{Token Decoder} ab. Frühere Arbeiten \cite{liu_conditional_2021, iashin_taming_2021} setzten auf einen \emph{autoregressiven} Ansatz, welcher aufeinanderfolgende Token basierend auf ihren Vorgängern vorhersagte. Ein solcher Ansatz kann allerdings zu kumulierten Fehlern führen, die sowohl die Leistungszeit als auch die Qualität beeinträchtigen. Zur Überwindung dieser Schwäche präsentiert das Paper einen \emph{nicht autoregressiven Token Decoder}, der auf \emph{Diskreter Diffusion} \cite{sohl-dickstein_deep_2015, austin_structured_2023} basiert. Anstatt die Mel-Spektrogramm-Token sequenziell vorherzusagen, prognostiziert \emph{Diffsound} alle Token simultan und optimiert sie iterativ. Der \emph{VQ-VAE} \cite{oord_neural_2018} lernt mithilfe eines \emph{Discriminators} das Vokabular der Spektrogramm-Token, um ein Spektrogramm zu generieren (siehe Abbildung \ref{fig:VQVAE}). In einem abschließenden Schritt konvertiert der \emph{Vocoder} das erstellte Spektrogramm in ein Audiosignal. Hierzu wurde der \emph{Vocoder MelGAN} \cite{kumar_melgan_2019} mit dem \emph{AudioSet} \cite{gemmeke_audio_2017} Datensatz trainiert. Zusätzlich kam der Datensatz \emph{AudioCaps} \cite{kim_audiocaps_2019} sowohl für Training als auch Validierung zum Einsatz. Trotz der Fortschritte besitzt diese Arbeit Limitierungen: So ist das Generierungssystem nicht vollständig End-to-End und sowohl der VQ-VAE, der Token-Decoder als auch der Vocoder werden separat trainiert. \cite{yang_diffsound_2023}

\begin{figure}[h]
\centering
\begin{subfigure}{1.0\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{Diffsound1.png}
  \caption[Diffsound Architektur]{Diffsound Architektur}
  \label{fig:DiffsoundArchitecture}
\end{subfigure}

\vspace{1em} % Optional: Add some vertical space between the subfigures

\begin{subfigure}{1.0\textwidth}
  \centering
  \includegraphics[width=0.7\textwidth]{Diffsound2.png}
  \caption[Diffsound VQ-VAE]{Diffsound Vector Quantized Variational Autoencoder}
  \label{fig:VQVAE}
\end{subfigure}
\caption[Diffsound Architektur]{Diffsound Architektur \cite{yang_diffsound_2023}}
\label{fig:test}
\end{figure}

Ebenfalls einen \emph{autoregressiven} Ansatz verfolgend, verspricht \emph{AUDIOGEN}\cite{kreuk_audiogen_2023} eine verbesserte Audiogenerierung auf der Grundlage textueller Eingaben im Vergleich zu \emph{Diffsound}. \emph{AUDIOGEN} operiert auf einer gelernten diskreten Audio-Repräsentation und ist in der Lage daraus Audiosignale zu generieren. Die Architektur dieses Modells besteht aus zwei Hauptkomponenten (siehe Abbildung \ref{fig:audiogen}). Die erste Komponente kodiert Audio mittels \emph{neural audio compression}\cite{zeghidour_soundstream_2021} in eine diskrete Tokensequenz. Das hierfür verwendete Modell wurde darauf trainiert, komprimierte Audiosignale zu rekonstruieren. Auf die durch diese erste Komponente erzeugten Audio-Tokens versucht ein \emph{autoregressiven} \emph{Transformer-decoder} \emph{Sprachmodell}, das dem \emph{GPT2} \cite{alec_radford_jeff_wu_rewon_child_david_luan_dario_amodei_ilya_sutskever_language_2019} ähnelt, unter Berücksichtigung von Textinformationen abzubilden. Während der Inferenz extrahiert das \emph{Transformer}-Modell die zur Eingabe passenden Tokens, die anschließend durch den Decoder der ersten Komponente in Audio umgewandelt werden können. Das entwickelte Modell zeigt jedoch Schwierigkeiten, temporale Umformulierungen korrekt zu verarbeiten. \cite{kreuk_audiogen_2023}

\begin{figure}[h]
  \centering
  \includegraphics[width=.5\textwidth]{graphics/Audiogen.png}
  \caption[AUDIOGEN Architektur]{AUDIOGEN Architektur \cite{kreuk_audiogen_2023}}
  \label{fig:audiogen}
\end{figure}

Das Modell \emph{Tango} \cite{ghosal_text--audio_2023} zieht Inspiration aus \emph{AudioLDM} und verfolgt das Ziel, Text in Audio mittels latenter Diffusion zu transformieren. Es hebt sich in bestimmten Aspekten von \emph{AudioLDM} ab und strebt an, überlegene Resultate trotz reduziertem Datensatz zu erziehlen. Die Konstruktionsweise setzt sich aus drei zentralen Bestandteilen zusammen (siehe Abbildung \ref{fig:tango}): einem \emph{Text Encoder} zur Verarbeitung der Eingabeprompts, einem \emph{Latenten Diffusionsmodell} (\emph{LDM})\cite{rombach_high-resolution_2022}, und einem \emph{Variational Autoencoder} (\emph{VAE})\cite{kingma_auto-encoding_2022}. Der \emph{Text Encoder}, angereichert durch das \emph{Large Language Model} (\emph{LLM}) \emph{FLAN-T5-LARGE} \cite{chung_scaling_2022}, formt eine textbasierte Darstellung des gegebenen Inputs. Untersuchungen \cite{dai_why_2023} zufolge vermögen \emph{FLAN-T5-Modelle} durch ihr extensives Vortraining - reich an Gedankenstrukturen, Argumentationslinien und Anweisungen - rasch und wirkungsvoll neue Aufgaben zu erlernen. Dieses intensive Vortraining, das älteren Textmodellen wie \emph{RoBERTa}\cite{liu_roberta_2019} fehlte, könnte die Betonung essenzieller Details vereinfachen, was eine optimierte Transformation von Texten in ihre auditiven Pendants begünstigen könnte. Das \emph{LDM}, adaptiert von \emph{AudioLDM}, wandelt die textbasierte Darstellung des Prompts in eine latente Audio-Abbildung mittels Spektogramm-Tokens um. Dieser Vorgang beruht auf dem synchronisierten Wechselspiel von Vorwärts- und Rückwärtsdiffusion: Dem Audio wird Rauschen beigemischt, welches anschließend unter Anleitung des Textes wieder entfernt wird. Bei der Auswertung wird der trainierte Rückwärtsdiffusionsablauf genutzt, um die Audio-Darstellung zu formen. Der \emph{VAE}-Decoder wandelt dies dann in ein Mel-Spektogramm um. Der \emph{Vocoder HiFi-GAN}\cite{kong_hifi-gan_2020}, identisch zu dem in \emph{AudioLDM}, konvertiert das Spektogramm letztlich in ein Audiosignal. Eine Schwäche des Modells liegt in der Erstellung nuancierter Audio-Outputs für ähnliche Texte, besonders wenn es nur auf kleineren Datensätzen trainiert wurde. Die Autoren empfehlen daher, dieses Hindernis durch Training auf umfangreicheren Datensätzen zu überbrücken, um die Kapazitäten des Modells in Bezug auf Differenzierung und Generierung von diversen Text-Audio-Kombinationen zu steigern. \cite{ghosal_text--audio_2023}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\textwidth]{graphics/Tango.png}
  \caption[Tango Architektur]{Tangos Architektur \cite{liu_roberta_2019}}
  \label{fig:tango}
\end{figure}

Ein zusätzliches Modell, das sich der Methode der \emph{Latenten Diffusion}\cite{rombach_high-resolution_2022} bedient, um Text in Audio zu konvertieren, ist \emph{Make-An-Audio} \cite{huang_make--audio_2023}. Parallel zur Entwicklung dieses Modells (siehe Abbildung \ref{fig:Make-An-Audio_Architecture}) wurde eine Methode konzipiert, um ungelabelten Audiosignalen eine Beschreibung zuzuweisen (siehe Abbildung \ref{fig:Make-An-Audio_DestillATIon}). Diese Methode soll den Mangel an gelabelten Audiosignalen adressieren. Der Prozess zur Umschreibung ungelabelter Audiosignale gliedert sich in zwei Stufen. In der ersten Stufe kommen vortrainierte Expertenmodelle \cite{xu_crnn-gru_2020, deshmukh_audio_2022, koepke_audio_2023} zum Einsatz, um Wissen zu destillieren. Diese Modelle generieren Texte, die das Audiosignal beschreiben. Der Text mit der höchsten \emph{CLAP}-Bewertung \cite{elizalde_clap_2022} wird als finaler Text ausgewählt. Zur Vermeidung von Overfitting im Modell wird in der zweiten Stufe aus einer Datenbank mit gelabelten Audioereignissen bis zu zwei Signale extrahiert und mit dem aus dem ersten Schritt generierten Paar kombiniert. Der \emph{Diffusions}-Prozess arbeitet auf einer latenten Repräsentation des Audios, die durch einen Spektrogramm-\emph{Autoencoder} erlernt wurde. Ein \emph{Encoder}-Netz empfängt ein Spektrogramm und komprimiert dieses in den \emph{Latentenraum}. Ein \emph{Decoder} zielt darauf ab, das komprimierte Audiosignal aus dem latenten Raum zu rekonstruieren. Das Modell wird daraufhin trainiert, den durch die Rekonstruktion entstandenen Verlust zu minimieren und mit einem zusätzlichen \emph{Diskriminator}, der das Unterscheiden zwischen echten und generierten Audiosignalen erlernt. Ein \emph{Vocoder} kann schließlich das rekonstruierte Spektrogramm in ein Audiosignal umwandeln. Abseits von textbasierten Eingaben können Audiosignale auch aus anderen Quellen, wie Bildern und Videos, generiert werden.
 

\begin{figure}[h]
\centering
\begin{subfigure}{1.0\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{graphics/Make-An_Audio.png}
  \caption[Make-An-Audio Architektur]{Make-An-Audio Architektur\cite{huang_make--audio_2023}. Die Parameter der mit einem Schloss gekennzeichnete Module bleiben während Trainings konstant}
  \label{fig:Make-An-Audio_Architecture}
\end{subfigure}

\vspace{1em} % Optional: Add some vertical space between the subfigures

\begin{subfigure}{1.0\textwidth}
  \centering
  \includegraphics[width=0.7\textwidth]{graphics/Make-An-Audio-Destillation.png}
  \caption[Make-An-Audio Destillation]{ake-An-Audio Destillation \cite{huang_make--audio_2023}}
  \label{fig:Make-An-Audio_DestillATIon}
\end{subfigure}
\caption[Make-An-Audio Module]{Make-An-Audio Module}
\label{fig:test}
\end{figure}

Die musikalische Anwendung neuronaler Netze beschränkt sich nicht ausschließlich auf die konditionierte Synthese von Klängen. Vielmehr ermöglichen solche Netzwerke auch eine zielgerichtete musikalische Generierung. Modelle wie \emph{MusicLM} \cite{agostinelli_musiclm_2023} und \emph{MusicGen} \cite{copet_simple_2023} stützen sich auf bewährte Modelle und Ansätze der akustischen Klangsynthese. Sie erweitern diese jedoch durch die Fähigkeit, vollständige musikalische Audiosignale zu generieren, die kohärente Strukturen über längere Zeiträume beibehalten. 

Das Modell \emph{MusicLM} basiert auf den Konzepten von \emph{AudioLM}. Es integriert akustische Informationen über \emph{Soundstream} \cite{zeghidour_soundstream_2021}, semantische Informationen mittels \emph{w2v-Bert} \cite{chung_w2v-bert_2021-1} und Audio-Informationen mithilfe von \emph{Mulan} \cite{huang_mulan_2022}. Im Training dieses Modells erfolgte eine Abbildung des latenten Raums von \emph{MuLan} zum latenten Raum von \emph{w2v-Bert} und ebenso eine Abbildung von \emph{w2v-Bert} zu \emph{Soundstream} unter Verwendung von \emph{Transformers}. Während der Inferenz können die erlernten Abbildungen genutzt werden. Eine von \emph{MuLan} generierte Texteingabe wird hierbei in den latenten Raum von \emph{SoundStream} transformiert, um anschließend in ein Audiosignal kodiert zu werden. \cite{agostinelli_musiclm_2023}


Im Gegensatz zu \emph{MusicLM} setzt \emph{MusicGen} nicht auf mehrere kaskadierende Modelle, sondern verwendet einen \emph{autoregressiven Transformer-basierten Decoder} \cite{vaswani_attention_2017}, der auf Text oder Melodien, wie beispielsweise Summen, konditioniert werden kann. Dieses Modell operiert auf einer komprimierten Darstellung von \emph{gesampelten} Audiosignalen, die durch \emph{EnCodec} \cite{defossez_high_2022} generiert wird. \emph{EnCodec} ist ein \emph{convolutional auto-encoder}, der einen \emph{Latentenraum} mithilfe von \emph{Residual Vector Quantization} \cite{zeghidour_soundstream_2021} erstellt. Abhängig von der Anzahl der in dem Quantifizierungsprozess verwendeten \emph{codebooks} werden entsprechend viele Tokens generiert, die speziell angeordnet und anschließend dem \emph{Decoder} von \emph{EnCodec} übergeben werden. Dieser kann unter der gegebenen Konditionierung eine Audiodatei erstellen. Für die Textkonditionierung wurden Modelle wie \emph{T5} \cite{raffel_exploring_2020}, \emph{FLAN-T5} \cite{roberts_scaling_2022} und \emph{CLAP} evaluiert, während für Melodien eine Konditionierung über ein \emph{Chromagramm} erfolgt.


\chapter{Methoden}

\section{Klangsynthese und Musikproduktion}

\subsection{Mathematische und physikalische Modellierung von Musik und Klang}\label{sec:music_math}
\glqq Musik ist eine Kunstform und kulturelle Aktivität, deren Medium der in Zeit organisierte Klang ist\grqq \footnote{"Music is an art form and cultural activity whose medium is sound organized in time"} \cite{tsuji_physics_2021}. Im Gegensatz zu Rauschen weisen die Klänge, die Musik aufbauen, Strukturen und Zusammenhänge auf, welche für das menschliche Gehör als angenehm wahrgenommen werden \cite{parker_good_2009}. 

\emph{Klang} charakterisiert sich durch ein intrinsisches Zusammenspiel aus physikalischen und perzeptiven Elementen. Auf der physikalischen Ebene manifestiert sich der Klang als Welle, die durch einen schwingenden Körper erzeugt und von einem Ort zum anderen propagiert. Diese Welle setzt sich aus einem Grundton sowie mehreren resonierenden Einzeltönen zusammen. Der resultierende Klang weist eine Vielzahl von Obertönen auf, die die charakteristischen klanglichen Eigenschaften, auch \emph{Klangfarben} genannt, hervorbringen. \cite{tsuji_physics_2021, parker_good_2009}

Diese diversen \emph{Töne} sind periodische Schwingungen, definiert durch eine \emph{Tonhöhe/Frequenz} $\omega$ (in $Hz$). Diese kann als Anzahl der Kompressionen an einem festgelegten Punkt pro Sekunde verstanden werden. Der Kehrwert der Frequenz, bezeichnet als \emph{Periode} $T=\frac{1}{\omega}$ (in $s$), gibt die Dauer an, die eine Kompression benötigt, um zwei identische Punkte zu durchlaufen. Töne mit einer niedrigen Frequenz erscheinen in unserer Wahrnehmung als tief und dumpf, wohingegen Töne mit hohen Frequenzen als leicht, schwebend und durchdringend empfunden werden. Die \emph{Amplitude} der Schwingung beschreibt die transportierte Energie und demzufolge die Lautstärke eines Tones. Aufgrund des Abstandsgesetzes wird diese von einer Schallquelle über die logarithmische \emph{Dezibel-Skala} (dB) angegeben. \cite{tsuji_physics_2021, parker_good_2009}

Die Analyse der Struktur eines Klangs konzentriert sich auf die Beziehungen und Interaktionen der diversen Töne, die den Klang konstituieren. Die elementarste Darstellung eines Tones ist der Sinuston, welcher durch eine Sinuskurve repräsentiert wird. Gemäß dem Fourier-Theorem besteht jeder Ton aus einer Kombination verschiedener Sinustöne (siehe Abbildung \ref{fig:evolution}). Dabei wird die tiefste dominierende Frequenz als \emph{Grundton} und die höheren Frequenzen als \emph{Obertöne} klassifiziert. Diese Obertöne variieren in ihrer Frequenz, Amplitude sowie ihrem zeitlichen Auf- und Abbau. Ein Oberton, dessen Frequenz ein ganzzahliges Vielfaches des Grundtones darstellt, wird als \emph{harmonischer} Oberton definiert. Ist dies nicht der Fall, wird der Oberton als \emph{inharmonisch} bezeichnet. Daraus folgt, dass verschiedene Instrumente, obwohl sie denselben Ton erzeugen, identische Grundschwingungen, aber unterschiedliche harmonische und inharmonische Obertöne aufweisen.\cite{parker_good_2009, white_physics_2014, ruschkowski_elektronische_2019}

\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{evolution.png}
  \caption[Fourier Reihe]{Zusammensetzung Sägezahn- und Rechtecksschwingungen aus harmonischen Oberschwingungen einer Sinusschwingung}
  \label{fig:evolution}
\end{figure}

Die Beschaffenheit eines Klangs lässt sich durch die Amplitudenfaktoren und die korrespondierenden Frequenzen der jeweiligen Töne beschreiben und in einem \emph{Frequenzspektrum} (siehe Abbildung \ref{fig:spectro}) darstellen. Diese Visualisierung ermöglicht die Identifikation dominanter Frequenzen oder Frequenzbereiche innerhalb des Signals und bietet eine Illustration der Klangfarbe sowie der spezifischen Charakteristik des Klangs. Mittels einer \emph{Fourier-Transformation} kann das zugehörige Spektrum für jedes gegebene Signal ermittelt werden. Umgekehrt erlaubt die \emph{Inverse Fourier-Transformation} die Rekonstruktion des Signals basierend auf seinem Spektrum. \cite{raffaseder_audiodesign_2010}

Zur Berechnung einer \emph{Diskreten Fourier-Transformation (DFT)} hat sich die \emph{Fast Fourier-Transformation (FFT)} als effizienter Algorithmus etabliert \cite{heideman_gauss_1985}. Zur Realisierung der Fourier-Transformation in Echtzeit wird auf die \emph{Short Time Fourier-Transformation} zurückgegriffen \cite{thyagarajan_introduction_2019}. 

Ein \emph{Spektrogramm} erlaubt die Analyse eines Signals sowohl im zeitlichen als auch im Frequenzbereich (siehe Abbildung \ref{fig:spectro}). Jedoch ist die Genauigkeit nicht stets gleichbleibend. Bei der Zerlegung des Signals in kurze zeitliche Segmente zur Spektrumsberechnung zeigen längere Segmente eine reduzierte zeitliche Auflösung, profitieren jedoch von einer präziseren Frequenzauflösung. Im Gegensatz dazu bieten kürzere Segmente eine höhere zeitliche Präzision, weisen aber eine geringere Frequenzauflösung auf. \cite{raffaseder_audiodesign_2010}

\emph{Spektogramme}, die sich auf die \emph{Mel-Skala} stützen, nennt man \emph{Mel-Spektogramm}. Diese Skala stellt akustische Frequenzen in einer annähernd logarithmischen Weise dar, sodass Tonhöhenintervalle, die ähnlich wahrgenommen werden, über den gesamten Hörbereich konstant abgebildet werden \cite{noauthor_librosamel_frequencies_nodate}.

Ein Klang, der ein kontinuierliches Frequenzspektrum (siehe Abbildung \ref{fig:spectro}) aufweist und viele Arten von Geräuschen aus unterschiedlichen Quellen beinhaltet, wird als \emph{Rauschen} definiert \cite{tsuji_physics_2021}.

Der für das menschliche Gehör wahrnehmbare Frequenzbereich, welcher von $20 , \text{Hz}$ bis $20 , \text{kHz}$ reicht, lässt sich in drei Segmente unterteilen: \emph{Bässe, Mitten und Höhen}. Die Bässe erstrecken sich von $20 , \text{Hz}$ bis $250 , \text{Hz}$. Die tiefen Mitten umfassen den Bereich von $250 , \text{Hz}$ bis $2 , \text{kHz}$, während sich die hohen Mitten von $2 , \text{kHz}$ bis $4 , \text{kHz}$ erstrecken. Frequenzen oberhalb von $4 , \text{kHz}$ werden als Höhen bezeichnet. \cite{raffaseder_audiodesign_2010}

\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{spectrums_flat.png}
  \caption[Spektren, Spektrogramme und Mel-Spektrogramme]{Spektren, Spektrogramme und Mel-Spektrogramme verschiedener Klänge}
  \label{fig:spectro}
\end{figure}


\subsection{Digitale Audiorepräsentation und Sampling}

Aufgrund der binären Beschaffenheit digitaler Computer erfordert die Darstellung und Verarbeitung von Audiosignalen eine Umwandlung des Signals von einem kontinuierlichen in einen diskreten Wertebereich. Das \emph{Abtasttheorem} postuliert, dass analoge Signale redundante Informationen beinhalten, die bei einer Übertragung eliminiert werden können. Daher genügt es, das Audiosignal durch Erfassen von Amplitudenwerten in regelmäßigen Intervallen zu reproduzieren und zu übermitteln. Um das Signal adäquat zu charakterisieren, sollte die Abtastrate wenigstens das Doppelte der maximalen Frequenz des Signals betragen. Diese Schranke wird als \emph{Nyquist-Frequenz} bezeichnet und ist gleich $2 \omega \mathrm{Hz}$. Da das menschliche Ohr lediglich Frequenzen bis zu $20 , \text{kHz}$ detektiert, wird in der Praxis oft eine \emph{Abtastrate} von $44.1 , \text{kHz}$ angewendet. Unterschreitet die Abtastrate die Nyquist-Frequenz, resultiert ein Informationsverlust, und die rekonstruierte Wellenform kann von der ursprünglichen abweichen. In solch einem Szenario ist der Vorgang der Signalrekonstruktion nicht länger deterministisch, was als \emph{Unterabtastung} bezeichnet wird. \cite{lai_practical_2004, shannon_communication_1949, ruschkowski_elektronische_2019}

Die \emph{Samplingtiefe} (engl. \emph{Bitdepth}) definiert den Wertebereich, innerhalb dessen die erfassten Amplitudenwerte dargestellt werden. Ein größerer Wertebereich ermöglicht eine präzisere Repräsentation des ursprünglichen Signals, erfordert jedoch auch einen erhöhten Speicherbedarf. Ist der Wertebereich hingegen eingeschränkt, müssen die Amplitudenwerte intensiver gerundet werden, was zu Artefakten und Unregelmäßigkeiten im Signal führen kann. \cite{thompson_understanding_2005}

\emph{Sound-Sampling} oder schlicht \emph{Sampling} in Verbindung mit digitaler oder analoger Klangmodifikation stellt eine eigenständige Technik der Klangsynthese dar. Obwohl es im Wesentlichen eine Reproduktion des entnommenen Signals darstellt, geht es über eine bloße Speicherung hinaus. Prinzipien und Verfahren aus der analogen Synthese und Klanggestaltung können auf die Reproduktion des Klangs angewendet werden. Das resultierende Instrument wird als \emph{Sampler} bezeichnet. Abhängig von seiner Komplexität ermöglicht es dem Nutzer, das Signal in einem definierten Bereich zu wiederholen, zu verlangsamen, zu beschleunigen, zu invertieren oder spezifische Intervalle neu zu ordnen. Dies bietet auch die Gelegenheit zum musikalischen Zitieren, was jedoch rechtliche Fragen bezüglich des klanglichen Diebstahls für Musiker, Verlage und Plattenlabels aufwirft. \cite{russ_sound_2009, ruschkowski_elektronische_2019, katz_capturing_2010}

Die Wiederverwendung und Aufarbeitung von Teilen oder vollständigen, bereits produzierten Werken ist in der Industrie gängige Praxis. Dieses Vorgehen könnte durch den Einsatz generativer künstlicher Intelligenz vermieden werden.

\subsection{Synthetisierung und Klangformung} \label{sec:synth+envelope}

Instrumente, die dazu dienen, musikalische Klänge durch elektronische Spannung zu generieren und zu manipulieren, werden als \emph{Synthesizer} bezeichnet \cite{dudenredaktion_synthesizer_nodate, pirkle_designing_2021}. Die Bestandteile eines solchen Instruments lassen sich in drei Hauptkategorien einteilen (siehe Abbildung \ref{fig:synth}): \emph{Erzeuger} (engl. \emph{source}), \emph{Modifikatoren} (engl. \emph{modifier})und \emph{Kontrollinstanzen} (engl. \emph{controls}). Erzeuger, wie beispielsweise Oszillatoren, generieren den Grundklang, welcher in diesem speziellen Projekt durch ein generatives Diffusions-Modell erzeugt wird. Modifikatoren, zu denen Filter und Effekte gehören, bearbeiten diesen generierten Klang. Kontrollinstanzen ermöglichen es dem Nutzer, die Parameter der Erzeuger und Modifikatoren zu definieren und flexibel anzupassen. Die Veränderung eines Parameters wird als \emph{Modulieren} bezeichnet, während die spezifischen Einstellungen der Parameter als \emph{Patch} gelten. \cite{pirkle_designing_2021}   

Die für die Realisierung des Projekts erforderlichen Komponenten werden im Nachfolgenden erläutert.

\begin{figure}[h]
  \centering
  \includegraphics[width=.5\textwidth]{graphics/synthstruc.png}
  \caption[Synth]{Komponenten eines Synthesizers \cite{russ_sound_2009}}
  \label{fig:synth}
\end{figure}


In dem Bestreben, sein fundiertes wissenschaftliches Verständnis und seine Expertise in den Bau musikalischer Instrumente zu integrieren, konzipierte der Physiker und Komponist Hugh Le Caine erste Apparaturen zur elektronischen Klangsynthese - und das 20 Jahre vor der Kommerzialisierung der ersten analogen Synthesizer \cite{young_gale_hugh_2013}. In diesem Zusammenhang formulierte Le Caine mehrere Grundsätze, die ein Instrumentenentwickler nach seiner Meinung verfolgen sollte. Er war der festen Überzeugung, dass ein Musiker maximale Kontrolle über die zentralen Parameter eines Klangs haben sollte. Dabei identifizierte er, dass die Ausdruckskraft eines Instruments insbesondere durch die Steuerung der \emph{Anschlagsdynamik} über den gesamten Lautstärkebereich eines Tones, durch die Regulierung seiner \emph{An- und Ausschwingszeiten} sowie durch die Klangfarbe bestimmt wird. \cite{ruschkowski_elektronische_2019}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\textwidth]{graphics/ADSR.png}
  \caption[Hüllkurve]{Beispiel einer Hüllkurve \cite{russ_sound_2009}}
  \label{fig:adsr}
\end{figure}

Allgemein wird das Ziel verfolgt, Töne zu generieren, die sich sowohl zeitlich als auch in ihrer Frequenz verändern oder übergehen, um akustisch ansprechende Ergebnisse zu erzielen \cite{pirkle_designing_2021}. Die Steuerung von Anschlagsdynamik sowie An- und Ausschwingszeiten wird durch einen \emph{Hüllkurvengenerator} (engl. \emph{envelope generator}) ermöglicht. Dieser Generator besteht in der Regel aus vier Teilen. Die \emph{Anschwellzeit} (engl. \emph{attack time}) gibt die Dauer an, die nach dem Anschlagen einer Taste vergeht, bis ein Ton seine höchste Lautstärke erreicht. Entsprechend bestimmt die \emph{Abschwellzeit} (engl. \emph{decay time}) den Zeitraum, in dem der Ton bis zu einem voreingestellten \emph{Lautstärkeniveau} (engl. \emph{sustain level}) abfällt. Dieses Niveau wird beibehalten, bis die Taste losgelassen wird. Danach kann eine Zeitspanne festgelegt werden, in der der Ton komplett ausklingt (engl. \emph{release time}). Aufgrund der Bezeichnungen dieser Teile wird der Generator oft als \emph{ADSR}-Generator (engl. \emph{attack, decay, sustain, release}) (Abbildung \ref{fig:adsr}) genannt. Instrumente wie Streichinstrumente, die mit einem Bogen gespielt werden, haben lange Anschwell-, Abschwell- und Ausschwingzeiten. Im Gegensatz dazu haben gezupfte Instrumente kürzere Anschwellzeiten. Klaviere und Schlaginstrumente zeichnen sich durch sehr schnelle Anschwellzeiten aus. \cite{ruschkowski_elektronische_2019, russ_sound_2009}

Die \emph{Klangfarbe}, die durch den speziellen Erzeuger entsteht, kann durch Filter weiter verändert werden. In der Elektrotechnik werden diese verwendet, um bestimmte Frequenzen zu entfernen. Insbesondere der \emph{Tiefpassfilter} (engl. \emph{low-pass}) und der \emph{Hochpassfilter} (engl. \emph{high-pass}) sind wichtig, da sie entweder die höheren oder tieferen Teile des Frequenzspektrums reduzieren. Der Punkt, an dem diese Reduzierung beginnt, wird als \emph{Beschneidungsfrequenz} (engl. \emph{cut-off-frequency}) bezeichnet. Mit diesen Filtern kann man bestimmte Obertöne verstärken oder abschwächen. Wenn man diese Filter hintereinander schaltet, erhält man einen \emph{Bandpassfilter}. Es ist auch möglich, Obertöne in der Nähe der Beschneidungsfrequenz zu betonen, was eine \emph{Resonanz} (engl. \emph{resonance}) nachahmt. \cite{ruschkowski_elektronische_2019}

\emph{Kontrollinstanzen} können in zwei Gruppen eingeteilt werden. Die erste Gruppe beinhaltet Steuerelemente, die durch die Performance beeinflusst werden, wie z.B. ein Klaviaturkeyboard. Dies ermöglicht dem Musiker, die Tonhöhe nach seinen Wünschen zu verändern und so das Instrument zu steuern. Die zweite Gruppe umfasst feste Parameter, die spezifisch für den Synthesizer sind. Diese werden verwendet, um die Klangcharakteristik zu bestimmen. Dazu gehören zum Beispiel die Drehregler und Tasten eines Synthesizers. \cite{russ_sound_2009}

Das Kommunikationsprotokoll \emph{MIDI} (\emph{Musical Instrument Digital Interface})\cite{midi_association_midi_nodate} wurde eingeführt, um eine einheitliche Steuerung von Parametern und Kommunikation zwischen elektronischen Geräten zu gewährleisten. Es hat sich als globaler Standard in der elektronischen Musikbranche etabliert und erlaubt MIDI-Instrumenten, Daten auszutauschen. \cite{ruschkowski_elektronische_2019}

Die Einführung des \emph{MIDI}-Protokoll hatte einen erheblichen Einfluss auf die Entwicklung und das Design von Synthesizern. Aufgrund der durch MIDI bedingten Standardisierung vieler Designaspekte von Synthesizern wurde die Klangerzeugungsmethode prominenter, während das funktionale Instrumentendesign weniger betont wurde. \cite{russ_sound_2009}

\section{Synthetisierung mittels Diffusion}

\glqq Die Methoden der Klangerzeugung mit traditionellen mechanischen Musikinstrumenten waren seit ihrer Entstehung nur unwesentlichen Veränderungen unterworfen. [...] Der Instrumentenbau wandelte sich lediglich deshalb im Laufe der Zeit, weil man die Spielbarkeit der Instrumente zu verbessern und ihren Klangcharakter den wechselnden musikalischen Idealvorstellungen anzupassen wünschte. Anders dagegen im Bereich elektronischer Klangerzeugung. Hier werden ständig neue Methoden zur Synthese von Klängen entwickelt.\grqq \, \cite{ruschkowski_elektronische_2019}

Die innovative Technik der Soundsynthese durch Diffusion bietet verschiedene Vorteile. Sie könnte die Abhängigkeit von aufgezeichneten Klängen, deren Aufnahme oft kostenintensiv ist, reduzieren. Künstliche Intelligenz in der Sound-Synthese könnte diesen Sektor demokratisieren und mehr Menschen Zugang zu benötigten Ressourcen bieten. Zudem könnten Nutzer mehr Kontrolle über Klangfarben und -eigenschaften erhalten. Kreativ gesehen ermöglicht diese Methode die Schaffung einzigartiger Klänge, die Künstlern als Inspirationsquelle dienen könnten. \cite{haohe_liu_audioldm_2023}


\subsection{Diffusion}
Unter der Prämisse, dass die untersuchten Daten einer bestimmten Verteilung entstammen, zielen generative Machine-Learning-Modelle darauf ab, diese Verteilung zu identifizieren und zu approximieren. Die Erstellung neuer Daten erfolgt durch das Ziehen einer Probe aus dieser nachgebildeten Verteilung. \cite{machine_learning_at_berkeley_diffusion_2022}

Der Prozess der \emph{Diffusion} im Maschinellen Lernen \cite{sohl-dickstein_deep_2015, ho_denoising_2020, nichol_improved_2021, dhariwal_diffusion_2021} wurde von der statistischen Thermodynamik \cite{jarzynski_equilibrium_1997} und der sequenziellen Monte-Carlo-Methodik \cite{neal_annealed_1998} inspiriert. Ziel ist es, iterative Verzerrungen einer Datenverteilung $x_0\sim q(\mathbf{x}_0)$ (\emph{forward diffusion}) durch einen umgekehrten Prozess zu korrigieren (\emph{reverse diffusion}). Das Ergebnis ist ein generatives Modell, das effizienter trainiert und bewertet werden kann als vergleichbare Ansätze. \cite{sohl-dickstein_deep_2015, nichol_improved_2021}

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\textwidth]{forwardDiffusion.png}
  \caption[Vorwärtsprozess Diffusion]{Vorwärtsprozess Diffusion \cite{machine_learning_at_berkeley_diffusion_2022}}
  \label{fig:forwardDiffusion}
\end{figure} 

Der Vorwärtsprozess (Abbildung \ref{fig:forwardDiffusion}) ist als Markov-Kette gestaltet \cite{sohl-dickstein_deep_2015, ho_denoising_2020}. Einem Datenpunkt $x_0$ wird ein geringes Rauschen hinzugefügt, das durch den Hyperparameter $\left\{\beta_t \in(0,1)\right\}_{t=1}^T$, auch als \emph{noise schedule} bekannt, bestimmt wird \cite{ho_denoising_2020, machine_learning_at_berkeley_diffusion_2022}. Hierbei wird \emph{Gauss'sches Rauschen} \cite{shannon_communication_1949} verwendet, repräsentiert durch eine Normalverteilung $\mathcal{N}$. Wiederholt man diesen Vorgang über $T$ Schritte, werden sukzessive Informationen entfernt, bis schließlich nur isotropisches Rauschen verbleibt \cite{machine_learning_at_berkeley_diffusion_2022}.

\begin{align}
   & q\left(\mathbf{x}^{(t)} \mid \mathbf{x}^{(t-1)}\right) = \mathcal{N}\left(\mathbf{x}^{(t)} ; \mathbf{x}^{(t-1)} \sqrt{1-\beta_t}, \mathbf{I} \beta_t\right) \\
   & q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)=\prod_{t=1}^T q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)
\end{align}

Unter Verwendung der Relation \( \alpha_t:=1-\beta_t \) und \( \bar{\alpha}_t:=\prod_{s=1}^t \alpha_s \) lässt sich die Gleichung für den Vorwärtsprozess wie folgt formulieren:

\begin{equation}
    q\left(\mathbf{x}_t \mid \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_t ; \sqrt{\bar{\alpha}_t} \mathbf{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)
\end{equation}

\begin{figure}[h]
  \centering
  \includegraphics[width=.8\textwidth]{reverseDiffusion.png}
  \caption[Rückwertsprozess Diffusion]{Rückwertsprozess Diffusion \cite{machine_learning_at_berkeley_diffusion_2022}}
  \label{fig:reverseDiffusion}
\end{figure} 

Während des Rückwärtsprozesses (siehe Abbildung \ref{fig:reverseDiffusion}) wird versucht, das eingefügte Rauschen $\epsilon \sim \mathcal{N}(0,1)$ schrittweise zu entfernen \cite{machine_learning_at_berkeley_diffusion_2022}. Dies lässt den umgekehrten Prozess so erscheinen, als würde er aus dem Rauschen neue Daten generieren \cite{machine_learning_at_berkeley_diffusion_2022}. Bei einem niedrigen Wert von $\beta$ stimmt die Rauschverteilung des Rückwärtsschritts \( q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right) \) mit dem Vorwärtsprozess überein \cite{sohl-dickstein_deep_2015}. Der erlernte Rückwärtsprozess \( p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right) \) kann dementsprechend approximiert werden \cite{ho_denoising_2020, machine_learning_at_berkeley_diffusion_2022, nichol_improved_2021}. Das Training fokussiert sich darauf, kleinere Abweichungen zu schätzen, anstatt den gesamten Ablauf in einem einzigen Schritt durch eine Funktion zu repräsentieren \cite{sohl-dickstein_deep_2015}.


\begin{align}
& p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_\theta\left(\mathbf{x}_t, t\right), \boldsymbol{\Sigma}_\theta\left(\mathbf{x}_t, t\right)\right)\\
& p_\theta\left(\mathbf{x}_{0: T}\right)=p\left(\mathbf{x}_T\right) \prod_{t=1}^T p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)
\end{align}

Das Erlernen des Rückwärtsprozesses wird durch ein neuronales Netzwerk durchgeführt. In jedem Schritt können das Netzwerk den Erwartungswert $\boldsymbol{\mu}_\theta$, das ursprüngliche Bild $\boldsymbol{x}_0$ oder das hinzugefügte Rauschen $\boldsymbol{\epsilon}$ vorhersagen \cite{ho_denoising_2020, nichol_improved_2021}. Untersuchungen zeigten, dass die Vorhersagen des Bildes weniger präzise waren. Daher wurde die Entscheidung getroffen, das hinzugefügte Rauschen $\boldsymbol{\epsilon}$ unter Verwendung der vereinfachten Verlustfunktion 3.6 vorherzusagen \cite{ho_denoising_2020}.

\begin{equation}
    L_{\text {simple }}=E_{t, x_0, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(x_t, t\right)\right\|^2\right]
\end{equation}

Der Erwartungswert $\boldsymbol{\mu}_\theta$ kann aus dem vorhergesagten Rauschen $\boldsymbol{\epsilon}$ mithilfe der nachstehenden Gleichung abgeleitet werden:

\begin{equation}
    \mu_\theta\left(x_t, t\right)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta\left(x_t, t\right)\right)
\end{equation}

Die in \cite{ho_denoising_2020} präsentierte Architektur des neuronalen Netzwerks basiert auf einem \emph{U-Net} \cite{ronneberger_u-net_2015}, welches auf einem \emph{Wide ResNet} \cite{zagoruyko_wide_2017} aufbaut \cite{ho_denoising_2020}.

In \cite{nichol_improved_2021} wurden signifikante Verbesserungen im Vergleich zu den Arbeiten von \cite{sohl-dickstein_deep_2015} und \cite{ho_denoising_2020} vorgenommen. Eine dieser Neuerungen war das Lernen der Varianz anstelle ihrer festen Einstellung. Zudem wurde der \emph{schedule} Parameter optimiert. Statt einer konstanten Einstellung, die dazu führte, dass die Datenpunkte am Ende übermäßig verrauscht waren und die anfänglichen Schritte zu viel Information verloren, wurde ein \emph{cosinus schedule} eingeführt \cite{nichol_improved_2021}.

\subsection{Latente Diffusion}

In \cite{rombach_high-resolution_2022} wurde das \emph{Latente Diffusionsmodell} (\emph{LDM}) vorgestellt, welches den Diffusionsprozess \cite{sohl-dickstein_deep_2015, ho_denoising_2020, nichol_improved_2021, dhariwal_diffusion_2021} erweitert, um die Generierung von Bildern basierend auf einer Eingabe zu ermöglichen. Anders als bei früheren Diffusionsprozessen fokussiert sich dieser Ansatz nicht auf die Pixelwerte der Bilder, sondern auf ihre latente Darstellung. Dies reduziert den Rechenaufwand erheblich und ermöglicht sowohl das Training als auch die Inferenz auf eingeschränkter Hardware. Die in Abbildung \ref{fig:LDM} dargestellte Architektur besteht aus drei Hauptteilen: Ein \emph{Variational Autoencoder}, der die Daten im Latentenraum kodiert und dekodiert, der Diffusionsteil und ein zusätzliches Modul, das den Diffusionsteil mithilfe von Text, Bildern oder anderen Eingaben konditioniert.

\begin{figure}[h]
  \centering
  \includegraphics[width=.8\textwidth]{LDM.png}
  \caption[LDM Architektur]{LDM Architektur\cite{rombach_high-resolution_2022}}
  \label{fig:LDM}
\end{figure} 

Durch das vorausgehende Training eines \emph{Variational Autoencoders} \cite{kingma_auto-encoding_2022} gemäß \cite{esser_taming_2021} wird der latente Raum für den Diffusionsprozess erzeugt. Dabei transformiert der resultierende \emph{Encoder} $\mathcal{E}$ ein RGB-Bild $x \in \mathbb{R}^{H \times W \times 3}$ in seine latente Darstellung $z \in \mathbb{R}^{h \times w \times c}$, dargestellt als $z=\mathcal{E}(x)$. Nach dem Rückwärtsdiffusionsprozess generiert ein \emph{Decoder} $\mathcal{D}$ das Bild zurück, wobei $\tilde{x}=\mathcal{D}(z)=\mathcal{D}(\mathcal{E}(x))$ \cite{rombach_high-resolution_2022}.

Dank der niedrigen Dimensionalität und Kompression des latenten Raumes eignet sich der Diffusionsprozess besser für \emph{likelihood}-basierte generative Modelle als die Verwendung der ursprünglichen Pixelwerte. Dies liegt daran, dass semantisch wichtige Informationen stärker hervorgehoben werden und der Rechenaufwand effizienter gestaltet wird. Das zugrundeliegende \emph{U-Net} \cite{ronneberger_u-net_2015} verwendet hauptsächlich \emph{2D-Konvolutionsschichten}, um die Bildverarbeitung zu optimieren. \cite{rombach_high-resolution_2022}

Um eine von $y$ abhängige Generierung zu ermöglichen, muss das Netzwerk $\epsilon_\theta\left(z_t, t, y\right)$ konditioniert werden. Dazu wird das verwendete \emph{U-Net} durch \emph{Cross Attention} \cite{vaswani_attention_2017} erweitert. Ein spezifischer Encoder $\tau_\theta$ passt sich an verschiedene Eingabemodalitäten an und erstellt eine Repräsentation $T_\theta(y) \in \mathbb{R}^{M \times d_\tau}$. Diese Repräsentation wird mittels \emph{Cross Attention} auf die Schichten des \emph{U-Net} angewendet. Die dabei verwendete Aufmerksamkeitsfunktion ist durch $\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d}}\right)$ definiert, wobei  $Q=W_Q^{(i)} \cdot \varphi_i\left(z_t\right), K=W_K^{(i)} \cdot \tau_\theta(y), V=W_V^{(i)} \cdot \tau_\theta(y)$ entsprechend gegeben sind. Die zugehörige Verlustfunktion wird im Folgenden beschrieben \cite{rombach_high-resolution_2022}.

\begin{equation}
L_{L D M}:=\mathbb{E}_{\mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0,1), t}\left[\left\|\epsilon-\epsilon_\theta\left(z_t, t, \tau_\theta(y)\right)\right\|_2^2\right]
\end{equation}

\subsection{Clap}

\emph{Large-Scale Contrastive Language-Audio Pretraining} (\emph{Clap})\cite{wu_large-scale_2023} erschafft, mittels \emph{Contrastive Learning} eine Vorverarbeitung auf Sprach-Audio-Daten durchzuführen. Ziel ist es, eine latente Repräsentation für Audio zu erstellen. Inspiriert wurde dieses Modell von \emph{Contrastive Language-Image Pretraining} (\emph{CLIP}) \cite{radford_learning_2021}, welches den Zusammenhang zwischen Bild- und Textdaten in ähnlicher Weise lernt. Analog zum bildlichen Kontext weisen Audio und Text überlappende Informationen auf. \cite{wu_large-scale_2023}.

Das \emph{Contrastive Learning}-Modell wurde mit dem spezifisch veröffentlichten Datensatz \emph{LAION-Audio-630K} trainiert. Dieser umfasst $633,526$ Paare aus Text und Audio ($X_i^a$, $X_i^t$), die aus verschiedenen Online-Quellen stammen. Dazu gehören menschliche Stimmen, Naturklänge und Audioeffekte. Zusätzlich wurden die Datensätze \emph{AudioCaps+Clotho} \cite{kim_audiocaps_2019} \cite{drossos_clotho_2019} und \emph{AudioSet} \cite{gemmeke_audio_2017} verwendet. Alle Audiodateien wurden in ein \emph{Mono}-Signal umgewandelt und haben eine Abtastrate von 48kHz. \cite{wu_large-scale_2023}

\begin{figure}[h]
  \centering
  \includegraphics[width=.6\textwidth]{Clap.png}
  \caption[Clap Architektur]{CLAP Architektur \cite{wu_large-scale_2023}}
  \label{fig:Clap}
\end{figure} 

Die Modellarchitektur, dargestellt in Abbildung \ref{fig:Clap}, generiert Embeddings $E_i^a$ und $E_i^t$ für die Audio- $X_i^a$ und Texteingabe $X_i^t$. Hierfür wird zunächst ein Encoder $f(\cdot)$ eingesetzt, dessen Ergebnis anschließend durch ein zweischichtiges \emph{Multi-Layer-Perceptron} (\emph{MLP}) mit \emph{ReLU} \cite{agarap_deep_2019} als Aktivierungsfunktion weiterverarbeitet wird. \cite{wu_large-scale_2023}

\begin{equation}
     E_i^a = M L P_{\text{audio}}\left(f_{\text{audio}}\left(X_i^a\right)\right)
\end{equation}
\begin{equation}
    E_i^t = M L P_{\text{text}}\left(f_{\text{text}}\left(X_i^t\right)\right)
\end{equation}

Um die Rechenzeit bei längeren Audiosignalen zu minimieren, wurde ein Ansatz entwickelt, der es ermöglicht, Audioeingaben unterschiedlicher Länge in konstanter Zeit zu verarbeiten. Dabei werden sowohl globale als auch lokale Signalinformationen berücksichtigt. Bei Signalen unter 10 Sekunden wird das Signal dreifach wiederholt und mit Nullen ergänzt, bis es 10 Sekunden erreicht. Für Signale über 10 Sekunden werden vier Eingaben erstellt: Einmal wird das gesamte Signal auf 10 Sekunden \emph{downgesampled} und zusätzlich werden drei zufällige 10-Sekunden-Abschnitte aus verschiedenen Signalbereichen entnommen. Der Audio-Encoder kombiniert anschließend diese globalen und lokalen Daten, um die relevanten Informationen zu extrahieren. \cite{wu_large-scale_2023}

Einige der genutzten Datensätze enthalten für Audiosignale lediglich Keywords oder Tags statt vollständiger Beschreibungen. Um diesem Mangel zu begegnen, wurde das \emph{T5}\cite{raffel_exploring_2020} \emph{Language}-Modell verwendet, um aus diesen Stichworten und Tags umfassende Beschreibungen zu erstellen. Zusätzlich wurden die generierten Beschreibungen von Voreingenommenheiten befreit, wie zum Beispiel durch das Entfernen geschlechtsspezifischer Verzerrungen (eng. \emph{gender de-biasing}). \cite{wu_large-scale_2023}

Die gleiche Verlustfunktion mit einem Temperaturparameter $\tau$, wie in \emph{Clip}\cite{radford_learning_2021}, wurde in den MLPs verwendet \cite{wu_large-scale_2023}.

\begin{equation}
L=\frac{1}{2 N} \sum_{i=1}^N\left(\log \frac{\exp \left(E_i^a \cdot E_i^t / \tau\right)}{\sum_{j=1}^N \exp \left(E_i^a \cdot E_j^t / \tau\right)}+\log \frac{\exp \left(E_i^t \cdot E_i^a / \tau\right)}{\sum_{j=1}^N \exp \left(E_i^t \cdot E_j^a / \tau\right)}\right)
\end{equation}

Als Audio-Encoder wurden die Modelle \emph{PANN}\cite{kong_panns_2020}, basierend auf einem \emph{Convolutional Neural Network} (CNN), und \emph{HTSAT}\cite{chen_hts-at_2022}, basierend auf einem \emph{Transformer}-Modell, evaluiert. Für den Text-Encoder wurden der Text-Encoder von \emph{Clip}\cite{radford_learning_2021}, \emph{Bert}\cite{devlin_bert_2019}, und \emph{RoBERTa}\cite{liu_roberta_2019} ebenfalls ausgewertet. \cite{wu_large-scale_2023}

Das trainierte Modell eignet sich sowohl für die Audio-Klassifikation als auch zur Bestimmung eines zugehörigen Audiosignals für einen gegebenen Text $(T\rightarrow A)$ \cite{wu_large-scale_2023}. Insbesondere für die Klangsynthese ist die Konversion von Text zu Audio von großer Relevanz. Die Untersuchungsergebnisse, die durch die Kombination verschiedener Encoder aus Tabelle \ref{tab:Clap} erzielt wurden, verdeutlichen, dass \emph{HTSAT}\cite{chen_hts-at_2022} als Audio-Encoder in Kombination mit \emph{Bert}\cite{devlin_bert_2019} oder \emph{RoBERTa}\cite{liu_roberta_2019} als Text-Encoder, abhängig vom verwendeten Datensatz, die optimalsten Ergebnisse erbrachte. \cite{wu_large-scale_2023}

\begin{table}[h]
  \centering
\begin{tabular}{lcc|cc}
\hline \multirow{2}{*}{ Model } & \multicolumn{2}{c|}{ AudioCaps $(\mathrm{mAP} @ 10)$} & \multicolumn{2}{c}{ Clotho (mAP@ 10) } \\
\cline { 2 - 5 } & $\mathrm{A} \rightarrow \mathrm{T}$ & $\mathrm{T} \rightarrow \mathrm{A}$ & $\mathrm{A} \rightarrow \mathrm{T}$ & $\mathrm{T} \rightarrow \mathrm{A}$ \\
\hline PANN+CLIP Trans. & 4.7 & 11.7 & 1.9 & 4.4 \\
PANN+BERT & 34.3 & 44.3 & 10.8 & 17.7 \\
PANN+RoBERTa & 37.5 & 45.3 & 11.3 & 18.4 \\
HTSAT+CLIP Trans. & 2.4 & 6.0 & 1.1 & 3.2 \\
HTSAT+BERT & 43.7 & 49.2 & $\mathbf{1 3 . 8}$ & $\mathbf{2 0 . 8}$ \\
HTSAT+RoBERTa & $\mathbf{4 5 . 7}$ & $\mathbf{5 1 . 3}$ & $\mathbf{1 3 . 8}$ & 20.4 \\
\hline
\end{tabular}
\caption[Encoder CLAP]{Evaluation der Encoder \cite{wu_large-scale_2023}}
  \label{tab:Clap}
\end{table}

\subsection{AudioLDM}
\label{sec:AudioLDM}

\emph{AudioLDM}\cite{liu_audioldm_2023} wurde entwickelt, um sowohl die Synthese von Text zu Audio als auch die textgesteuerte Audio-Manipulation mittels \emph{Latenter Diffusion}\cite{rombach_high-resolution_2022} durchzuführen. Es bietet den Vorteil qualitativ hochwertiger Ergebnisse bei minimalem Rechenaufwand. Während \emph{Diffsound}\cite{yang_diffsound_2023} Audio-Text-Paare für Trainingszwecke nutzt, hat \emph{AudioLDM} nachgewiesen, dass durch die Verwendung einer \emph{CLAP}-Repräsentation\cite{wu_large-scale_2023} hochwertige Resultate erlangt werden können. \cite{liu_audioldm_2023}.

Die Entscheidung, Natürliche Sprache anstelle von Labels für Eingabe und Konditionierung zu nutzen, wurde getroffen, um akustische Merkmale in einer flexibleren, deskriptiveren und präziseren Weise darstellen zu können. Frühere Ansätze im Bereich Text-zu-Audio (TTA) waren durch das Fehlen großer Mengen an qualitativ hochwertigen Audio-Text-Daten limitiert\cite{liu_separate_2022}. Obwohl Methoden der Textvorverarbeitung\cite{gemmeke_audio_2017, yang_diffsound_2023} versuchten, dieses Problem zu adressieren, konnten sie die komplexen Beziehungen zwischen Klangereignissen nicht vollständig erfassen, was zu einer Beeinträchtigung der Generierungsleistung führte. Das Modell \emph{AudioLDM} begegnete dieser Herausforderung, indem es im Training ausschließlich auf Audiodaten zurückgriff und somit überzeugendere Ergebnisse im Vergleich zu Ansätzen mit gepaarten Audio-Text-Daten erzielte \cite{liu_audioldm_2023}


\begin{figure}[h]
  \centering
  \includegraphics[width=.6\textwidth]{AudioLDM.png}
  \caption[AudioLDM Architektur]{Architektur für die Text-zu-Audio Generierung von AudioLDM\cite{liu_audioldm_2023}}
  \label{fig:AudioLDM}
\end{figure} 

Die in Abbildung \ref{fig:AudioLDM} dargestellte Architektur integriert mehrere Komponenten: Einen \emph{VAE}\cite{kingma_auto-encoding_2022}, welcher Mel-Spektrogramme in einen latenten Raum transformiert; einen nach dem \emph{CLAP}-Prinzip\cite{wu_large-scale_2023} entworfenen Audio- und Text-Encoder zur Erzeugung von Embeddings, die zur Konditionierung des \emph{LDMs} dienen; einen \emph{LDM}, der die inhärente Verteilung modelliert; sowie einen \emph{Vocoder}, der das generierte Spektrogramm in ein hörbares Audiosignal konvertiert. \cite{liu_audioldm_2023}

Die \emph{CLAP}-Module erzeugen Embeddings sowohl für Text, repräsentiert durch $\boldsymbol{E}^y \in \mathbb{R}^L$, als auch für Audio, dargestellt durch $\boldsymbol{E}^x \in \mathbb{R}^L$. In diesem Kontext steht $x$ für eine Audioprobe und $y$ für eine Textbeschreibung. Die Funktionen $f_{\text {text }}(\cdot)$ und $f_{\text {audio }}(\cdot)$ agieren hierbei als Encoder. Als Audioencoder kam \emph{HTSAT}\cite{chen_hts-at_2022} zum Einsatz, während für den Textencoder \emph{RoBERTa}\cite{liu_roberta_2019} verwendet wurde. Während des Trainingsprozesses des \emph{LDMs} fungierte $\boldsymbol{E}^x$ des jeweiligen Trainingsdatenpunkts als Konditionierungselement, wohingegen $\boldsymbol{E}^y$ zur Generierung des gewünschten Signals herangezogen wurde \cite{liu_audioldm_2023}.

Der für den \emph{LDM} verwendete Latentraum, beschrieben durch $\boldsymbol{z} \in \mathbb{R}^{C \times \frac{T}{r} \times \frac{F}{r}}$, wurde mittels eines \emph{VAE} auf der Grundlage von Spektrogrammen trainiert. In dieser Darstellung bezeichnet $r$ das \emph{Kompressionsniveau}. Die Variablen $T$ und $F$ repräsentieren die Zeit- und Frequenzdimensionen, während $C$ die Anzahl der Kanäle angibt. Ein gegebenes Audiosignal wird durch die \emph{STFT}-Methode (siehe Referenz \ref{sec:music_math}) in ein Spektrogramm $\boldsymbol{X} \in \mathbb{R}^{T \times F}$ überführt. Im Generierungsprozess wird aus der latenten Variable $\hat{\boldsymbol{z}}_o$ ein Spektrogramm $\hat{\boldsymbol{X}}$ erzeugt. Dieses generierte Spektrogramm wird anschließend durch den \emph{Vocoder}, welcher auf \emph{HiFi-GAN}\cite{kong_hifi-gan_2020} basiert, in ein Audiosignal $\hat{x}$ konvertiert. \cite{liu_audioldm_2023}

Das konzeptuelle Modell \emph{LDM} verfolgt die Absicht, die fundamentale Verteilung $q\left(\boldsymbol{z}0 \mid \boldsymbol{E}^y\right)$ durch die Annäherung an $p\theta\left(\boldsymbol{z}_0 \mid \boldsymbol{E}^y\right)$ zu repräsentieren. In diesem Kontext wurden sowohl die Verlustfunktion als auch der Rückwärtsprozess entsprechend definiert: \cite{liu_audioldm_2023}

\begin{equation}
    L_n(\theta)=\mathbb{E}_{\boldsymbol{z}_0, \boldsymbol{\epsilon}, n}\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{z}_n, n, \boldsymbol{E}^x\right)\right\|_2^2
\end{equation}
\begin{equation}
    p_\theta\left(\boldsymbol{z}_{n-1} \mid \boldsymbol{z}_n, \boldsymbol{E}^y\right)=\mathcal{N}\left(\boldsymbol{z}_{n-1} ; \boldsymbol{\mu}_\theta\left(\boldsymbol{z}_n, n, \boldsymbol{E}^y\right), \boldsymbol{\sigma}_n^2 \boldsymbol{I}\right)
\end{equation}
\begin{equation}
    p_\theta\left(\boldsymbol{z}_{0: N} \mid \boldsymbol{E}^y\right)=p\left(\boldsymbol{z}_N\right) \prod^N p_\theta\left(\boldsymbol{z}_{n-1} \mid \boldsymbol{z}_n, \boldsymbol{E}^y\right) \\
\end{equation}

Der Erwartungswert wird wie folgt parametrisiert und ergibt sich aus dem im Rückwärtsverfahren ermittelten Rauschen: $\boldsymbol{\epsilon}_\theta\left(\boldsymbol{z}_n, n, \boldsymbol{E}^y\right)$ \cite{liu_audioldm_2023}.

\begin{equation}
    \boldsymbol{\mu}_\theta\left(\boldsymbol{z}_n, n, \boldsymbol{E}^y\right)=\frac{1}{\sqrt{\alpha_n}}\left(\boldsymbol{z}_n-\frac{\beta_n}{\sqrt{1-\bar{\alpha}_n}} \boldsymbol{\epsilon}_\theta\left(\boldsymbol{z}_n, n, \boldsymbol{E}^y\right)\right)
\end{equation}

Ferner wurde nachgewiesen, dass der \emph{Cross-Attention}-Mechanismus gemäß \cite{rombach_high-resolution_2022} entbehrlich ist \cite{liu_audioldm_2023}.

\subsection{AudioLDM2}

\emph{AudioLDM2}\cite{liu_audioldm2_2023} baut auf den erarbeiteten Konzepten von \emph{AudioLDM}\cite{liu_audioldm_2023} auf und verspricht durch eine neuartige Architektur (siehe Abbildung \ref{fig:AudioLDM2}) in Verbindung mit innovativen Konzepten verbesserte Resultate in der Audiogenerierung mittels neuronaler Netze. Die treibende Kraft hinter dieser Entwicklung war die Erkenntnis, dass im Bereich der Audiosynthese zwar verschiedene Modelle für unterschiedliche Anwendungen wie Spracherzeugung, Musikgenerierung und Klangsynthese existieren, diese jedoch häufig so spezifisch und beschränkt für eine bestimmte Aufgabe oder Domäne entwickelt werden, dass ihre Anwendbarkeit in einem übergeordneten Kontext limitiert ist. Das vorgestellte Modell verfolgt das Ziel, diese diversen spezifischen Herausforderungen in einem einzigen Modell zu integrieren. Zu diesem Zweck wurde eine Audio-Repräsentation namens \emph{language of audio} (\emph{LOA}) konzipiert, die sich effizient über verschiedene Domänen und Modalitäten, wie Text, Audio und verschiedene Medien, generalisieren lässt. Auf Basis dieser Repräsentation wurde ein \emph{Latente-Diffusion}-Modell trainiert, um die Audiosynthese zu realisieren. \cite{liu_audioldm2_2023}

\begin{figure}[h]
  \centering
  \includegraphics[width=.8\textwidth]{AudioLDM2.png}
  \caption[AudioLDM2 Architektur]{Architektur von AudioLDM2\cite{liu_audioldm2_2023}}
  \label{fig:AudioLDM2}
\end{figure} 

Die Synthese eines Audiosignals $x \in \mathbb{R}^{L_s}$, wobei $L_s$ die Dauer des Signals in Sekunden angibt, kann als Funktion $\mathcal{H}: C \mapsto x$ im Hypothesenraum $\mathcal{H}$ dargestellt werden. In dieser Funktion stellt $C$ die Eingabe dar, auf deren Basis das Signal generiert wird. Diese Eingabe kann sowohl Text als auch diverse Medienformate umfassen. Das Erlernen dieser Funktion stellt aufgrund der signifikanten Unterschiede in den Verteilungen von $C$ und $x$ eine besondere Herausforderung dar. Ein vorgeschlagener Lösungsweg besteht in einer Abstraktion durch \emph{LOA}. Die Darstellung eines Audiosignals im \emph{LOA}-Format wird als $Y=\mathcal{A}(x)$ definiert, wobei $\mathcal{A}$ einen Audio-zu-LOA-Encoder darstellt. $\mathcal{A}$ kann entweder durch vordefinierte Regeln definiert oder mittels \emph{self-supervised learning} \cite{tan_regeneration_2023} trainiert werden. Um verschiedenste Eingabeformate zu unterstützen und in \emph{LOA} zu konvertieren, wurde der Übersetzer \emph{any modality to LOA translator} $\mathcal{M}$ als $\hat{Y}=\mathcal{M}(C)$ definiert. Infolgedessen kann die Signalgenerierungsfunktion entsprechend formuliert werden.  \cite{liu_audioldm2_2023}

\begin{equation}
    \mathcal{H}_0=\mathcal{G} \circ \mathcal{M}: C \mapsto \hat{Y} \mapsto x
\end{equation}

Hierbei extrahiert $\mathcal{G}$ ein Audiosignal aus der \emph{LOA}-Repräsentation. Falls $\mathcal{M}$ durch $\mathcal{A}$ ersetzt wird  (wie in Gleichung \ref{eqn:h1} gezeigt), tritt nur $x$ als Trainingsdaten für $\mathcal{G}$ auf. Dementsprechend kann $\mathcal{G}$ \emph{self-supervised} ohne den Einsatz von Labels trainiert werden, wodurch das Problem einer geringen Anzahl an gelabelten Audiopaaren umgangen wird. \cite{liu_audioldm2_2023}

\begin{equation}
\label{eqn:h1}
    \mathcal{H}_0=\mathcal{G} \circ \mathcal{A}: C \mapsto Y \mapsto x
\end{equation}

Um eine vielfältige Audio-Repräsentation $Y$ für Sprache, Musik und Soundeffekte zu erstellen,sollte diese in der Lage sein, sowohl semantische als auch akustische Informationen zu erfassen. Zur Erfüllung dieser Kriterien wurde für den \emph{Audio zu LOA}-Encoder das mittels \emph{self-supervised} Verfahren vortrainierte Modell \emph{AudioMAE} \cite{huang_masked_2023} herangezogen. Das Modell trainiert eine Repräsentation von ungelabelten Audiosignalen mithilfe eines Encoder-Decoder-Ansatzes. Hierbei werden dem Encoder \emph{Mel-Spektrogramme} mit maskierten Bereichen übergeben, welche der Decoder anschließend zu rekonstruieren versucht. Nachdem die Repräsentation $Y$ generiert wurde, kann diese durch den Einsatz von \emph{average-max pooling} \cite{liu_simple_2023} weiter zu $Y_\lambda$ verfeinert werden. \cite{liu_audioldm2_2023}

Das Erlernen von $\mathcal{G}$ wurde in Anlehnung an \emph{AudioLDM}\cite{liu_audioldm_2023} mittels \emph{Latenter Diffusion}\cite{rombach_high-resolution_2022} durchgeführt (vgl. \ref{sec:AudioLDM}). Der Diffusionsvorgang erreignet sich in einem latenten Raum, der mittels eines \emph{VAE}\cite{kingma_auto-encoding_2022} erlernt wurde. Die Diffusion findet nicht direkt auf $Y$ statt, da sich \emph{AudioMAE} nicht in erster Linie auf den Erhalt der Qualität während des Rekonstruktionsvorgangs konzentriert. Ein \emph{VAE} hingegen kennzeichnet sich durch eine überlegene Rekonstruktionsfähigkeit und ein intensiveres Kompressionsverhältnis. Während \emph{AudioMAE} primär semantische Aspekte hervorhebt, legt der \emph{VAE} sein Augenmerk verstärkt auf akustische Details. \cite{liu_audioldm2_2023}

Die durch den \emph{VAE} generierte latente Repräsentation $z$ wird mittels eines Encoders, der Downsampling verwendet, und eines Decoders, der Upsampling einsetzt, erlernt. Ein gegebenes Spektrogramm $X$ wird in $z$ umgewandelt und gemäß $\mathcal{V}: X \mapsto z \mapsto \hat{X}$ rekonstruiert. Aus $\hat{X}$ kann mithilfe eines vortrainierten vortrainierten \emph{HiFiGAN}\cite{kong_hifi-gan_2020} ein Audiosignal generieren. Die Optimierung des \emph{VAE} basiert auf den Differenzen zwischen $X$ und $\hat{X}$. \cite{liu_audioldm2_2023}

Der Vorwärtsprozess des \emph{LDM} im latenten Raum mit dem Hyperparameter $\beta_t \in[0,1]$ ist wie folgt definiert \cite{liu_audioldm2_2023}:
\begin{equation}
    q\left(z_t \mid z_{t-1}\right)=\sqrt{1-\beta_t} z_{t-1}+\sqrt{\beta_t} \epsilon_t, t \in 2, \ldots, T
\end{equation}

Der Rückwärtsprozess hingegen ist definiert als \cite{liu_audioldm2_2023}:
\begin{equation}
q\left(z_t \mid z_0\right)=\sqrt{\alpha_t} z_0+\sqrt{1-\alpha_t} \epsilon_t
\end{equation}

Die zugehörige Verlustfunktion lautet \cite{liu_audioldm2_2023}:
\begin{equation}
\operatorname{argmin}_\phi\left[\mathbb{E}_{z_0, Y, t \sim\{1, \ldots, T\}}\left\|\mathcal{G}\left(\sqrt{\alpha_t} z_0+\sqrt{1-\alpha_t} \epsilon_t, t, Y ; \phi\right)-\epsilon_t\right\|\right]
\end{equation}

Für $\mathcal{G}$ kam ein \emph{Transformer-UNet} (\emph{T-UNET}) zum Einsatz. Dieses Netzwerk vereint einen Encoder, der Downsampling nutzt, mit einem Decoder, der Upsampling einsetzt. Im Anschluss an einen \emph{Konvolutions}-Block werden zudem \emph{Transformer}-Blöcke eingebunden. \cite{liu_audioldm2_2023}

Das Modell $\mathcal{M}$ ist von Bedeutung, da während der Inferenz $\mathcal{A}$ nicht verfügbar ist. Zur Generierung von $\hat{Y}$ ist ein alternatives Modell $\mathcal{M}_\theta: C \rightarrow \hat{Y}$ erforderlich. Dabei stellt $\theta$ die anlernbaren Parameter dar. Um der Ausgabe des \emph{AudioMAE} optimal zu entsprechen, wurde dieses Problem als Aufgabe der Sprachmodellierung definiert. Als Basis für dieses Modell dient der \emph{Generative Pre-trained Transformer 2} (\emph{GPT-2})\cite{alec_radford_jeff_wu_rewon_child_david_luan_dario_amodei_ilya_sutskever_language_2019}. Dieser wurde mittels eines \emph{unsupervised} Verfahrens auf einem umfangreichen Textdatensatz trainiert. Sein Hauptziel ist die Bewältigung von Aufgaben im Kontext des \emph{Natural Language Processing} (\emph{NLP}), wie Textvervollständigung, Frage-Antwort-Systeme oder Sprachübersetzung. Das Modell \emph{GPT-2} wurde spezifisch für den Einsatz in \emph{AudioLDM2} mittels \emph{teacher forcing}\cite{kolen_field_2001} verfeinert.Gegeben ein Datensatz $C$, wird das Modell durch die Maximierung der Likelihood optimiert. $C$ kann diverse Daten repräsentieren, darunter Audio, Text, Phoneme oder visuelle Daten. Zur Extraktion essenzieller Merkmale kommt ein \emph{Mischung von Experten}-Ansatz (engl. \emph{mixture of experts})\cite{masoudnia_mixture_2014} zum Einsatz, wodurch ein vielfältiges Informationsspektrum zugänglich gemacht wird. Ein \emph{Linearer Adapter} (engl. \emph{linear adaptor}) dient dazu, sämtliche Merkmale auf die einheitliche Dimension $D$ zu bringen. Verschiedenste Systeme können zur Extraktion dieser Merkmale genutzt werden. \cite{liu_audioldm2_2023}

Zur Nutzung von Text als Kondition $C$ kamen sowohl \emph{CLAP}\cite{wu_large-scale_2023} als auch \emph{FLAN-T5}\cite{chung_scaling_2022} zum Einsatz. Da \emph{CLAP} Schwierigkeiten bei der angemessenen Verarbeitung temporaler und semantischer Informationen aufwies, fungierte \emph{FLAN-T5} zusätzlich als zusätzlicher Encoder. Des Weiteren diente \emph{CLAP} der Generierung von Paraphrasen für Audiosignale, welche ursprünglich keine entsprechenden Umschreibungen besaßen, wie es bei Text-zu-Sprach-Aufgaben vorkommt. Ein weiterer in der Arbeit implementierter Encoder ist der \emph{Phoneme Encoder}. Dieser wurde speziell konzipiert, um Informationen über Phoneme, die kleinsten sprachlichen Einheiten, zu kodieren. Für die Kodierung visueller Daten wurde \emph{ImageBind}\cite{girdhar_imagebind_2023} eingesetzt. \cite{liu_audioldm2_2023}

Während des Finetunings entschied ein probabilistischer Mechanismus über das Konditionierungssignal. In $25\%$ der Fälle dienten die von \emph{AudioMAE} extrahierten Informationen als \emph{Ground Truth}. Demgegenüber wurden in den verbleibenden $75\%$ der Szenarien die von \emph{GPT} generierten Daten herangezogen. \cite{liu_audioldm2_2023}

\section{Implementierung des Neuronalen Synthesizers}
\subsection{Audioprogrammierung}
Die Geschwindigkeit der Datenverarbeitung stellt ein maßgebliches Kriterium bei der Wahl der Programmiersprache für die Entwicklung und Implementierung von digitalen Audioprozessen dar. Für Echtzeitanwendungen gilt insbesondere, dass der Code so effizient wie möglich gestaltet sein sollte, um jegliche Latenz zu minimieren. Unter Berücksichtigung dieser Prämissen fällt die Wahl häufig auf eine Realisierung in \emph{C/C++}. \cite{doumler_c_2015, boulanger_audio_2011}

C++ wurde als Erweiterung der Programmiersprache C entwickelt und behält dennoch C als eine seiner Untermengen bei. Es baut auf den fundamentalen Prinzipien von C auf, insbesondere auf der hardwarenahen Programmierung und der Fähigkeit, auf den meisten Systemen zu operieren. Ergänzend erweitert C++ das Repertoire um Facetten der Datenabstraktion sowie um objektorientierte und generische Programmieransätze. \cite{stroustrup_c_1997}

\subsection{JUCE Framework}
\glqq\emph{JUCE} ist das am häufigsten verwendete Framework für die Entwicklung von Audioanwendungen und -Plugins. Es handelt sich dabei um eine Open-Source-C++-Codebasis, die zur Erstellung eigenständiger Software auf Windows, macOS, Linux, iOS und Android sowie VST-, VST3-, AU-, AUv3-, AAX- und LV2-Plugins verwendet werden kann.\grqq \footnote{
JUCE is the most widely used framework for audio application and plug-in development. It is an open source C++ codebase that can be used to create standalone software on Windows, macOS, Linux, iOS and Android, as well VST, VST3, AU, AUv3, AAX and LV2 plug-ins.
}\cite{noauthor_juce_nodate}

Es bietet eine Abstraktion für die Verarbeitung von Audiosamples und \emph{MIDI} von den nativen Audiogeräten auf jeder Plattform oder einer \emph{Host-DAW}. Die von \emph{JUCE} angebotene Bibliothek für \emph{digitale Signalverarbeitungs (DSP)}-Bausteine ermöglicht eine rasche Prototypisierung und Implementierung verschiedener Audioeffekte, Filter, Instrumente und Generatoren. \cite{noauthor_juce_nodate} Zudem bietet \emph{JUCE} eine Vielzahl von Klassen, die gängige Herausforderungen in der Entwicklung von Audioprojekten adressieren. Dies schließt die Verwaltung von Grafiken, Sound, Benutzerinteraktion und Netzwerkkommunikation mit ein. \cite{robinson_getting_2013}

In dieser Arbeit wird das \emph{JUCE}-Framework mittels \emph{CMake}, \glqq eine Open-Source-, plattformübergreifende Werkzeugfamilie, die zur Erstellung, zum Testen und zum Verpacken von Software entwickelt wurde\grqq \footnote{CMake is an open source, cross-platform family of tools designed to build, test, and package software.} \cite{noauthor_cmake_nodate} eingesetzt, um die durch das Diffusionsnetz erstellten Klänge spielbar und manipulierbar zu gestalten. 

Das \emph{Juce}-Modul \emph{PluginGuiMagic} \cite{walz_plugin_nodate} erleichtert und beschleunigt die Gestaltung von Benutzeroberflächen für die zu entwickelnde Anwendung.

\subsection{AudioLDM-Anbindung} \label{sec:api}
Um ein vom den Diffusionsmodellen \emph{AudioLDM} und \emph{AudioLDM2} erzeugtes Sample mithilfe des im \emph{JUCE}-Framework entwickelten digitalen Instruments wiederzugeben, muss Inferenz der Modelle aus dem \emph{C++}-Quellcode des Instruments möglich sein.

Eine optimale Methode wäre die Umwandlung des trainierten Modells in ein \emph{ONNX}-Modell \cite{noauthor_onnx_nodate-1}, wodurch eine binäre Repräsentation des Modells generiert würde. Mittels der \emph{ONNX-Runtime} \cite{noauthor_onnx_nodate} ließe sich diese in \emph{C++} integrieren und abrufen, um Inferenzoperationen auszuführen und so die Soundgenerierung zu realisieren. Dieser Ansatz könnte insbesondere das Kompilieren und die Verbreitung des finalisierten Instruments samt des trainierten Modells fördern. Eine alternative Herangehensweise wäre, das Modell in ein \emph{Torchscript}-Modell \cite{noauthor_torchscript_nodate} zu konvertieren und dieses dann in C++ zu laden. \cite{oli_larkin_machine_2023}

Bislang ließ sich weder ein \emph{ONNX}- noch ein \emph{Torchscript}-Modell für das \emph{AudioLDM}-/\emph{AudioLDM2}-Modell erstellen. Dies liegt an der komplexen Interaktion verschiedener eingesetzter Modelle und Module, von denen einige gegenwärtig weder von \emph{ONNX} noch von \emph{Torchscript} unterstützt werden. Daher wurde für die Realisierung dieses Projekts die Erstellung einer \emph{API (Application Programming Interface)} bevorzugt. Durch definierte Endpunkte ermöglicht diese API die Generierung einer lokalen Instanz des Modells, welche zur Erzeugung benötigter Samples dient. Als Basis für diese Implementierung dient das \emph{FastAPI}-Framework \cite{noauthor_fastapi_nodate}. Auf dem Python-Server wird eine Instanz des \emph{AudioLDM}-/\emph{AudioLDM2}-Modells betrieben, welche mithilfe der \emph{Diffusers}-Bibliothek von \emph{Huggingface} \cite{noauthor_huggingface_nodate-1, noauthor_huggingface_nodate} als \emph{Pipeline} genutzt wird. Die Anwendung dieser Pipeline erlaubt Inferenzen mit einer gewissen Abstraktionsebene und ihre Nutzung auf unterschiedlichen Systemen. Zuvor waren die veröffentlichten Modelle etwa nicht auf Rechnern ohne GPU, wie den M-Modellen von Apple, lauffähig. Doch durch die Konfigurierbarkeit des in der Pipeline genutzten Prozessors ließ sich dieses Hindernis überwinden. Zudem verhindert der Einsatz einer API, dass bei mehreren Synthesizer-Instanzen jeweils ein neues Modell initialisiert werden muss, was eine parallele Nutzung mehrerer Modelle unnötig macht.

Die implementierten Endpunkte beinhalten einen zur Initialisierung eines Modells. Für diese Initialisierung sind Angaben zum \emph{Gerät}, welches den auszuführenden Prozessor bestimmt, und zum \emph{Modell}, welches das zu verwendende vortrainierte Modell festlegt, erforderlich. Der Endpunkt für die Klanggenerierung benötigt Eingaben wie \emph{Prompt}, \emph{negativer Prompt}, \emph{Audiolänge}, \emph{Anzahl der Inferenzschritte} und \emph{Leitskala} und optional einen \emph{Seed}. Als Rückgabe erfolgt ein in \emph{Base64} kodiertes Audiosignal.

\begin{figure}
  \centering
  \includegraphics[width=.4\textwidth]{graphics/Server.png}
  \caption[AudiLDM Anbindung]{Anbindung an den \emph{AudioLDM}-Server}
  \label{fig:server}
\end{figure}

\subsection{Sampler}

Für die Implementierung des Samplers wurde der von \emph{Juce} bereitgestellten Sampler \cite{noauthor_sampler_nodate} verwendet. Darüber hinaus wurde die Funktionalität erweitert, um gezielte Tonverschiebungen durchzuführen und den Pitch-Bender eines Keyboards zu verwenden.

\chapter{Ergebnisse}
Das Zusammenspiel der vorgestellten Komponenten und Elemente führte zu einem digitalen Instrument, welches als Standalone oder als Plugin in einem VST3 oder AU-Format in einer DAW wie \emph{Ableton} \cite{noauthor_ableton_nodate}, \emph{Logic} \cite{noauthor_logic_nodate} und anderen, unter Windows und Mac, verwendet werden kann.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{synth.png}
  \caption[Benutzeroberfläche]{Benutzeroberfläche}
  \label{fig:synth}
\end{figure} 

Die daraus entstandene Benutzeroberfläche (Abbildung \ref{fig:synth}) ermöglicht es einem Nutzer, den Server für die API (siehe \ref{sec:api}) auf einem gewünschten Port  zu starten und ein Modell zu initiieren, indem aus einer Auswahl von vorab trainierten Modellen gewählt wird. Die Verwendung des Modells auf verschiedenen Hardwarekonfigurationen (mit und ohne GPU) und Betriebssystemen wird durch die Auswahl an unterstützten Geräten ermöglicht. Nach der Initialisierung eines Modells kann ein gewünschter Klang, der durch die Texteingabefelder \emph{Prompt} und \emph{Negative Prompt} definiert wird, synthetisiert werden. Auch die Länge des zu erzeugenden Klangs in Sekunden (\emph{audio length}), die Anzahl der Inferenzschritte (\emph{number of inference steps}) und die Leitskala (\emph{guidance scale}) lassen sich einstellen. Um die Erzeugung determiniert zu machen, kann auch ein \emph{Seed} in Form eines 8-Byte großen Integers im Wertebereich von $0$ bis $2^{64}-1$ mit angegeben werden.  

Die \emph{Hüllkurve} des \emph{Samplers} (siehe \ref{sec:synth+envelope}) lässt sich über vier Parameter modifizieren. Hierbei sind die Zeitspannen in Sekunden für die Anschwell- (engl. \emph{attack}), Abschwell- (engl. \emph{decay}) und Ausschwingphasen (engl. \emph{release}) sowie das Level, auf das die Lautstärke absinken soll (engl. \emph{sustain}), festlegbar. Der von dem Instrument generierte Klang kann mithilfe des Sliders "Verstärkung" (engl. \emph{gain}) auf eine bestimmte Lautstärke angepasst werden.

Da die Tonhöhe des generierten Klangs nicht im Voraus bestimmt werden kann und der Klang standardmäßig der \emph{Midi}-Note $C4$ zugeordnet wird, besteht die Möglichkeit, das Instrument so zu stimmen, dass die Tonhöhe des Klangs der gespielten Note entspricht. Hierbei kann der Klang kontinuierlich um bis zu zwölf Halbtöne, also einer Oktave, nach oben oder unten angepasst werden.

Das produzierte Signal wird durch eine Wellenform- und Spektrogramm-Anzeige visualisiert. Dies soll dem Nutzer dabei helfen, die Ergebnisse des Modells besser nachzuvollziehen und das erfolgreiche Generieren zu erkennen. Zudem lassen sich aus dieser Visualisierung bestimmte Klangstrukturen herauslesen.

Die Soundqualität des Instrumentes und die Fähigkeit die gewünschten musikalischen Merkmale aus der textuellen Eingabe zu generieren hängt von dem zur Verwendung kommenden Modell, und der Nutzer spezifischen Eingabe der Parameter und des Promptes ab. 

Eine Sammlung des Verhalten der verschiedene Modelle auf speziellen Parameter und Eingaben wird auf \url{https://suckrowpierre.github.io/TtPS.github.io/}\cite{pierre-louis_suckrow_text-zu-spielbarem-klang_nodate}  visualisiert und ermöglicht eine Evaluierung dieser (siehe \ref{chap:prompts}).

Es lässt sich entnehmen, dass bei gleichen Parametern und Prompt für unterschiedliche \emph{Torch}-Geräte unterschiedliche Audiosignale erzeugt werden. Eine großen unterschied in der Qualität zwischen den Geräten lässt sich nicht eindeutig festlegen. Man kann jedoch feststellen, dass wenn eine Prompt auf den Geräten \glqq\emph{mps}\grqq und \glqq\emph{cuda}\grqq eine schlechtes Ergebnis liefert, dass mittels \glqq\emph{cpu}\grqq generierten Audiosignal für den gleichen Prompt, noch mehr ungewünschte Artefakte und unangenehme Geräusche auftreten. //TODO generate for cuda and show example

Aus \cite{noauthor_audioldm_nodate-1, noauthor_audioldm_nodate} geht hervor, welchen erwarteten Einfluss der Wert der \glqq Leitskala\grqq (eng. \glqq guidance-scale\grqq) auf die Signalqualität hat. Obgleich höhere Werte das beschriebene Prompt präziser darstellen, resultieren sie in einer verminderten Audioqualität. Zur Untersuchung dieses Sachverhalts wurden Audiosignale der Prompts (\glqq Gentle guitar strum, Tribal African drum circle, vocal harmonies\grqq) mittels sämtlicher verfügbarer vorab trainierter Modelle und \glqq Leitskala\grqq-Werten von $1,2,3,4,5$ generiert. Es zeigt sich, dass Audiosignale, die mit einem Wert von $1$ erzeugt wurden, die geringste Qualität aufweisen. Werte zwischen $2-4$ scheinen hingegen eine bessere Audioqualität zu gewährleisten. Die wichtigen Audiocharachteristika scheinen sich in diesem Bereich heraus zu kristallisieren und tiefe Frequenzen kommen zur Prägung. Weiterhin bestätigt sich, dass höhere Werte den gegebenen Prompt detaillierter repräsentieren. Ein exemplarischer Fall hierfür ist der Prompt \glqq Gentle guitar strum\grqq, generiert mit \emph{audioldm-m-full}\cite{noauthor_cvsspaudioldm-m-full_nodate}. Bei diesem zeigen Signale mit einem Wert unter $4$ diverse Audioereignisse, die an eine Gitarre erinnern. Erst bei einem Wert von $4$ oder höher ist ein ein einzelnes eindeutiges Gitarrenzupfen zu identifizieren.

Die \emph{Inferenzschritte} (engl. \emph{Inferencesteps}) geben die Anzahl der Entrauschungsschritte im Diffusionsprozess an. Eine höhere Anzahl dieser Schritte soll zu verbesserten Ergebnissen bei gleichzeitig längerer Inferenzzeit führen \cite{noauthor_audioldm_nodate-1, noauthor_audioldm_nodate}. Um die Auswirkungen der \emph{Inferenzschritte} zu analysieren, wurden wieder Audiosignale aus drei unterschiedlichen Prompts (\glqq ambient texture\grqq, \glqq techno kickdrum\grqq, \glqq Long evolving drone\grqq) mithilfe aller vorab trainierten Modelle und für Inferenzschritte-Anzahlen von $5,10,20,50,100,200,400$ generiert. Hierbei zeigten die Werte $5$ und $400$ die ungünstigsten Ergebnisse: Signale mit $5$ Schritten resultierten in dumpfen und wenig strukturierten Klängen. Hingegen schienen Werte im Bereich von $10-200$ adäquate Resultate zu liefern. Es wurden oftmals nur marginale Qualitätsveränderungen pro Schritt beobachtet. Die Ergebnisse weisen zudem darauf hin, dass \emph{AudioLDM2} im Vergleich zu \emph{AudioLDM} eine größere Anzahl an Schritten benötigt, um qualitativ hochwertige Signale zu produzieren. Bei $400$ Schritten hingegen erscheint die Anzahl überdimensioniert, da das resultierende Signal verstärkt abstrakte Klangereignisse und unerwünschte Artefakte aufweist.

Um die Auswirkungen der gewünschten Audiosignallänge auf die Resultate zu untersuchen, wurden für die drei Prompts (\glqq Chirping birds at dawn\grqq, \glqq A choir pad\grqq, \glqq An ambient electronic pad\grqq) mittels der Modelle \emph{audioldm-m-full}\cite{noauthor_cvsspaudioldm-m-full_nodate} und \emph{audioldm2}\cite{noauthor_cvsspaudioldm2_nodate} Signale mit den Längen $5,10,20,30$ generiert. Bei ausgedehnteren Signalen scheint die Qualität leicht abzunehmen. Offenbar haben die Modelle Schwierigkeiten, die Konsistenz über längere Zeiträume hinweg beizubehalten. In einigen dieser Beispiele manifestiert sich ein Phänomen, das als \emph{Pitchdrifting} bezeichnet wird, wodurch die wahrgenommene Tonhöhe variiert. Diese leicht reduzierte Qualität könnte jedoch gezielt als stilistisches Mittel verwendet werden.

Die Einwirkung verschiedener \emph{Negative-Prompts} (engl. \emph{negative prompt}) auf die Prompts (\glqq Soft flute note\grqq, \glqq Choir\grqq, \glqq A muted trumpet\grqq) wurde analysiert. Mittels der Modelle \emph{audioldm-m-full}\cite{noauthor_cvsspaudioldm-m-full_nodate} und \emph{audioldm2-music}\cite{noauthor_cvsspaudioldm2-music_nodate} wurden Signale unter Verwendung der \emph{Negativen Prompts} (\glqq low quality\grqq, \glqq average quality\grqq, \glqq harsh noise\grqq, \glqq dissonant chords\grqq, \glqq distorted sounds\grqq, \glqq clashing frequencies\grqq, \glqq feedback loop\grqq, \glqq clattering\grqq, \glqq inharmonious\grqq, \glqq noise\grqq, \glqq high pitch\grqq, \glqq artefacts\grqq) generiert. Die Beobachtungen zeigen, dass einige \emph{Negative-Prompts} das Ergebnis positiv beeinflussen, während andere das Signal mit zusätzlichem Rauschen beeinträchtigen. Insbesondere \glqq low quality\grqq, \glqq noise\grqq, \glqq harsh noise\grqq und \glqq average quality\grqq schienen das Resultat am meisten zu verbessern. Der \emph{Negative-Prompt} \glqq distorted sounds\grqq zeigte in einigen Fällen positive Effekte, bewirkte jedoch im Zusammenhang mit dem \emph{Prompt} \glqq Soft flute note\grqq und dem Modell \emph{audioldm-m-full} eine Verschlechterung. \glqq high pitch\grqq scheint hohe Obertöne zu unterdrücken und tiefere Töne bekommen mehr Präsenz, was eine bewusste Entscheidung seien sollte. Andere \emph{Negative-Prompts}, wie \glqq dissonant chords\grqq, \glqq distorted sounds\grqq, \glqq clashing frequencies\grqq, \glqq feedback loop\grqq, \glqq clattering\grqq, \glqq inharmonious\grqq und \glqq artefacts\grqq, scheinen das Resultat zu beeinträchtigen. Die anfängliche Annahme, dass \emph{AudioLDM2} aufgrund von \emph{LOLA} effizienter mit \emph{Negative-Prompts} interagieren könnte, wurde durch die ungünstigen Ergebnisse von \emph{audioldm2-music} in den gegebenen Tests nicht bestätigt.


Prompts, die nur ein einzelnes kurzes nicht wiederholendes Audioereigniss umschreiben,wie \glqq A single kickdrum\grqq; 
\glqq A kickdrum\grqq; \glqq A clap\grqq; \glqq A snare\grqq; etc.., erzeugen nicht das erwarte Ergebniss, sonder Signale, in denen Audioereignisse mehrfach vorkommen. Die Modelle versuchen nicht das gewünschte einzelne Ereigniss zu synthetisieren, sondern versuchen das gwünschte Ergebniss in einem musikalischen Rhytmus zu erzeugen 

\chapter{Diskussion}

Das Aufzeigen, der Möglichkeit Instrumente basierend auf künstlicher Intelligenz zu entwickeln stellt eine beachtliche Erweiterung der musikalischen Klangerzeugung dar. Die Untersuchung des digitalen Instruments und die Evaluation der anwendbaren Modelle legen nahe, dass die Generierung, Manipulation und Reproduktion von durch Künstliche Intelligenz synthetisierten Klängen ein erhebliches kreatives Potential besitzen. Dennoch zeigen sich bestimmte limitierende Schwachstellen. Insbesondere die identifizierten Mängel, das gelegentlich unvorhersehbare Verhalten der Modelle und die eingeschränkte Umsetzung des Samplers gehören dazu.

Es wird deutlich, dass die untersuchten Modelle bisher nicht adäquat auf Eingabeaufforderungen im Kontext der Musikproduktion reagieren. Die Unfähigkeit, einzelne Klangereignisse zu generieren, das Versäumnis, erwartete Klangmerkmale von spezifischen Instrumenten zu modellieren, sowie einige verrauschte Ergebnisse, deuten auf vorhandene Defizite hin. Diese könnten möglicherweise durch das Training mit mehr Audiodaten, die in der Musikproduktion verwendet werden, behoben werden. Trainingsdaten hierfür liegen bei Synthesizer- oder Samplermanufaktoren, Audiobibliothken wie Splice etc.., Musikstudios oder Produzenten selber. Privatpersonen, welche Musikproduktion betreiben besitzen meist selber eine Bibliothek an Klängen, welche Verwendung in deren Kreationen findet. Basierend auf diesen könnte ein gewünschtes Modell weiter gefinedtuned werden um dieses Individueller und persönlicher zu gestalten. Ein weiterer Idee, wäre diese verschiedenen Biblitoheken von verschiedenen Privat Personen zu einer großen nutzbaren Datenmenge zusammenzuführen, welche anschließend für Finetuning oder Training genutzt werden. Dieser Community-Driven Ansatz wäre ein demokratisierter Weg den Mangeln an Datensätze in diesem Bereich zu überwinden. 

Das Scheitern des Erstellen eines ONNX, bzw einer TorchScript Repräsentation des Modells um Inferenz in C++ zu betreiben, bringt die Schwierigkeit mit sich das Modell samt nötiger Verknüpfung mit der Python API zu bündeln und verteilen. Eine gesamte Implementation in einer Sprache wäre deutlich wünschenswerter und einfacherer. Ein Server und eine Api wären somit nicht mehr von Nöten und 

Die rudimentäre Implementierung des Samplers verhindert aktuell die Festlegung von Start- und Endpunkten für die Wiedergabe des generierten Signals. Solche Parameter könnten eine intuitive Methode darstellen, um aus mehreren oder wiederholten Klangereignissen das gewünschte auszuwählen. Auch fehlt die Option, bestimmte Klangbereiche zu loopen, um einen kontinuierlichen Klang zu erzeugen, ein Feature, das in industriell genutzten Samplern seit den 1990er Jahren Standard ist. In diesem Kontext erhebt sich die Frage, ob die Entwicklung eines eigenen Samplers tatsächlich erforderlich ist. Alternativ könnte eine Benutzeroberfläche ausreichen, die sich in bestehende Musikproduktionssoftware integrieren lässt, um Klänge zu generieren und mittels der Werkzeuge des jeweiligen Software-Ökosystems zu manipulieren. Ein solches Instrument könnte Anwendungen wie Splice potenziell obsolet machen, vorausgesetzt, das Modell ist ausreichend leistungsfähig. Die in AudioLDM vorgestellten Generierungsoptionen, wie Inpainting oder Style-Transfer, könnten in einem derartigen Tool integriert werden, sodass es beispielsweise einem Nutzer ermöglicht wird, bestimmte unerwünschte Abschnitte eines Spektrogramms zu entfernen oder Audiodateien zu importieren und mittels Eingabeaufforderungen zu modifizieren.

Falls jedoch die Entwicklung eines eigens entwickelten Samplers weiterverfolgt wird könnten diesem auch Aspekte des maschinellen Lernens zugeführt werden. Wenn auf einem Klavier unterschiedliche Noten gespielt werden habe diese unterschiedliche Klangfarben und somit unterschiedlichen Spektren \cite{parker_good_2009}. Es reicht nicht aus den Klang in die gewünschte Frequenz zu bringen. Um ein realisitischeres Ergebniss zu erziehlen müssen die kleinen Unterschiede der Oberschwingungen für jede Tonhöhe modulliert werden. Genau dies könnte mit einem ML-Modell erlernt werden. Als Eingabe würde ein Audiosignal dienen, aus dem das Modell jeweils eine Signal für alle möglichen Tonhöhen erzeugt. Um dieses zu trainieren wäre eine große Datenmenge an Audiosignalen von nöten, die alle Tonhöhen von verschiedenen Instrumenten umfassen. 

Weitere Mängel des Instrumentes welche adressiert und verbessert werden könnten, wären zum Beispiel das manuelle Stimmen des Instruments, welches sich automatisierten ließ. Eine weitere große Schwäche, ist das bisher generierte Klänge und Einstellungen sich nicht wie üblich bei digitalen Instrumenten speichern und abrufen lassen. 

Dem Prinzip der Additivensynthese, folgend, könnten dem Instrument mehr als eine Soundquelle beigeführt werden, und diese mit Mixern und Filtern zusammen führen. 


TODO Ref Roland sytnh with multiple sound sources  

Schwächen:

AudioLDM

Zu wenig Training über Musikproduktions Keywords

Nervige Obertöne

Keine zuordnung zu Genres, Künstler, Epochen

Keine Automatische Pitch Erkennung

16.000 khz 

--------------

Stärken:

Neue Soundsynthese

Ambiend Pads

Sound Experimentierung

Happy Little Accidents


--------------

Verbesserung:

Automatische Pitch Zuordnung
Filter einbauen

Mehr Zuordnung zu Emotionen 

AudioLDM 2

Schneller Inferenz

Spezielle Hardware 

ML-Sampler

Spektogramm Manipulation 

Loop Points

Modulare Aufbau 

Finetuning auf mehr musikalischen Daten

Indivduelles Finetuning 

Eine Programmiersprache

ONNX

Binäredaten

Envelope Visualiesieren

Additive Synth

Convelution Mixer

Presets speichern 

Schon Präsente Spuren Analysieren und Lücken finden 

ML-Effekte

Mehre Modell Auswahl 

Modell zusammenbauen ? 





\chapter{Schlussfolgerung}
\label{sec:conclusion}


\printbibliography

All links were last followed on \today{}.

\appendix

\chapter{Prompts}
\label{chap:prompts}

Die auf \url{https://suckrowpierre.github.io/TtPS.github.io/}\cite{pierre-louis_suckrow_text-zu-spielbarem-klang_nodate} vorgestellten Ergebnisse wurden folgender maßen erstellt.

Mittels den folgenden Prompts wurden für jedes der Modelle \emph{audioldm-m-full, audioldm-l-full, audioldm2-large, audioldm2-music} \cite{liu_audioldm_2023,liu_audioldm2_2023} jeweils ein Audiosignal mittels dem Quellcode \ref{lst:generator}\cite{pierre-louis_suckrow_thesismodelsresultsgenerator_2023} generiert. 

\begin{multicols}{4}
\fontsize{8pt}{11pt}\selectfont
\begin{itemize}
    \item A single kickdrum
    \item A kickdrum
    \item A distored kickdrum
    \item A kickdrum with a lot of reverb
    \item Deep house kick
    \item Techno kick
    \item A clap
    \item A weird clap
    \item A snare
    \item A snare with reverb tail
    \item A string orchestra
    \item A analog synth
    \item A distorted analog synth
    \item A string synth
    \item dreamy nostalgic strings
    \item Ambient pads
    \item A guitar string
    \item A distorted guitar string
    \item A tight snare
    \item 808 bass
    \item A sub-bass
    \item Jazz ride cymbal
    \item A realistic hi-hat pattern
    \item A blues harmonica
    \item A smooth saxophone
    \item A jazz trumpet solo
    \item Electric piano chords
    \item Grand piano arpeggios
    \item Harpsichord playing Baroque
    \item A sitar drone
    \item A tabla rhythm
    \item Fast-paced conga drums
    \item A marimba melody
    \item An upright bass groove
    \item Lo-fi beats
    \item A vocoder voice
    \item A children's choir
    \item An operatic soprano
    \item A flute ensemble
    \item An oboe solo
    \item Bassoon in a chamber setting
    \item Steel drums playing Calypso
    \item A bagpipe melody
    \item A didgeridoo drone
    \item Djembe in a world music context
    \item A triangle ting
    \item Cowbell in a funk setting
    \item A gong hit
    \item Wind chimes in C major
    \item A celesta playing a lullaby
    \item A Hammond organ in gospel style
    \item A Moog synthesizer lead
    \item Square wave arpeggiator
    \item FM synthesis bells
    \item Noise sweep
    \item White noise with a filter sweep
    \item Vinyl crackle
    \item Tape hiss
    \item Reverse cymbal
    \item Side-chained pad
    \item Glitched vocal sample
    \item Auto-tuned vocals
    \item A theremin playing a melody
    \item Beatboxing sequence
    \item Finger snaps in a jazz setting
    \item A full orchestral crescendo
    \item A muted trumpet
    \item A slide guitar in a blues context
    \item Clawhammer banjo
    \item Double-bass pizzicato
    \item A haunting choir pad
    \item Ambient rainforest sounds
    \item Foley footsteps
    \item Synth brass in an '80s style
    \item Harp glissando
    \item Ticking clock sample
    \item Ambient ocean waves
    \item Jazz brush kit groove
    \item A detuned square wave synth
    \item An aggressive dubstep wobble bass
    \item Shrill piccolo solo
    \item Melodic pan flute
    \item Sustained pipe organ chord
    \item Choir singing in Latin
    \item A fingerstyle ukulele
    \item Flamenco guitar playing fast runs
    \item Hard rock power chords
    \item Acoustic guitar with a capo on 5th fret
    \item A sultry bossa nova rhythm
    \item Swing band with a prominent double bass
    \item Spooky theremin melody
    \item A pulsing trance bassline
    \item Crunchy breakbeat loop
    \item Reggae offbeat chords
    \item A barbershop quartet
    \item Bowed double bass drone
    \item Glissando on a grand harp
    \item Layered vocal harmonies
    \item A slow, haunting Gregorian chant
    \item A distorted metal guitar solo
    \item A mellow Rhodes piano groove
    \item A detuned radio frequency scan
    \item Spacey sci-fi effects
    \item A mandolin playing bluegrass
    \item Chorus of kazoos playing a melody
    \item Melodica in a reggae context
    \item Pop punk palm-muted guitar
    \item Bright and shimmering glockenspiel
    \item High-pitched bat squeaks
    \item Traditional Japanese koto melody
    \item Tribal African drum circle
    \item A drone from a tanpura
    \item Cajun accordion riff
    \item Complex polyrhythmic hand percussion
    \item A twangy banjo in a bluegrass setting
    \item Psychedelic sitar in a raga style
    \item A punchy techno kick
    \item Rich choir aahs and ooohs
    \item Massive cinematic drum hits
    \item A fluid slap bass line
    \item Balkan brass band upbeat tune
    \item Tense suspense string section
    \item Shruti box drone in C\#
    \item Chirping birds at dawn
    \item Tuvan throat singing melody
    \item A haunting reverb-drenched ebow guitar
    \item Energetic punk rock drum fill
    \item Middle Eastern oud improvisation
    \item Polka accordion and tuba duet
    \item Smooth and soulful Motown bassline
    \item An eerie wind howling effect
    \item Traditional Chinese guzheng melody
    \item A slapstick comedy woodblock
    \item Bouncy reggaeton beat with dembow rhythm
    \item Echoing canyon yodel
\end{itemize}
\end{multicols}

Die Liste an musikalisch und instrumentalen Prompts wurden unteranderem mittels \emph{GPT-4}\cite{openai_gpt-4_2023} auf \url{https://chat.openai.com/} mittels folgendem Prompt am 8. September 2023 generiert. 

\lstdefinestyle{gpt}{
    basicstyle=\ttfamily\small,
    breaklines=true
}

\begin{lstlisting}[style=gpt]
For a scientific work of mine, I'm evaluating the music abilities of diffusion models that can generate audio. For that, I have a script that generates audio for different models based on a list of prompts. My list looks like the following. Please add more prompts to test the musical and instrumental abilities of the models. Keep in mind the people using the models would be musicians, studio engineers, and music producers. 


A single kickdrum
A kickdrum
A distored kickdrum
A kickdrum with a lot of reverb
A clap
A weird clap
A snare
A string orchestra
A analog synth
A distorted analog synth
A string synth
dreamy nostalgic strings
Ambient pads
A guitar string
A distorted guitar string
\end{lstlisting}

\begin{Listing}
\lstinputlisting[language=Python, frame=single,basicstyle=\scriptsize]{code/generator.py}
 \caption{Python code zur Generierung der Ergebnisse}
  \label{lst:generator}
\end{Listing}
%\input{latexhints/latexhints-english}
\pagestyle{empty}
\renewcommand*{\chapterpagestyle}{empty}
\Affirmation
\end{document}
